{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c477dc17",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4de54aa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, TFBertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ba3e9d6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5a7446a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "41db7cd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "686cde84",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "1a2cdd37",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "33441d2e18ad439d89e28834c4201ee5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dafff488bc6046e9872217002efb6612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nsmc.py:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. NSMC 데이터셋 불러오기\n",
    "dataset = load_dataset(\"nsmc\")  # Hugging Face에서 NSMC 데이터셋 로드\n",
    "train_dataset = dataset[\"train\"]  # train split 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6c6949b4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.weight', 'cls.predictions.decoder.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. 토크나이저와 모델 로드 (예: BERT 기반 한국어 모델)\n",
    "model_name = \"klue/bert-base\"  # 또는 \"klue/bert-base\" 등 한국어 모델 사용 가능\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ff598e82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 토크나이즈 함수 정의\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=None  # Trainer가 텐서로 변환하므로 딕셔너리 형태로 반환\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e1893c6f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c9997ef5d8bd4373a419e070685df209",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. 토크나이즈 적용\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\", \"id\"]  # 불필요한 열 제거\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "9854a2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. train/val 분리\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=123)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a6021183",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: {'labels': 0, 'input_ids': [2, 3790, 2170, 4027, 24304, 24, 2532, 5675, 18, 20608, 2119, 3760, 11531, 1819, 2075, 2088, 3758, 2079, 12488, 2119, 4239, 3788, 2283, 2097, 2223, 2088, 6641, 4177, 25175, 2118, 3926, 18395, 2573, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Train columns: ['labels', 'input_ids', 'token_type_ids', 'attention_mask']\n",
      "Sample 0 input_ids length: 128\n",
      "Sample 1 input_ids length: 128\n",
      "Sample 2 input_ids length: 128\n",
      "Sample 3 input_ids length: 128\n",
      "Sample 4 input_ids length: 128\n"
     ]
    }
   ],
   "source": [
    "# 6. 데이터 확인 (디버깅용)\n",
    "print(\"Train sample:\", train_dataset[0])\n",
    "print(\"Train columns:\", train_dataset.column_names)\n",
    "for i in range(min(5, len(train_dataset))):\n",
    "    print(f\"Sample {i} input_ids length:\", len(train_dataset[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "fd505db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # 빠른 테스트를 위해 1로 설정\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy=\"steps\",  # 평가 주기 설정\n",
    "    save_strategy=\"steps\",       # 체크포인트 저장 전략\n",
    "    save_steps=1000,             # 체크포인트 저장 주기\n",
    "    load_best_model_at_end=True, # 학습 끝난 후 최적 모델 로드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "01a8fefe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 1:14:42, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.355300</td>\n",
       "      <td>0.318039</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.319700</td>\n",
       "      <td>0.305719</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.302400</td>\n",
       "      <td>0.283015</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.283000</td>\n",
       "      <td>0.289858</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.271300</td>\n",
       "      <td>0.270443</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.278500</td>\n",
       "      <td>0.253696</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.244400</td>\n",
       "      <td>0.248830</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-1000\n",
      "Configuration saved in ./results/checkpoint-1000/config.json\n",
      "Model weights saved in ./results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-2000\n",
      "Configuration saved in ./results/checkpoint-2000/config.json\n",
      "Model weights saved in ./results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-3000\n",
      "Configuration saved in ./results/checkpoint-3000/config.json\n",
      "Model weights saved in ./results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-4000\n",
      "Configuration saved in ./results/checkpoint-4000/config.json\n",
      "Model weights saved in ./results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-5000\n",
      "Configuration saved in ./results/checkpoint-5000/config.json\n",
      "Model weights saved in ./results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-6000\n",
      "Configuration saved in ./results/checkpoint-6000/config.json\n",
      "Model weights saved in ./results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-6000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./results/checkpoint-7000\n",
      "Configuration saved in ./results/checkpoint-7000/config.json\n",
      "Model weights saved in ./results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./results/checkpoint-7000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./results/checkpoint-7000 (score: 0.2488296777009964).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.3001753224690755, metrics={'train_runtime': 4483.0838, 'train_samples_per_second': 26.767, 'train_steps_per_second': 1.673, 'total_flos': 7893331660800000.0, 'train_loss': 0.3001753224690755, 'epoch': 1.0})"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 8. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 9. 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42c13227",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketing 적용을 위한 모델\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b4fba9e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다이나믹패딩을 위한 토크나이즈 함수 정의\n",
    "def nonpadding_tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5005eb6c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "3299f430",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3304dcb6c204488a115cfb30d3331f8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 패딩 제거 토크나이즈 적용\n",
    "nonpadding_tokenized_dataset = train_dataset.map(\n",
    "    nonpadding_tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\", \"id\"]  # 불필요한 열 제거\n",
    ")\n",
    "nonpadding_tokenized_dataset = nonpadding_tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3cfc5664",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용하지 않은 데이터셋 생성\n",
    "\n",
    "nonpadding_split_dataset = nonpadding_tokenized_dataset.train_test_split(test_size=0.2, seed=123)\n",
    "nonpadding_train_dataset = nonpadding_split_dataset[\"train\"]\n",
    "nonpadding_val_dataset = nonpadding_split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a3f4631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "PyTorch: setting up devices\n",
      "The default value for the training argument `--report_to` will change in v5 (from all installed integrations to none). In v5, you will need to use `--report_to all` to get the same behavior as now. You should start updating your code and make this info disappear :-).\n"
     ]
    }
   ],
   "source": [
    "# bucketing을 위한 학습 설정\n",
    "\n",
    "bucket_training_args = TrainingArguments(\n",
    "    output_dir='./bucket_results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./bucket_logs',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    group_by_length=True,  # Bucketing 활성화: 비슷한 길이의 샘플을 그룹화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "2ca458e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "961f6487",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketing을 위한 data_collator생성\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,          # 배치 내 최대 길이에 맞춰 패딩\n",
    "    pad_to_multiple_of=8   # GPU 효율성을 위해 8의 배수로 패딩 (선택적)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "508b0e14",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 18:16, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>0.236800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>0.219000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.225300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>0.237700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.223200</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>0.221900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.227100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>0.215600</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.218900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5500</td>\n",
       "      <td>0.222900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.235100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6500</td>\n",
       "      <td>0.233700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.231000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7500</td>\n",
       "      <td>0.252100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving model checkpoint to ./bucket_results/checkpoint-500\n",
      "Configuration saved in ./bucket_results/checkpoint-500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-1000\n",
      "Configuration saved in ./bucket_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-1000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-1500\n",
      "Configuration saved in ./bucket_results/checkpoint-1500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-1500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-1500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-1500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-2000\n",
      "Configuration saved in ./bucket_results/checkpoint-2000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-2000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-2500\n",
      "Configuration saved in ./bucket_results/checkpoint-2500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-2500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-2500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-2500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-3000\n",
      "Configuration saved in ./bucket_results/checkpoint-3000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-3000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-3500\n",
      "Configuration saved in ./bucket_results/checkpoint-3500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-3500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-3500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-3500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-4000\n",
      "Configuration saved in ./bucket_results/checkpoint-4000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-4000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-4500\n",
      "Configuration saved in ./bucket_results/checkpoint-4500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-4500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-4500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-4500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-5000\n",
      "Configuration saved in ./bucket_results/checkpoint-5000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-5000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-5500\n",
      "Configuration saved in ./bucket_results/checkpoint-5500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-5500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-5500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-5500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-6000\n",
      "Configuration saved in ./bucket_results/checkpoint-6000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-6000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-6500\n",
      "Configuration saved in ./bucket_results/checkpoint-6500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-6500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-6500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-6500/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-7000\n",
      "Configuration saved in ./bucket_results/checkpoint-7000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-7000/special_tokens_map.json\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-7500\n",
      "Configuration saved in ./bucket_results/checkpoint-7500/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-7500/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-7500/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-7500/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.23017427775065105, metrics={'train_runtime': 1097.1004, 'train_samples_per_second': 109.379, 'train_steps_per_second': 6.836, 'total_flos': 1656612982310400.0, 'train_loss': 0.23017427775065105, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_collator 적용한 학습 수행\n",
    "bucket_trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=bucket_training_args,\n",
    "    train_dataset=nonpadding_train_dataset,\n",
    "    eval_dataset=nonpadding_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # Dynamic Padding 적용\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "bucket_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "3d627a00",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#pip list | grep evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "8f519daf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "863ab1bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2f3b0bcffb14fcab522eb667c4ef126",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "a40026a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 03:56]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bucketing 적용 전 예측\n",
    "\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_pred_labels = val_predictions.predictions.argmax(-1)\n",
    "val_true_labels = val_predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb7bb35b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9024\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용 전 검증 정확도\n",
    "\n",
    "val_accuracy = accuracy_metric.compute(predictions=val_pred_labels, references=val_true_labels)\n",
    "print(f\"Validation Accuracy: {val_accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "cd26914c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용 후 예측\n",
    "\n",
    "bucket_val_predictions = bucket_trainer.predict(nonpadding_val_dataset)\n",
    "bucket_val_pred_labels = bucket_val_predictions.predictions.argmax(-1)\n",
    "bucket_val_true_labels = bucket_val_predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "d1be19af",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with bucket: 0.9024\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용 후 검증 정확도\n",
    "\n",
    "bucket_val_accuracy = accuracy_metric.compute(predictions=bucket_val_pred_labels, references=val_true_labels)\n",
    "print(f\"Validation Accuracy with bucket: {bucket_val_accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "345d7591",
   "metadata": {},
   "source": [
    "#### bucketing과 다이나믹 패딩 적용 전 후 비교\n",
    " - 학습 시간 : 적용 전 74분, 적용 후 18분\n",
    " - 추론 소요시간 : 적용 전 4분, 적용후 1분 내외 \n",
    " - 검증정확도 : 적용 전 0.9024, 적용 후 0.9024\n",
    "\n",
    "##### 학습시간은 약 4배 빨라졌고 검증 정확도는 성능이 유지되었음\n",
    " - 같은 모델에서 학습 됐을 것으로 보임(bucket_trainer의 첫 에폭 손실값이 너무 낮음)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
