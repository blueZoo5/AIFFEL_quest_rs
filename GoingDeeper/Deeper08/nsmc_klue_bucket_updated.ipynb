{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cd12bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow\n",
    "import numpy as np\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "22807ac5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModel, TFBertForPreTraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a3e9fdaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, BertTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "47bb67ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoConfig"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2b0722da",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import TrainingArguments, Trainer, AutoModelForSequenceClassification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "37886a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0f983764",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63c7c0acc00c459d83a471abe43ab940",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/3.74k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08d18812f1244ec68dd19bf51ad2030d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "nsmc.py:   0%|          | 0.00/3.18k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 1. NSMC 데이터셋 불러오기\n",
    "dataset = load_dataset(\"nsmc\")  # Hugging Face에서 NSMC 데이터셋 로드\n",
    "train_dataset = dataset[\"train\"]  # train split 사용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "dc9829a0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.dense.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.decoder.weight', 'cls.seq_relationship.bias', 'cls.predictions.decoder.bias', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# 2. 토크나이저와 모델 로드 (예: BERT 기반 한국어 모델)\n",
    "model_name = \"klue/bert-base\"  # 또는 \"klue/bert-base\" 등 한국어 모델 사용 가능\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "51a34c16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. 토크나이즈 함수 정의\n",
    "def tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=\"max_length\",\n",
    "        max_length=128,\n",
    "        return_tensors=None  # Trainer가 텐서로 변환하므로 딕셔너리 형태로 반환\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "e6251d29",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30179f1db3624694a6413aa249ecb94c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 4. 토크나이즈 적용\n",
    "tokenized_dataset = train_dataset.map(\n",
    "    tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\", \"id\"]  # 불필요한 열 제거\n",
    ")\n",
    "tokenized_dataset = tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "7630fa7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 5. train/val 분리\n",
    "split_dataset = tokenized_dataset.train_test_split(test_size=0.2, seed=123)\n",
    "train_dataset = split_dataset[\"train\"]\n",
    "val_dataset = split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a777490f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train sample: {'token_type_ids': [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], 'labels': 0, 'input_ids': [2, 3790, 2170, 4027, 24304, 24, 2532, 5675, 18, 20608, 2119, 3760, 11531, 1819, 2075, 2088, 3758, 2079, 12488, 2119, 4239, 3788, 2283, 2097, 2223, 2088, 6641, 4177, 25175, 2118, 3926, 18395, 2573, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]}\n",
      "Train columns: ['attention_mask', 'input_ids', 'labels', 'token_type_ids']\n",
      "Sample 0 input_ids length: 128\n",
      "Sample 1 input_ids length: 128\n",
      "Sample 2 input_ids length: 128\n",
      "Sample 3 input_ids length: 128\n",
      "Sample 4 input_ids length: 128\n"
     ]
    }
   ],
   "source": [
    "# 6. 데이터 확인 (디버깅용)\n",
    "print(\"Train sample:\", train_dataset[0])\n",
    "print(\"Train columns:\", train_dataset.column_names)\n",
    "for i in range(min(5, len(train_dataset))):\n",
    "    print(f\"Sample {i} input_ids length:\", len(train_dataset[i][\"input_ids\"]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a76c3077",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 7. 학습 설정\n",
    "training_args = TrainingArguments(\n",
    "    output_dir='./results',\n",
    "    num_train_epochs=1,  # 빠른 테스트를 위해 1로 설정\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./logs',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy=\"steps\",  # 평가 주기 설정\n",
    "    save_strategy=\"steps\",       # 체크포인트 저장 전략\n",
    "    save_steps=1000,             # 체크포인트 저장 주기\n",
    "    load_best_model_at_end=True, # 학습 끝난 후 최적 모델 로드\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d6cec8f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# 8. Trainer 설정\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    ")\n",
    "\n",
    "# 9. 학습 시작\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "3b880fc8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/bert-base were not used when initializing BertForSequenceClassification: ['cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.weight', 'cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.predictions.transform.dense.bias', 'cls.predictions.bias', 'cls.predictions.decoder.bias', 'cls.predictions.decoder.weight', 'cls.seq_relationship.weight']\n",
      "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at klue/bert-base and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용을 위한 모델\n",
    "model2 = AutoModelForSequenceClassification.from_pretrained(model_name, num_labels=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f8fc6458",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다이나믹패딩을 위한 토크나이즈 함수 정의\n",
    "def nonpadding_tokenize_function(example):\n",
    "    return tokenizer(\n",
    "        example[\"document\"],\n",
    "        truncation=True,\n",
    "        padding=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "3c8323a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = dataset[\"train\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a0b7cf92",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c93fce35bd3f4b47a6f0a3e3f3922da6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/150000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 패딩 제거 토크나이즈 적용\n",
    "nonpadding_tokenized_dataset = train_dataset.map(\n",
    "    nonpadding_tokenize_function,\n",
    "    batched=True,\n",
    "    remove_columns=[\"document\", \"id\"]  # 불필요한 열 제거\n",
    ")\n",
    "nonpadding_tokenized_dataset = nonpadding_tokenized_dataset.rename_column(\"label\", \"labels\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "72e72f6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 패딩 적용하지 않은 데이터셋 생성\n",
    "\n",
    "nonpadding_split_dataset = nonpadding_tokenized_dataset.train_test_split(test_size=0.2, seed=123)\n",
    "nonpadding_train_dataset = nonpadding_split_dataset[\"train\"]\n",
    "nonpadding_val_dataset = nonpadding_split_dataset[\"test\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "91072ede",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketing을 위한 학습 설정\n",
    "\n",
    "bucket_training_args = TrainingArguments(\n",
    "    output_dir='./bucket_results',\n",
    "    num_train_epochs=1,\n",
    "    per_device_train_batch_size=16,\n",
    "    per_device_eval_batch_size=64,\n",
    "    warmup_steps=500,\n",
    "    weight_decay=0.01,\n",
    "    logging_dir='./bucket_logs',\n",
    "    do_train=True,\n",
    "    do_eval=True,\n",
    "    eval_steps=1000,\n",
    "    evaluation_strategy=\"steps\",  # 평가 주기 설정\n",
    "    save_strategy=\"steps\",       # 체크포인트 저장 전략\n",
    "    save_steps=1000,             # 체크포인트 저장 주기\n",
    "    load_best_model_at_end=True, # 학습 끝난 후 최적 모델 로드\n",
    "    group_by_length=True,  # Bucketing 활성화: 비슷한 길이의 샘플을 그룹화\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "6c65d7d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorWithPadding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4c0a5d8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# bucketing을 위한 data_collator생성\n",
    "\n",
    "data_collator = DataCollatorWithPadding(\n",
    "    tokenizer=tokenizer,\n",
    "    padding=True,          # 배치 내 최대 길이에 맞춰 패딩\n",
    "    pad_to_multiple_of=8   # GPU 효율성을 위해 8의 배수로 패딩 (선택적)\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "f3a4baf8",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running training *****\n",
      "  Num examples = 120000\n",
      "  Num Epochs = 1\n",
      "  Instantaneous batch size per device = 16\n",
      "  Total train batch size (w. parallel, distributed & accumulation) = 16\n",
      "  Gradient Accumulation steps = 1\n",
      "  Total optimization steps = 7500\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='7500' max='7500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [7500/7500 34:44, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>0.353700</td>\n",
       "      <td>0.382512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>0.331000</td>\n",
       "      <td>0.296016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>0.303000</td>\n",
       "      <td>0.287602</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>0.288500</td>\n",
       "      <td>0.289535</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5000</td>\n",
       "      <td>0.269900</td>\n",
       "      <td>0.271328</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6000</td>\n",
       "      <td>0.277100</td>\n",
       "      <td>0.251906</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7000</td>\n",
       "      <td>0.248700</td>\n",
       "      <td>0.252512</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-1000\n",
      "Configuration saved in ./bucket_results/checkpoint-1000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-1000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-1000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-1000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-2000\n",
      "Configuration saved in ./bucket_results/checkpoint-2000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-2000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-2000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-2000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-3000\n",
      "Configuration saved in ./bucket_results/checkpoint-3000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-3000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-3000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-3000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-4000\n",
      "Configuration saved in ./bucket_results/checkpoint-4000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-4000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-4000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-4000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-5000\n",
      "Configuration saved in ./bucket_results/checkpoint-5000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-5000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-5000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-5000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-6000\n",
      "Configuration saved in ./bucket_results/checkpoint-6000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-6000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-6000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-6000/special_tokens_map.json\n",
      "***** Running Evaluation *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n",
      "Saving model checkpoint to ./bucket_results/checkpoint-7000\n",
      "Configuration saved in ./bucket_results/checkpoint-7000/config.json\n",
      "Model weights saved in ./bucket_results/checkpoint-7000/pytorch_model.bin\n",
      "tokenizer config file saved in ./bucket_results/checkpoint-7000/tokenizer_config.json\n",
      "Special tokens file saved in ./bucket_results/checkpoint-7000/special_tokens_map.json\n",
      "\n",
      "\n",
      "Training completed. Do not forget to share your model on huggingface.co/models =)\n",
      "\n",
      "\n",
      "Loading best model from ./bucket_results/checkpoint-6000 (score: 0.25190553069114685).\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=7500, training_loss=0.30200464680989586, metrics={'train_runtime': 2087.0105, 'train_samples_per_second': 57.499, 'train_steps_per_second': 3.594, 'total_flos': 1656612982310400.0, 'train_loss': 0.30200464680989586, 'epoch': 1.0})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# data_collator 적용한 학습 수행\n",
    "bucket_trainer = Trainer(\n",
    "    model=model2,\n",
    "    args=bucket_training_args,\n",
    "    train_dataset=nonpadding_train_dataset,\n",
    "    eval_dataset=nonpadding_val_dataset,\n",
    "    tokenizer=tokenizer,\n",
    "    data_collator=data_collator,  # Dynamic Padding 적용\n",
    ")\n",
    "\n",
    "# 학습 시작\n",
    "bucket_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "feb3a23e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n",
      "Collecting evaluate\n",
      "  Downloading evaluate-0.4.3-py3-none-any.whl (84 kB)\n",
      "     |████████████████████████████████| 84 kB 2.6 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: packaging in /opt/conda/lib/python3.9/site-packages (from evaluate) (21.3)\n",
      "Requirement already satisfied: pandas in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.3.3)\n",
      "Requirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.9/site-packages (from evaluate) (4.62.3)\n",
      "Requirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2021.11.1)\n",
      "Requirement already satisfied: xxhash in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.0.2)\n",
      "Collecting datasets>=2.0.0\n",
      "  Downloading datasets-3.4.1-py3-none-any.whl (487 kB)\n",
      "     |████████████████████████████████| 487 kB 13.9 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: multiprocess in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.70.12.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.9/site-packages (from evaluate) (1.21.4)\n",
      "Requirement already satisfied: dill in /opt/conda/lib/python3.9/site-packages (from evaluate) (0.3.4)\n",
      "Requirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.9/site-packages (from evaluate) (2.26.0)\n",
      "Collecting huggingface-hub>=0.7.0\n",
      "  Downloading huggingface_hub-0.29.3-py3-none-any.whl (468 kB)\n",
      "     |████████████████████████████████| 468 kB 62.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (6.0)\n",
      "Collecting tqdm>=4.62.1\n",
      "  Downloading tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "     |████████████████████████████████| 78 kB 10.2 MB/s            \n",
      "\u001b[?25hCollecting fsspec[http]>=2021.05.0\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "     |████████████████████████████████| 183 kB 70.2 MB/s            \n",
      "\u001b[?25hRequirement already satisfied: aiohttp in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.8.1)\n",
      "Collecting pyarrow>=15.0.0\n",
      "  Downloading pyarrow-19.0.1-cp39-cp39-manylinux_2_28_x86_64.whl (42.1 MB)\n",
      "     |████████████████████████████████| 42.1 MB 66.0 MB/s            \n",
      "\u001b[?25hCollecting requests>=2.19.0\n",
      "  Downloading requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "     |████████████████████████████████| 64 kB 6.5 MB/s             \n",
      "\u001b[?25hRequirement already satisfied: filelock in /opt/conda/lib/python3.9/site-packages (from datasets>=2.0.0->evaluate) (3.4.0)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.9/site-packages (from huggingface-hub>=0.7.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.9/site-packages (from packaging->evaluate) (3.0.6)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (1.26.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.10)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2.0.8)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.9/site-packages (from requests>=2.19.0->evaluate) (2021.10.8)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.9/site-packages (from pandas->evaluate) (2021.3)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (5.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.7.2)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (21.2.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (4.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.9/site-packages (from aiohttp->datasets>=2.0.0->evaluate) (1.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.9/site-packages (from python-dateutil>=2.7.3->pandas->evaluate) (1.16.0)\n",
      "Installing collected packages: tqdm, requests, fsspec, pyarrow, huggingface-hub, datasets, evaluate\n",
      "  Attempting uninstall: tqdm\n",
      "    Found existing installation: tqdm 4.62.3\n",
      "    Uninstalling tqdm-4.62.3:\n",
      "      Successfully uninstalled tqdm-4.62.3\n",
      "  Attempting uninstall: requests\n",
      "    Found existing installation: requests 2.26.0\n",
      "    Uninstalling requests-2.26.0:\n",
      "      Successfully uninstalled requests-2.26.0\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2021.11.1\n",
      "    Uninstalling fsspec-2021.11.1:\n",
      "      Successfully uninstalled fsspec-2021.11.1\n",
      "  Attempting uninstall: pyarrow\n",
      "    Found existing installation: pyarrow 6.0.1\n",
      "    Uninstalling pyarrow-6.0.1:\n",
      "      Successfully uninstalled pyarrow-6.0.1\n",
      "  Attempting uninstall: huggingface-hub\n",
      "    Found existing installation: huggingface-hub 0.0.19\n",
      "    Uninstalling huggingface-hub-0.0.19:\n",
      "      Successfully uninstalled huggingface-hub-0.0.19\n",
      "  Attempting uninstall: datasets\n",
      "    Found existing installation: datasets 1.14.0\n",
      "    Uninstalling datasets-1.14.0:\n",
      "      Successfully uninstalled datasets-1.14.0\n",
      "Successfully installed datasets-3.4.1 evaluate-0.4.3 fsspec-2024.12.0 huggingface-hub-0.29.3 pyarrow-19.0.1 requests-2.32.3 tqdm-4.67.1\n",
      "\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "#pip install evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "88f7eaeb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습완료된 bucketing 적용 전 모델 로드\n",
    "\n",
    "checkpoint_path = \"./results/checkpoint-7000\"  # validation loss가 가장 낮았던 체크포인트\n",
    "loaded_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=2)\n",
    "loaded_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# 새로운 Trainer 생성\n",
    "trainer = Trainer(\n",
    "    model=loaded_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_dataset,\n",
    "    eval_dataset=val_dataset,\n",
    "    tokenizer=loaded_tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4dad6d53",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading configuration file ./bucket_results/checkpoint-6000/config.json\n",
      "Model config BertConfig {\n",
      "  \"_name_or_path\": \"klue/bert-base\",\n",
      "  \"architectures\": [\n",
      "    \"BertForSequenceClassification\"\n",
      "  ],\n",
      "  \"attention_probs_dropout_prob\": 0.1,\n",
      "  \"classifier_dropout\": null,\n",
      "  \"hidden_act\": \"gelu\",\n",
      "  \"hidden_dropout_prob\": 0.1,\n",
      "  \"hidden_size\": 768,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 3072,\n",
      "  \"layer_norm_eps\": 1e-12,\n",
      "  \"max_position_embeddings\": 512,\n",
      "  \"model_type\": \"bert\",\n",
      "  \"num_attention_heads\": 12,\n",
      "  \"num_hidden_layers\": 12,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"position_embedding_type\": \"absolute\",\n",
      "  \"problem_type\": \"single_label_classification\",\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.11.3\",\n",
      "  \"type_vocab_size\": 2,\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32000\n",
      "}\n",
      "\n",
      "loading weights file ./bucket_results/checkpoint-6000/pytorch_model.bin\n",
      "All model checkpoint weights were used when initializing BertForSequenceClassification.\n",
      "\n",
      "All the weights of BertForSequenceClassification were initialized from the model checkpoint at ./bucket_results/checkpoint-6000.\n",
      "If your task is similar to the task the model of the checkpoint was trained on, you can already use BertForSequenceClassification for predictions without further training.\n",
      "Didn't find file ./bucket_results/checkpoint-6000/added_tokens.json. We won't load it.\n",
      "loading file ./bucket_results/checkpoint-6000/vocab.txt\n",
      "loading file ./bucket_results/checkpoint-6000/tokenizer.json\n",
      "loading file None\n",
      "loading file ./bucket_results/checkpoint-6000/special_tokens_map.json\n",
      "loading file ./bucket_results/checkpoint-6000/tokenizer_config.json\n"
     ]
    }
   ],
   "source": [
    "# 학습완료된 bucketing 적용 후 모델 로드\n",
    "\n",
    "checkpoint_path = \"./bucket_results/checkpoint-6000\"  # validation loss가 가장 낮았던 체크포인트\n",
    "bucket_model = AutoModelForSequenceClassification.from_pretrained(checkpoint_path, num_labels=2)\n",
    "bucket_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)\n",
    "\n",
    "# data_collator 적용한 모델 불러오기\n",
    "bucket_trainer = Trainer(\n",
    "    model=bucket_model,\n",
    "    args=bucket_training_args,\n",
    "    train_dataset=nonpadding_train_dataset,\n",
    "    eval_dataset=nonpadding_val_dataset,\n",
    "    tokenizer=bucket_tokenizer,\n",
    "    data_collator=data_collator,  # Dynamic Padding 적용\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b1daebfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "import evaluate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "00e99459",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2f1fa8847fb4dc08e8c42681fe94b7f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading builder script:   0%|          | 0.00/4.20k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "accuracy_metric = evaluate.load(\"accuracy\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "186e9505",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 03:49]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bucketing 적용 전 예측\n",
    "\n",
    "val_predictions = trainer.predict(val_dataset)\n",
    "val_pred_labels = val_predictions.predictions.argmax(-1)\n",
    "val_true_labels = val_predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "549b495b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy: 0.9011\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용 전 검증 정확도\n",
    "\n",
    "val_accuracy = accuracy_metric.compute(predictions=val_pred_labels, references=val_true_labels)\n",
    "print(f\"Validation Accuracy: {val_accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "5d577eda",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "***** Running Prediction *****\n",
      "  Num examples = 30000\n",
      "  Batch size = 64\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='469' max='469' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [469/469 02:32]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# bucketing 적용 후 예측\n",
    "\n",
    "bucket_val_predictions = bucket_trainer.predict(nonpadding_val_dataset)\n",
    "bucket_val_pred_labels = bucket_val_predictions.predictions.argmax(-1)\n",
    "bucket_val_true_labels = bucket_val_predictions.label_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "5da549e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Accuracy with bucket: 0.8962\n"
     ]
    }
   ],
   "source": [
    "# bucketing 적용 후 검증 정확도\n",
    "\n",
    "bucket_val_accuracy = accuracy_metric.compute(predictions=bucket_val_pred_labels, references=val_true_labels)\n",
    "print(f\"Validation Accuracy with bucket: {bucket_val_accuracy['accuracy']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4f541fb",
   "metadata": {},
   "source": [
    "#### bucketing과 다이나믹 패딩 적용 전 후 비교\n",
    " - 학습 시간 : 적용 전 74분, 적용 후 34분\n",
    " - 추론 소요시간 : 적용 전 4분, 적용후 2분 30초\n",
    " - 검증정확도 : 적용 전 0.9011, 적용 후 0.8962\n",
    "\n",
    "##### 학습시간은 약 2배 빨라졌고 검증 정확도는 성능이 비슷한 수준에서 유지되었음\n",
    "##### bucketing과 다이나믹 패딩 적용 전 후 학습에 동일한 모델변수를 넣어서 수행했을때 검증 정확도 향상은 크지 않았음\n",
    "##### 데이터를 불러올때 판다스 데이터프레임으로 사용하면 허깅페이스 데이터셋 만들 때 아래와 같은 에러 발생\n",
    " - ValueError: Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' 'truncation=True' to have batched tensors with the same length."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
