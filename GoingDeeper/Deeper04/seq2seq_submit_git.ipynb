{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d2eac0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0185b2de",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "75a3db40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "489cb97e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['개인용 컴퓨터 사용의 상당 부분은 \"이것보다 뛰어날 수 있느냐?\"\\n', '모든 광마우스와 마찬가지 로 이 광마우스도 책상 위에 놓는 마우스 패드를 필요로 하지 않는다.\\n', '그러나 이것은 또한 책상도 필요로 하지 않는다.\\n']\n"
     ]
    }
   ],
   "source": [
    "enc_train_data = 'korean-english-park.train.ko'\n",
    "dec_train_data = 'korean-english-park.train.en'\n",
    "enc_val_data = 'korean-english-park.dev.ko'\n",
    "dec_val_data = 'korean-english-park.dev.en'\n",
    "enc_test_data = 'korean-english-park.test.ko'\n",
    "dec_test_data = 'korean-english-park.test.en'\n",
    "\n",
    "# 데이터 읽기\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "    return data\n",
    "\n",
    "# 파일 읽기\n",
    "enc_train_data = load_data(enc_train_data)\n",
    "dec_train_data = load_data(dec_train_data)\n",
    "enc_val_data = load_data(enc_val_data)\n",
    "dec_val_data = load_data(dec_val_data)\n",
    "enc_test_data = load_data(enc_test_data)\n",
    "dec_test_data = load_data(dec_test_data)\n",
    "\n",
    "# 데이터 미리 보기\n",
    "print(enc_train_data[:3])  # 첫 3개 항목 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bf804036",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data = enc_train_data + enc_val_data\n",
    "dec_data = dec_train_data + dec_val_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d17af7b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e38859a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1000"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_val_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b4fd96ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수기호 확인하는 함수\n",
    "def find_special_char(data):\n",
    "    # 한글과 숫자를 제외한 특수문자만 찾는 정규표현식\n",
    "    pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "    \n",
    "    special_chars = []\n",
    "    \n",
    "    # 리스트의 각 항목에 대해 특수기호를 찾음\n",
    "    for text in data:\n",
    "        if isinstance(text, str):  # 문자열인 경우에만 처리\n",
    "            # 정규표현식을 통해 특수문자 추출\n",
    "            special_chars.extend(re.findall(pattern, text))\n",
    "    \n",
    "    return special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1593e348",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '{',\n",
       " '}',\n",
       " '~',\n",
       " '±',\n",
       " '²',\n",
       " '´',\n",
       " '·',\n",
       " '×',\n",
       " 'é',\n",
       " '˙',\n",
       " '˝',\n",
       " '–',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '•',\n",
       " '‥',\n",
       " '…',\n",
       " '℃',\n",
       " 'ℓ',\n",
       " '▲',\n",
       " '▶',\n",
       " '〈',\n",
       " '〉',\n",
       " '〔',\n",
       " '〕',\n",
       " 'い',\n",
       " 'か',\n",
       " 'き',\n",
       " 'く',\n",
       " 'さ',\n",
       " 'ざ',\n",
       " 'し',\n",
       " 'す',\n",
       " 'そ',\n",
       " 'て',\n",
       " 'と',\n",
       " 'ば',\n",
       " 'ぶ',\n",
       " 'ぷ',\n",
       " 'ま',\n",
       " 'や',\n",
       " 'よ',\n",
       " 'ら',\n",
       " 'り',\n",
       " 'ん',\n",
       " 'ㄴ',\n",
       " 'ㅇ',\n",
       " 'ㅋ',\n",
       " 'ㆍ',\n",
       " '㈜',\n",
       " '㎝',\n",
       " '㎞',\n",
       " '㎠',\n",
       " '㎡',\n",
       " '㎢',\n",
       " '㎾',\n",
       " '一',\n",
       " '万',\n",
       " '三',\n",
       " '上',\n",
       " '不',\n",
       " '丙',\n",
       " '中',\n",
       " '主',\n",
       " '之',\n",
       " '九',\n",
       " '亞',\n",
       " '交',\n",
       " '京',\n",
       " '仁',\n",
       " '代',\n",
       " '佛',\n",
       " '促',\n",
       " '信',\n",
       " '修',\n",
       " '假',\n",
       " '備',\n",
       " '傷',\n",
       " '像',\n",
       " '僞',\n",
       " '價',\n",
       " '億',\n",
       " '兆',\n",
       " '光',\n",
       " '克',\n",
       " '兒',\n",
       " '內',\n",
       " '全',\n",
       " '公',\n",
       " '兵',\n",
       " '再',\n",
       " '切',\n",
       " '前',\n",
       " '劉',\n",
       " '力',\n",
       " '加',\n",
       " '動',\n",
       " '勝',\n",
       " '北',\n",
       " '區',\n",
       " '卍',\n",
       " '占',\n",
       " '印',\n",
       " '卿',\n",
       " '反',\n",
       " '受',\n",
       " '古',\n",
       " '司',\n",
       " '吉',\n",
       " '同',\n",
       " '名',\n",
       " '吳',\n",
       " '吾',\n",
       " '命',\n",
       " '和',\n",
       " '品',\n",
       " '商',\n",
       " '善',\n",
       " '喩',\n",
       " '器',\n",
       " '四',\n",
       " '因',\n",
       " '國',\n",
       " '園',\n",
       " '圓',\n",
       " '團',\n",
       " '在',\n",
       " '地',\n",
       " '坤',\n",
       " '型',\n",
       " '城',\n",
       " '基',\n",
       " '堀',\n",
       " '報',\n",
       " '場',\n",
       " '塔',\n",
       " '墓',\n",
       " '壽',\n",
       " '外',\n",
       " '多',\n",
       " '大',\n",
       " '天',\n",
       " '夫',\n",
       " '奔',\n",
       " '奴',\n",
       " '姓',\n",
       " '婦',\n",
       " '子',\n",
       " '字',\n",
       " '孩',\n",
       " '學',\n",
       " '宇',\n",
       " '安',\n",
       " '官',\n",
       " '室',\n",
       " '家',\n",
       " '富',\n",
       " '察',\n",
       " '寧',\n",
       " '寶',\n",
       " '寺',\n",
       " '對',\n",
       " '導',\n",
       " '小',\n",
       " '局',\n",
       " '居',\n",
       " '屋',\n",
       " '山',\n",
       " '岩',\n",
       " '島',\n",
       " '峽',\n",
       " '川',\n",
       " '州',\n",
       " '市',\n",
       " '席',\n",
       " '常',\n",
       " '平',\n",
       " '幹',\n",
       " '店',\n",
       " '度',\n",
       " '座',\n",
       " '庫',\n",
       " '康',\n",
       " '廣',\n",
       " '建',\n",
       " '式',\n",
       " '弗',\n",
       " '强',\n",
       " '影',\n",
       " '後',\n",
       " '心',\n",
       " '性',\n",
       " '愁',\n",
       " '愚',\n",
       " '愛',\n",
       " '慶',\n",
       " '懷',\n",
       " '戰',\n",
       " '所',\n",
       " '手',\n",
       " '掃',\n",
       " '掌',\n",
       " '換',\n",
       " '播',\n",
       " '故',\n",
       " '效',\n",
       " '敎',\n",
       " '敢',\n",
       " '敵',\n",
       " '文',\n",
       " '斌',\n",
       " '新',\n",
       " '方',\n",
       " '族',\n",
       " '旗',\n",
       " '日',\n",
       " '早',\n",
       " '明',\n",
       " '星',\n",
       " '時',\n",
       " '晉',\n",
       " '晴',\n",
       " '書',\n",
       " '曾',\n",
       " '最',\n",
       " '會',\n",
       " '月',\n",
       " '本',\n",
       " '朱',\n",
       " '李',\n",
       " '東',\n",
       " '林',\n",
       " '査',\n",
       " '株',\n",
       " '案',\n",
       " '棺',\n",
       " '植',\n",
       " '楊',\n",
       " '機',\n",
       " '權',\n",
       " '次',\n",
       " '止',\n",
       " '正',\n",
       " '死',\n",
       " '殘',\n",
       " '毁',\n",
       " '母',\n",
       " '毛',\n",
       " '毬',\n",
       " '民',\n",
       " '氣',\n",
       " '水',\n",
       " '氷',\n",
       " '江',\n",
       " '沈',\n",
       " '河',\n",
       " '油',\n",
       " '法',\n",
       " '泥',\n",
       " '泰',\n",
       " '流',\n",
       " '浅',\n",
       " '海',\n",
       " '淘',\n",
       " '淺',\n",
       " '港',\n",
       " '湖',\n",
       " '湛',\n",
       " '溫',\n",
       " '滑',\n",
       " '澤',\n",
       " '濠',\n",
       " '濤',\n",
       " '濱',\n",
       " '瀋',\n",
       " '灣',\n",
       " '火',\n",
       " '無',\n",
       " '然',\n",
       " '焼',\n",
       " '煙',\n",
       " '照',\n",
       " '燭',\n",
       " '父',\n",
       " '爾',\n",
       " '物',\n",
       " '獨',\n",
       " '獸',\n",
       " '率',\n",
       " '王',\n",
       " '珀',\n",
       " '現',\n",
       " '球',\n",
       " '理',\n",
       " '琥',\n",
       " '環',\n",
       " '生',\n",
       " '産',\n",
       " '田',\n",
       " '男',\n",
       " '異',\n",
       " '畵',\n",
       " '疆',\n",
       " '疑',\n",
       " '疫',\n",
       " '病',\n",
       " '登',\n",
       " '發',\n",
       " '的',\n",
       " '省',\n",
       " '眞',\n",
       " '石',\n",
       " '社',\n",
       " '神',\n",
       " '票',\n",
       " '祭',\n",
       " '福',\n",
       " '私',\n",
       " '種',\n",
       " '空',\n",
       " '突',\n",
       " '章',\n",
       " '管',\n",
       " '節',\n",
       " '築',\n",
       " '精',\n",
       " '紅',\n",
       " '紙',\n",
       " '結',\n",
       " '維',\n",
       " '總',\n",
       " '纏',\n",
       " '置',\n",
       " '羅',\n",
       " '美',\n",
       " '群',\n",
       " '習',\n",
       " '翔',\n",
       " '老',\n",
       " '者',\n",
       " '耳',\n",
       " '聖',\n",
       " '聲',\n",
       " '職',\n",
       " '肢',\n",
       " '胡',\n",
       " '能',\n",
       " '膚',\n",
       " '臺',\n",
       " '舊',\n",
       " '航',\n",
       " '船',\n",
       " '花',\n",
       " '芽',\n",
       " '英',\n",
       " '草',\n",
       " '華',\n",
       " '董',\n",
       " '葬',\n",
       " '蓄',\n",
       " '蓋',\n",
       " '蔣',\n",
       " '藥',\n",
       " '藻',\n",
       " '號',\n",
       " '街',\n",
       " '衛',\n",
       " '要',\n",
       " '視',\n",
       " '親',\n",
       " '覺',\n",
       " '言',\n",
       " '詩',\n",
       " '誌',\n",
       " '語',\n",
       " '談',\n",
       " '論',\n",
       " '諡',\n",
       " '諸',\n",
       " '識',\n",
       " '貧',\n",
       " '買',\n",
       " '費',\n",
       " '賣',\n",
       " '質',\n",
       " '赤',\n",
       " '走',\n",
       " '超',\n",
       " '足',\n",
       " '路',\n",
       " '身',\n",
       " '軌',\n",
       " '軍',\n",
       " '輔',\n",
       " '輿',\n",
       " '辛',\n",
       " '近',\n",
       " '迷',\n",
       " '送',\n",
       " '通',\n",
       " '進',\n",
       " '道',\n",
       " '邦',\n",
       " '邸',\n",
       " '郞',\n",
       " '郡',\n",
       " '都',\n",
       " '鄕',\n",
       " '配',\n",
       " '酒',\n",
       " '酸',\n",
       " '醒',\n",
       " '醜',\n",
       " '野',\n",
       " '銀',\n",
       " '銅',\n",
       " '錦',\n",
       " '鎔',\n",
       " '長',\n",
       " '門',\n",
       " '開',\n",
       " '間',\n",
       " '陳',\n",
       " '陽',\n",
       " '雙',\n",
       " '難',\n",
       " '雨',\n",
       " '雲',\n",
       " '電',\n",
       " '霧',\n",
       " '靑',\n",
       " '靜',\n",
       " '非',\n",
       " '頁',\n",
       " '頂',\n",
       " '頭',\n",
       " '題',\n",
       " '類',\n",
       " '食',\n",
       " '香',\n",
       " '馬',\n",
       " '駐',\n",
       " '體',\n",
       " '髮',\n",
       " '魚',\n",
       " '鳥',\n",
       " '鳩',\n",
       " '麥',\n",
       " '麻',\n",
       " '鼓',\n",
       " '\\uf0d7',\n",
       " '金',\n",
       " '懶',\n",
       " '不',\n",
       " '良',\n",
       " '女',\n",
       " '年',\n",
       " '戀',\n",
       " '領',\n",
       " '料',\n",
       " '遼',\n",
       " '龍',\n",
       " '柳',\n",
       " '輪',\n",
       " '率',\n",
       " '識',\n",
       " '！',\n",
       " '（',\n",
       " '）',\n",
       " '－',\n",
       " '．',\n",
       " '１',\n",
       " '２',\n",
       " '３',\n",
       " '４',\n",
       " '５',\n",
       " '８',\n",
       " '？',\n",
       " '\\U000d51d1'}"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(find_special_char(enc_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1360208c",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'!',\n",
       " '\"',\n",
       " '#',\n",
       " '$',\n",
       " '%',\n",
       " '&',\n",
       " \"'\",\n",
       " '(',\n",
       " ')',\n",
       " '*',\n",
       " '+',\n",
       " ',',\n",
       " '-',\n",
       " '.',\n",
       " '/',\n",
       " ':',\n",
       " ';',\n",
       " '<',\n",
       " '=',\n",
       " '>',\n",
       " '?',\n",
       " '@',\n",
       " '[',\n",
       " '\\\\',\n",
       " ']',\n",
       " '^',\n",
       " '_',\n",
       " '`',\n",
       " '~',\n",
       " '¡',\n",
       " '¢',\n",
       " '£',\n",
       " '¥',\n",
       " '¦',\n",
       " '¨',\n",
       " '®',\n",
       " '¯',\n",
       " '°',\n",
       " '±',\n",
       " '´',\n",
       " 'µ',\n",
       " '·',\n",
       " '¸',\n",
       " '¹',\n",
       " 'º',\n",
       " '»',\n",
       " '½',\n",
       " '¾',\n",
       " '¿',\n",
       " 'À',\n",
       " 'Á',\n",
       " 'Â',\n",
       " 'Ç',\n",
       " 'Ê',\n",
       " 'Î',\n",
       " 'Ï',\n",
       " 'Ñ',\n",
       " 'Ù',\n",
       " 'Û',\n",
       " 'â',\n",
       " 'æ',\n",
       " 'ç',\n",
       " 'ñ',\n",
       " 'ó',\n",
       " 'ö',\n",
       " 'ø',\n",
       " 'û',\n",
       " '˝',\n",
       " '–',\n",
       " '—',\n",
       " '―',\n",
       " '‘',\n",
       " '’',\n",
       " '“',\n",
       " '”',\n",
       " '…',\n",
       " '℃',\n",
       " '∼'}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "set(find_special_char(dec_data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d92524f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence, s_token=False, e_token=False):\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "\n",
    "    if s_token:\n",
    "        sentence = '<start> ' + sentence\n",
    "\n",
    "    if e_token:\n",
    "        sentence += ' <end>'\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ec0a202b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 제 23차 연례 컴덱스 박람회의 개회사를 한 케이츠는 2년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다 .\n",
      "영어: <start> Gates , who opened the 23rd annual Comdex trade show , said there was a negative perception of high tech following the collapse of the tech bubble about two years ago . <end>\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 50000\n",
    "\n",
    "for kor in enc_data[:num_examples]:\n",
    "    enc_corpus.append(preprocess_sentence(kor))\n",
    "    \n",
    "for eng in dec_data[:num_examples]:\n",
    "    dec_corpus.append(preprocess_sentence(eng, s_token=True, e_token=True))\n",
    "\n",
    "print(\"한국어:\", enc_corpus[100])\n",
    "print(\"영어:\", dec_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b401134c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "import MeCab\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "a8acb6e1",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /aiffel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "fedf321f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(corpus, lang='ko'):\n",
    "    tokenizer = tf.keras.preprocessing.text.Tokenizer(filters='')\n",
    "\n",
    "    tokenized_corpus = []\n",
    "    \n",
    "    for sentence in corpus:\n",
    "        if lang == 'ko':  # 한국어 텍스트일 경우 MeCab 사용\n",
    "            parsed = mecab.parse(sentence)\n",
    "            tokens = [line.split('\\t')[0] for line in parsed.splitlines() if line]\n",
    "        elif lang == 'en':  # 영어 텍스트일 경우 NLTK 사용\n",
    "            tokens = nltk.word_tokenize(sentence)\n",
    "        else:\n",
    "            raise ValueError(\"Language should be either 'ko' or 'en'.\")\n",
    "\n",
    "        # 리스트에 토큰 추가\n",
    "        tokenized_corpus.append(' '.join(tokens))\n",
    "    \n",
    "    tokenizer.fit_on_texts(tokenized_corpus)  # 토큰을 기반으로 Tokenizer 학습\n",
    "    \n",
    "    # 특수 토큰 추가\n",
    "    tokenizer.word_index['<start>'] = len(tokenizer.word_index) + 1\n",
    "    tokenizer.word_index['<end>'] = len(tokenizer.word_index) + 2\n",
    "    \n",
    "    tensor = tokenizer.texts_to_sequences(tokenized_corpus)  # 텍스트를 시퀀스로 변환\n",
    "    tensor = tf.keras.preprocessing.sequence.pad_sequences(tensor, padding='post')  # 시퀀스 길이를 동일하게 맞추기\n",
    "\n",
    "    return tensor, tokenizer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c0b1ae01",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_corpus, enc_tokenizer = tokenize(enc_corpus)\n",
    "dec_corpus, dec_tokenizer = tokenize(dec_corpus, lang='en')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "1fccf300",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_corpus, dec_corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a906e4dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "cb8a1fe0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "fb36d17a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "49e03421",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Output: (16, 30, 512)\n",
      "Decoder Output: (16, 40154)\n",
      "Decoder Hidden State: (16, 512)\n",
      "Attention: (16, 30, 1)\n"
     ]
    }
   ],
   "source": [
    "BATCH_SIZE     = 16\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512\n",
    "embedding_dim = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)\n",
    "\n",
    "# sample input\n",
    "sequence_len = 30\n",
    "\n",
    "sample_enc = tf.random.uniform((BATCH_SIZE, sequence_len))\n",
    "sample_output = encoder(sample_enc)\n",
    "\n",
    "print ('Encoder Output:', sample_output.shape)\n",
    "\n",
    "sample_state = tf.random.uniform((BATCH_SIZE, units))\n",
    "\n",
    "sample_logits, h_dec, attn = decoder(tf.random.uniform((BATCH_SIZE, 1)),\n",
    "                                     sample_state, sample_output)\n",
    "\n",
    "print ('Decoder Output:', sample_logits.shape)\n",
    "print ('Decoder Hidden State:', h_dec.shape)\n",
    "print ('Attention:', attn.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0570259f",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "655273da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정 (베스트 모델 저장)\n",
    "def create_checkpoint_manager(encoder, decoder, optimizer, checkpoint_dir='./checkpoints'):\n",
    "    checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "    manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=1)\n",
    "    return checkpoint, manager"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "dbbfd4e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "724946bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "\n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "e1878ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 체크포인트 설정\n",
    "checkpoint_dir = './checkpoints'\n",
    "checkpoint_prefix = os.path.join(checkpoint_dir, \"ckpt\")\n",
    "checkpoint = tf.train.Checkpoint(optimizer=optimizer, encoder=encoder, decoder=decoder)\n",
    "checkpoint_manager = tf.train.CheckpointManager(checkpoint, checkpoint_dir, max_to_keep=3)\n",
    "\n",
    "best_val_loss = float('inf')  # 초기 검증 손실"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "560de532",
   "metadata": {},
   "outputs": [],
   "source": [
    "# eval_step 함수 정의\n",
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "572cacd3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1: 100%|██████████| 2500/2500 [18:57<00:00,  2.20it/s, loss=2.2594]\n",
      "Test Epoch 1: 100%|██████████| 625/625 [02:05<00:00,  4.97it/s, test_loss=2.2634]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "모델 체크포인트 저장 완료!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 2: 100%|██████████| 2500/2500 [18:58<00:00,  2.20it/s, loss=2.2431]\n",
      "Test Epoch 2: 100%|██████████| 625/625 [01:35<00:00,  6.51it/s, test_loss=2.2756]\n",
      "Epoch 3: 100%|██████████| 2500/2500 [18:57<00:00,  2.20it/s, loss=2.2439]\n",
      "Test Epoch 3: 100%|██████████| 625/625 [01:35<00:00,  6.51it/s, test_loss=2.2742]\n",
      "Epoch 4: 100%|██████████| 2500/2500 [18:57<00:00,  2.20it/s, loss=2.2434]\n",
      "Test Epoch 4: 100%|██████████| 625/625 [01:35<00:00,  6.51it/s, test_loss=2.2751]\n",
      "Epoch 5:  55%|█████▌    | 1375/2500 [10:25<08:33,  2.19it/s, loss=2.2415]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 10\n",
    "# Training Process\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list, desc=f\"Epoch {epoch + 1}\")\n",
    "\n",
    "    # 학습 (Training)\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_postfix(loss=f\"{total_loss.numpy() / (batch + 1):.4f}\")\n",
    "    \n",
    "    # 검증 (Validation)\n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list, desc=f\"Test Epoch {epoch + 1}\")\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_postfix(test_loss=f\"{test_loss.numpy() / (test_batch + 1):.4f}\")\n",
    "    \n",
    "    avg_val_loss = test_loss.numpy() / len(t)  # 검증 손실 평균\n",
    "\n",
    "    # 베스트 모델 체크포인트 저장\n",
    "    if avg_val_loss < best_val_loss:\n",
    "        best_val_loss = avg_val_loss\n",
    "        checkpoint_manager.save()\n",
    "        print(\"모델 체크포인트 저장 완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae861038",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6326b329",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f207e60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))\n",
    "\n",
    "\n",
    "translate(\"Can I have some coffee?\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1eab7201",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "\"\\n- 1. 번역 결과는 '>>>>>>>'과 같은 형태로 나옴\\n\\n- 2. 어텐션 스코어가 대부분 동일한 값을 가지고 있음\\n[0.03348084]\\n[0.03345907]\\n[0.03342958]\\n[0.03339833]\\n[0.03337143]\\n[0.03335103]\\n[0.03333669]\\n[0.03332708]\\n[0.03332083]\\n[0.03331683]\\n[0.03331429]\\n[0.03331268]\\n[0.03331165]\\n[0.03331099]\\n[0.03331057]\\n[0.0333103 ]\\n[0.03331012]...\\n- 3. 토크나이저 내부 단어집합을 살펴보니 특수토큰이 별도로 지정되지 않고 분리되어 있음\\n[(1, '<'),\\n(2, '>'),\\n(3, 'the'),\\n(4, ','),\\n(5, '.'),\\n(6, 'end'),\\n(7, 'start'),\\n ...\\n \\n - 취합하면 토크나이징 방법에 문제가 있을 수 있고 모델 구조도 함께 살펴볼 예정\\n \""
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 이슈사항 정리\n",
    "'''\n",
    "- 1. 번역 결과는 '>>>>>>>'과 같은 형태로 나옴\n",
    "\n",
    "- 2. 어텐션 스코어가 대부분 동일한 값을 가지고 있음\n",
    "[0.03348084]\n",
    "[0.03345907]\n",
    "[0.03342958]\n",
    "[0.03339833]\n",
    "[0.03337143]\n",
    "[0.03335103]\n",
    "[0.03333669]\n",
    "[0.03332708]\n",
    "[0.03332083]\n",
    "[0.03331683]\n",
    "[0.03331429]\n",
    "[0.03331268]\n",
    "[0.03331165]\n",
    "[0.03331099]\n",
    "[0.03331057]\n",
    "[0.0333103 ]\n",
    "[0.03331012]...\n",
    "- 3. 토크나이저 내부 단어집합을 살펴보니 특수토큰이 별도로 지정되지 않고 분리되어 있음\n",
    "[(1, '<'),\n",
    "(2, '>'),\n",
    "(3, 'the'),\n",
    "(4, ','),\n",
    "(5, '.'),\n",
    "(6, 'end'),\n",
    "(7, 'start'),\n",
    " ...\n",
    " \n",
    " - 취합하면 토크나이징 방법에 문제가 있을 수 있고 모델 구조도 함께 살펴볼 예정\n",
    " '''"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
