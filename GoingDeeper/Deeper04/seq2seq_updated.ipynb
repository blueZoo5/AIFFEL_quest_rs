{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c12d5b22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import matplotlib.ticker as ticker\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import re\n",
    "import os\n",
    "import io"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e10c0134",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "완료!\n"
     ]
    }
   ],
   "source": [
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "import matplotlib.font_manager as fm\n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"완료!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "0edec3fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train_data = 'korean-english-park.train.ko'\n",
    "dec_train_data = 'korean-english-park.train.en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a4d1bd27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 읽기\n",
    "def load_data(file_path):\n",
    "    with open(file_path, 'r', encoding='utf-8') as file:\n",
    "        data = file.readlines()\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cd83d409",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_data = load_data(enc_train_data)\n",
    "dec_data = load_data(dec_train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f79bb208",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "94123"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(enc_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b8862960",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 특수기호 확인하는 함수\n",
    "def find_special_char(data):\n",
    "    # 한글과 숫자를 제외한 특수문자만 찾는 정규표현식\n",
    "    pattern = r'[^가-힣0-9a-zA-Z\\s]'\n",
    "    \n",
    "    special_chars = []\n",
    "    \n",
    "    # 리스트의 각 항목에 대해 특수기호를 찾음\n",
    "    for text in data:\n",
    "        if isinstance(text, str):  # 문자열인 경우에만 처리\n",
    "            # 정규표현식을 통해 특수문자 추출\n",
    "            special_chars.extend(re.findall(pattern, text))\n",
    "    \n",
    "    return special_chars"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "ccd50cee",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['氷',\n",
       " '㎠',\n",
       " '地',\n",
       " '場',\n",
       " '修',\n",
       " '反',\n",
       " '平',\n",
       " 'や',\n",
       " '愛',\n",
       " '董',\n",
       " '天',\n",
       " '夫',\n",
       " '管',\n",
       " '㎝',\n",
       " '會',\n",
       " '料',\n",
       " '島',\n",
       " '火',\n",
       " '赤',\n",
       " '四',\n",
       " '佛',\n",
       " 'ㆍ',\n",
       " '~',\n",
       " '局',\n",
       " '戀',\n",
       " '丙',\n",
       " '淘',\n",
       " '英',\n",
       " '神',\n",
       " '故',\n",
       " '油',\n",
       " '兆',\n",
       " 'か',\n",
       " '者',\n",
       " '器',\n",
       " '察',\n",
       " '輔',\n",
       " '㈜',\n",
       " '節',\n",
       " '８',\n",
       " '店',\n",
       " '鼓',\n",
       " '種',\n",
       " '進',\n",
       " '國',\n",
       " '州',\n",
       " '吳',\n",
       " '手',\n",
       " '克',\n",
       " '幹',\n",
       " '\\uf0d7',\n",
       " '岩',\n",
       " '公',\n",
       " '㎞',\n",
       " '―',\n",
       " '電',\n",
       " '．',\n",
       " '輪',\n",
       " 'し',\n",
       " '習',\n",
       " '{',\n",
       " '瀋',\n",
       " '?',\n",
       " '蓄',\n",
       " '敎',\n",
       " '靑',\n",
       " '“',\n",
       " '强',\n",
       " '泰',\n",
       " '難',\n",
       " '紅',\n",
       " '對',\n",
       " 'ㄴ',\n",
       " '´',\n",
       " 'い',\n",
       " '億',\n",
       " '送',\n",
       " '蔣',\n",
       " '藻',\n",
       " 'é',\n",
       " '軍',\n",
       " '臺',\n",
       " '頂',\n",
       " '省',\n",
       " 'そ',\n",
       " '金',\n",
       " '３',\n",
       " '切',\n",
       " '上',\n",
       " '±',\n",
       " '頭',\n",
       " '^',\n",
       " '`',\n",
       " '言',\n",
       " 'て',\n",
       " ';',\n",
       " '銀',\n",
       " '２',\n",
       " '銅',\n",
       " '勝',\n",
       " '疫',\n",
       " '走',\n",
       " '燭',\n",
       " '室',\n",
       " '軌',\n",
       " '老',\n",
       " \"'\",\n",
       " '華',\n",
       " '光',\n",
       " 'す',\n",
       " '邸',\n",
       " '球',\n",
       " '命',\n",
       " '照',\n",
       " '之',\n",
       " '迷',\n",
       " '麻',\n",
       " '駐',\n",
       " '霧',\n",
       " '˝',\n",
       " '輿',\n",
       " '北',\n",
       " 'ま',\n",
       " '℃',\n",
       " '諡',\n",
       " '配',\n",
       " '效',\n",
       " '花',\n",
       " '濱',\n",
       " '蓋',\n",
       " '”',\n",
       " '毬',\n",
       " '!',\n",
       " '氣',\n",
       " '亞',\n",
       " ',',\n",
       " '#',\n",
       " '湖',\n",
       " '４',\n",
       " '買',\n",
       " '多',\n",
       " '峽',\n",
       " '時',\n",
       " '語',\n",
       " '毁',\n",
       " '‥',\n",
       " '中',\n",
       " '@',\n",
       " '父',\n",
       " '’',\n",
       " '男',\n",
       " '１',\n",
       " '新',\n",
       " '次',\n",
       " '李',\n",
       " '鄕',\n",
       " '屋',\n",
       " '因',\n",
       " '然',\n",
       " '路',\n",
       " '査',\n",
       " '海',\n",
       " '結',\n",
       " '懶',\n",
       " '懷',\n",
       " '奔',\n",
       " '私',\n",
       " '現',\n",
       " '街',\n",
       " '諸',\n",
       " '九',\n",
       " '正',\n",
       " '視',\n",
       " '朱',\n",
       " '足',\n",
       " '酸',\n",
       " '康',\n",
       " '楊',\n",
       " '東',\n",
       " '卍',\n",
       " '野',\n",
       " '子',\n",
       " '滑',\n",
       " '川',\n",
       " 'よ',\n",
       " '辛',\n",
       " '髮',\n",
       " '心',\n",
       " '植',\n",
       " 'ㅇ',\n",
       " '社',\n",
       " '鎔',\n",
       " '>',\n",
       " '傷',\n",
       " ')',\n",
       " '株',\n",
       " '産',\n",
       " '良',\n",
       " '〔',\n",
       " '５',\n",
       " 'ぷ',\n",
       " '仁',\n",
       " 'さ',\n",
       " '藥',\n",
       " '非',\n",
       " '羅',\n",
       " '胡',\n",
       " '通',\n",
       " '主',\n",
       " '酒',\n",
       " '善',\n",
       " '能',\n",
       " '*',\n",
       " '王',\n",
       " '煙',\n",
       " '纏',\n",
       " '團',\n",
       " '－',\n",
       " '識',\n",
       " '溫',\n",
       " '舊',\n",
       " '寶',\n",
       " '吉',\n",
       " '同',\n",
       " '全',\n",
       " 'り',\n",
       " '受',\n",
       " '魚',\n",
       " '費',\n",
       " '近',\n",
       " '空',\n",
       " '（',\n",
       " '代',\n",
       " '雲',\n",
       " '疑',\n",
       " '婦',\n",
       " '？',\n",
       " '慶',\n",
       " '錦',\n",
       " '的',\n",
       " '㎡',\n",
       " '三',\n",
       " '體',\n",
       " '雙',\n",
       " '郡',\n",
       " '泥',\n",
       " '後',\n",
       " '姓',\n",
       " '船',\n",
       " '劉',\n",
       " '假',\n",
       " '膚',\n",
       " '報',\n",
       " '區',\n",
       " '掌',\n",
       " '晴',\n",
       " '法',\n",
       " '居',\n",
       " '性',\n",
       " '醜',\n",
       " '琥',\n",
       " '題',\n",
       " '開',\n",
       " '一',\n",
       " 'ば',\n",
       " '雨',\n",
       " '領',\n",
       " '田',\n",
       " '.',\n",
       " '不',\n",
       " '塔',\n",
       " '案',\n",
       " '！',\n",
       " '身',\n",
       " '式',\n",
       " '誌',\n",
       " '群',\n",
       " '\"',\n",
       " '澤',\n",
       " '濠',\n",
       " '書',\n",
       " 'ぶ',\n",
       " '登',\n",
       " '㎾',\n",
       " '坤',\n",
       " '方',\n",
       " '+',\n",
       " '〕',\n",
       " '止',\n",
       " '影',\n",
       " '寺',\n",
       " '官',\n",
       " 'ℓ',\n",
       " '要',\n",
       " '˙',\n",
       " '發',\n",
       " '所',\n",
       " '不',\n",
       " '–',\n",
       " '港',\n",
       " '型',\n",
       " '(',\n",
       " '×',\n",
       " '航',\n",
       " '職',\n",
       " '宇',\n",
       " '石',\n",
       " '無',\n",
       " '像',\n",
       " '廣',\n",
       " '生',\n",
       " '學',\n",
       " '珀',\n",
       " '河',\n",
       " '大',\n",
       " '力',\n",
       " '靜',\n",
       " '遼',\n",
       " '建',\n",
       " '道',\n",
       " '福',\n",
       " '=',\n",
       " '\\U000d51d1',\n",
       " '墓',\n",
       " '頁',\n",
       " '林',\n",
       " '間',\n",
       " '異',\n",
       " '江',\n",
       " 'く',\n",
       " '衛',\n",
       " '-',\n",
       " '病',\n",
       " '動',\n",
       " '敵',\n",
       " '覺',\n",
       " '…',\n",
       " '焼',\n",
       " '本',\n",
       " '郞',\n",
       " '月',\n",
       " '芽',\n",
       " '聲',\n",
       " '日',\n",
       " '園',\n",
       " '品',\n",
       " '美',\n",
       " '$',\n",
       " '文',\n",
       " '柳',\n",
       " '貧',\n",
       " '外',\n",
       " '和',\n",
       " '字',\n",
       " 'き',\n",
       " '富',\n",
       " '明',\n",
       " '堀',\n",
       " '獨',\n",
       " '毛',\n",
       " '座',\n",
       " '浅',\n",
       " '商',\n",
       " '播',\n",
       " '理',\n",
       " '葬',\n",
       " '/',\n",
       " '邦',\n",
       " '環',\n",
       " '小',\n",
       " '女',\n",
       " '肢',\n",
       " '香',\n",
       " '馬',\n",
       " '戰',\n",
       " '棺',\n",
       " '談',\n",
       " 'ざ',\n",
       " '斌',\n",
       " '流',\n",
       " '信',\n",
       " '圓',\n",
       " '母',\n",
       " '·',\n",
       " '万',\n",
       " '京',\n",
       " '司',\n",
       " '僞',\n",
       " '敢',\n",
       " '▶',\n",
       " '灣',\n",
       " '聖',\n",
       " 'ら',\n",
       " '家',\n",
       " '}',\n",
       " '權',\n",
       " '前',\n",
       " '率',\n",
       " '基',\n",
       " '內',\n",
       " '麥',\n",
       " '城',\n",
       " '章',\n",
       " '鳥',\n",
       " '祭',\n",
       " '淺',\n",
       " '導',\n",
       " '席',\n",
       " '%',\n",
       " '‘',\n",
       " '親',\n",
       " '早',\n",
       " '度',\n",
       " '弗',\n",
       " '食',\n",
       " '換',\n",
       " '卿',\n",
       " '長',\n",
       " '•',\n",
       " '總',\n",
       " '鳩',\n",
       " '超',\n",
       " '旗',\n",
       " '詩',\n",
       " '愚',\n",
       " '號',\n",
       " '都',\n",
       " 'ん',\n",
       " '機',\n",
       " '在',\n",
       " '翔',\n",
       " '物',\n",
       " '率',\n",
       " '識',\n",
       " '耳',\n",
       " '名',\n",
       " '星',\n",
       " '市',\n",
       " '）',\n",
       " '常',\n",
       " '票',\n",
       " '突',\n",
       " '壽',\n",
       " '陳',\n",
       " '_',\n",
       " '孩',\n",
       " ']',\n",
       " '再',\n",
       " '加',\n",
       " '論',\n",
       " '龍',\n",
       " 'と',\n",
       " '湛',\n",
       " '獸',\n",
       " '喩',\n",
       " '曾',\n",
       " '死',\n",
       " '兵',\n",
       " '精',\n",
       " '愁',\n",
       " '質',\n",
       " '掃',\n",
       " '年',\n",
       " '畵',\n",
       " '[',\n",
       " '晉',\n",
       " '醒',\n",
       " '▲',\n",
       " '庫',\n",
       " '殘',\n",
       " '古',\n",
       " '奴',\n",
       " '〉',\n",
       " '促',\n",
       " '置',\n",
       " '門',\n",
       " '陽',\n",
       " '印',\n",
       " ':',\n",
       " '備',\n",
       " '類',\n",
       " '水',\n",
       " '紙',\n",
       " '山',\n",
       " '沈',\n",
       " '價',\n",
       " '濤',\n",
       " '眞',\n",
       " '民',\n",
       " '兒',\n",
       " '&',\n",
       " '寧',\n",
       " '築',\n",
       " '安',\n",
       " '㎢',\n",
       " '草',\n",
       " '族',\n",
       " '〈',\n",
       " '占',\n",
       " '交',\n",
       " '賣']"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(find_special_char(enc_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "841d41a1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['½',\n",
       " '\\\\',\n",
       " 'Î',\n",
       " '˝',\n",
       " 'ç',\n",
       " 'ó',\n",
       " '℃',\n",
       " '¸',\n",
       " '·',\n",
       " '”',\n",
       " 'µ',\n",
       " '°',\n",
       " '!',\n",
       " ',',\n",
       " 'Û',\n",
       " '#',\n",
       " '~',\n",
       " '<',\n",
       " '.',\n",
       " 'â',\n",
       " '\"',\n",
       " 'Ñ',\n",
       " '¥',\n",
       " '%',\n",
       " '‘',\n",
       " '+',\n",
       " '@',\n",
       " '’',\n",
       " '—',\n",
       " '»',\n",
       " '–',\n",
       " 'ñ',\n",
       " 'Ù',\n",
       " 'Á',\n",
       " '(',\n",
       " '®',\n",
       " '―',\n",
       " 'ö',\n",
       " 'À',\n",
       " '?',\n",
       " '“',\n",
       " '¦',\n",
       " 'º',\n",
       " '¯',\n",
       " '´',\n",
       " 'ø',\n",
       " 'Ç',\n",
       " '=',\n",
       " '_',\n",
       " ']',\n",
       " '-',\n",
       " '>',\n",
       " '¡',\n",
       " ')',\n",
       " '¹',\n",
       " '…',\n",
       " '[',\n",
       " '¿',\n",
       " 'Ï',\n",
       " '∼',\n",
       " '±',\n",
       " '^',\n",
       " '`',\n",
       " '$',\n",
       " 'Ê',\n",
       " ';',\n",
       " ':',\n",
       " '*',\n",
       " '¨',\n",
       " '£',\n",
       " 'æ',\n",
       " \"'\",\n",
       " 'û',\n",
       " '&',\n",
       " '¾',\n",
       " '¢',\n",
       " '/',\n",
       " 'Â']"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "list(set(find_special_char(dec_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "517d25aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "\n",
    "    sentence = re.sub(r\"([?.!,])\", r\" \\1 \", sentence)\n",
    "    sentence = re.sub(r'[\" \"]+', \" \", sentence)\n",
    "    sentence = re.sub(r\"[^가-힣0-9a-zA-Z?.!,]+\", \" \", sentence)\n",
    "\n",
    "    sentence = sentence.strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e67c8908",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "한국어: 제 23차 연례 컴덱스 박람회의 개회사를 한 케이츠는 2년여전 기술 산업의 거품이 붕괴된 이후에 첨단 기술에 대해 부정적인 인식이 있다고 말했다 .\n",
      "영어: Gates , who opened the 23rd annual Comdex trade show , said there was a negative perception of high tech following the collapse of the tech bubble about two years ago .\n"
     ]
    }
   ],
   "source": [
    "enc_corpus = []\n",
    "dec_corpus = []\n",
    "\n",
    "num_examples = 30000\n",
    "\n",
    "for kor in enc_data[:num_examples]:\n",
    "    enc_corpus.append(preprocess_sentence(kor))\n",
    "    \n",
    "for eng in dec_data[:num_examples]:\n",
    "    dec_corpus.append(preprocess_sentence(eng))\n",
    "\n",
    "print(\"한국어:\", enc_corpus[100])\n",
    "print(\"영어:\", dec_corpus[100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "7525a1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.preprocessing.text import Tokenizer\n",
    "from tensorflow.keras.preprocessing.sequence import pad_sequences\n",
    "import MeCab"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "291c255e",
   "metadata": {},
   "outputs": [],
   "source": [
    "mecab = MeCab.Tagger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d6992bd3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 한국어 토크나이저\n",
    "def encoder_tokenizer(encoder_texts):\n",
    "\n",
    "    tokenized_texts = [\" \".join(mecab.parse(sentence)) for sentence in encoder_texts]\n",
    "\n",
    "    # Keras Tokenizer 생성 및 학습\n",
    "    enc_tokenizer = Tokenizer(oov_token=\"<OOV>\")\n",
    "    enc_tokenizer.fit_on_texts(tokenized_texts)\n",
    "\n",
    "    # 시퀀스로 변환\n",
    "    encoder_sequences = enc_tokenizer.texts_to_sequences(tokenized_texts)\n",
    "\n",
    "    # 패딩 적용\n",
    "    encoder_input = pad_sequences(encoder_sequences, padding=\"post\")\n",
    "\n",
    "    return encoder_input, enc_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "f40cdb17",
   "metadata": {},
   "outputs": [],
   "source": [
    "#영어 토크나이저\n",
    "def decoder_tokenizer(decoder_texts):\n",
    "\n",
    "    START_TOKEN = \"<start>\"\n",
    "    END_TOKEN = \"<end>\"\n",
    "\n",
    "    # 문장별 토큰화 및 특수 토큰 추가\n",
    "    tokenized_texts = [f\"{START_TOKEN} {sentence.lower()} {END_TOKEN}\" for sentence in decoder_texts]\n",
    "\n",
    "    # Keras Tokenizer 생성 및 학습\n",
    "    dec_tokenizer = Tokenizer(oov_token=\"<OOV>\", filters='')\n",
    "    dec_tokenizer.fit_on_texts(tokenized_texts)\n",
    "\n",
    "    # 시퀀스로 변환\n",
    "    decoder_sequences = dec_tokenizer.texts_to_sequences(tokenized_texts)\n",
    "\n",
    "    # 패딩 적용\n",
    "    decoder_input = pad_sequences(decoder_sequences, padding=\"post\")\n",
    "\n",
    "    return decoder_input, dec_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42e015e5",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoder Input Tensor: [[15 16  2  2  6 17 18  7 15 16 19 20 11 12  3 19 21  8  4  9  4  3  3 21\n",
      "   8 35  2  3 36  4 22  7  4  9  4  3 37  4  9  5  8  4  3 23 24 12 25  7\n",
      "  23 24 26 27 28  4  3  3 26 27 28  4 10 11]\n",
      " [13 14  5  2  2  6  3 13 14  5 22 10 38  9 10 39  2 40 13 14  2  2  6  5\n",
      "   2  2  6 29 30  2  2  6 17 18  3 29 30 31 41 20  7 31 32 33 34 12 25  7\n",
      "  32 33 34  5  8  4  3  3  5  8  4 10 11  0]]\n",
      "Decoder Input Tensor: [[ 2  4  5  6  7  8  3]\n",
      " [ 2  9 10 11  3  0  0]]\n"
     ]
    }
   ],
   "source": [
    "# 예제 데이터\n",
    "encoder_texts = [\"안녕하세요 반갑습니다\", \"자연어 처리는 재미있어요\"]\n",
    "decoder_texts = [\"hello nice to meet you\", \"nlp is fun\"]\n",
    "\n",
    "# 토크나이저 실행\n",
    "encoder_input, encoder_vocab = encoder_tokenizer(encoder_texts)\n",
    "decoder_input, decoder_vocab = decoder_tokenizer(decoder_texts)\n",
    "\n",
    "print(\"Encoder Input Tensor:\", encoder_input)\n",
    "print(\"Decoder Input Tensor:\", decoder_input)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "8157ca26",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "enc_corpus, enc_tokenizer = encoder_tokenizer(enc_corpus)\n",
    "dec_corpus, dec_tokenizer = decoder_tokenizer(dec_corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b8e4bb60",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<keras_preprocessing.text.Tokenizer at 0x7d85e73ae4c0>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "c1cd1b0a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([145,  27,   2, ...,   0,   0,   0], dtype=int32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "enc_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "15c0baa1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<end>'"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_tokenizer.index_word[6]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "e0012b44",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([   5,  217,    8, 1231, 5860,   17,   43,   90,   73,  253,   47,\n",
       "        341,    6,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0,    0,    0,    0,    0,    0,    0,\n",
       "          0,    0,    0,    0,    0], dtype=int32)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dec_corpus[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1af49e4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "enc_train, enc_val, dec_train, dec_val = train_test_split(enc_corpus, dec_corpus, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "022a17c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class BahdanauAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, units):\n",
    "        super(BahdanauAttention, self).__init__()\n",
    "        self.w_dec = tf.keras.layers.Dense(units)\n",
    "        self.w_enc = tf.keras.layers.Dense(units)\n",
    "        self.w_com = tf.keras.layers.Dense(1)\n",
    "    \n",
    "    def call(self, h_enc, h_dec):\n",
    "        # h_enc shape: [batch x length x units]\n",
    "        # h_dec shape: [batch x units]\n",
    "\n",
    "        h_enc = self.w_enc(h_enc)\n",
    "        h_dec = tf.expand_dims(h_dec, 1)\n",
    "        h_dec = self.w_dec(h_dec)\n",
    "\n",
    "        score = self.w_com(tf.nn.tanh(h_dec + h_enc))\n",
    "        \n",
    "        attn = tf.nn.softmax(score, axis=1)\n",
    "\n",
    "        context_vec = attn * h_enc\n",
    "        context_vec = tf.reduce_sum(context_vec, axis=1)\n",
    "\n",
    "        return context_vec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "04e7018f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, enc_units):\n",
    "        super(Encoder, self).__init__()\n",
    "        \n",
    "        self.enc_units = enc_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(enc_units,\n",
    "                                       return_sequences=True)\n",
    "        \n",
    "    def call(self, x):\n",
    "        out = self.embedding(x)\n",
    "        out = self.gru(out)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "666f59a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self, vocab_size, embedding_dim, dec_units):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.dec_units = dec_units\n",
    "        self.embedding = tf.keras.layers.Embedding(vocab_size, embedding_dim)\n",
    "        self.gru = tf.keras.layers.GRU(dec_units,\n",
    "                                       return_sequences=True,\n",
    "                                       return_state=True)\n",
    "        self.fc = tf.keras.layers.Dense(vocab_size)\n",
    "\n",
    "        self.attention = BahdanauAttention(self.dec_units)\n",
    "\n",
    "    def call(self, x, h_dec, enc_out):\n",
    "        context_vec, attn = self.attention(enc_out, h_dec)\n",
    "\n",
    "        out = self.embedding(x)\n",
    "        out = tf.concat([tf.expand_dims(context_vec, 1), out], axis=-1)\n",
    "        \n",
    "        out, h_dec = self.gru(out)\n",
    "        out = tf.reshape(out, (-1, out.shape[2]))\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out, h_dec, attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "032e8438",
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE     = 16\n",
    "SRC_VOCAB_SIZE = len(enc_tokenizer.index_word) + 1\n",
    "TGT_VOCAB_SIZE = len(dec_tokenizer.index_word) + 1\n",
    "\n",
    "units         = 512\n",
    "embedding_dim = 256\n",
    "\n",
    "encoder = Encoder(SRC_VOCAB_SIZE, embedding_dim, units)\n",
    "decoder = Decoder(TGT_VOCAB_SIZE, embedding_dim, units)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "a6395894",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss = loss_object(real, pred)\n",
    "    \n",
    "    mask = tf.cast(mask, dtype=loss.dtype)\n",
    "    loss *= mask\n",
    "    \n",
    "    return tf.reduce_mean(loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "bc798dee",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def train_step(src, tgt, encoder, decoder, optimizer, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        enc_out = encoder(src)\n",
    "        h_dec = enc_out[:, -1]\n",
    "        \n",
    "        dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "        for t in range(1, tgt.shape[1]):\n",
    "            pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "            loss += loss_function(tgt[:, t], pred)\n",
    "            dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "\n",
    "    variables = encoder.trainable_variables + decoder.trainable_variables\n",
    "    gradients = tape.gradient(loss, variables)\n",
    "    optimizer.apply_gradients(zip(gradients, variables))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "71747621",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "92ad85d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "@tf.function\n",
    "def eval_step(src, tgt, encoder, decoder, dec_tok):\n",
    "    bsz = src.shape[0]\n",
    "    loss = 0\n",
    "\n",
    "    enc_out = encoder(src)\n",
    "\n",
    "    h_dec = enc_out[:, -1]\n",
    "    \n",
    "    dec_src = tf.expand_dims([dec_tok.word_index['<start>']] * bsz, 1)\n",
    "\n",
    "    for t in range(1, tgt.shape[1]):\n",
    "        pred, h_dec, _ = decoder(dec_src, h_dec, enc_out)\n",
    "\n",
    "        loss += loss_function(tgt[:, t], pred)\n",
    "        dec_src = tf.expand_dims(tgt[:, t], 1)\n",
    "        \n",
    "    batch_loss = (loss / int(tgt.shape[1]))\n",
    "    \n",
    "    return batch_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2c2662c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch  1: 100%|██████████| 1500/1500 [39:48<00:00,  1.59s/it, Loss 2.1376] \n",
      "Test Epoch  1: 100%|██████████| 375/375 [03:31<00:00,  1.77it/s, Test Loss 2.1375]\n",
      "Epoch  2: 100%|██████████| 1500/1500 [37:24<00:00,  1.50s/it, Loss 2.1147]\n",
      "Test Epoch  2: 100%|██████████| 375/375 [03:00<00:00,  2.07it/s, Test Loss 2.1514]\n",
      "Epoch  3: 100%|██████████| 1500/1500 [37:25<00:00,  1.50s/it, Loss 2.1152]\n",
      "Test Epoch  3: 100%|██████████| 375/375 [03:01<00:00,  2.07it/s, Test Loss 2.1629]\n",
      "Epoch  4: 100%|██████████| 1500/1500 [37:24<00:00,  1.50s/it, Loss 2.1152]\n",
      "Test Epoch  4: 100%|██████████| 375/375 [03:01<00:00,  2.07it/s, Test Loss 2.1568]\n",
      "Epoch  5: 100%|██████████| 1500/1500 [37:23<00:00,  1.50s/it, Loss 2.1151]\n",
      "Test Epoch  5:  65%|██████▌   | 244/375 [01:58<01:03,  2.06it/s, Test Loss 2.1747]"
     ]
    }
   ],
   "source": [
    "EPOCHS = 5\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss = train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                                dec_train[idx:idx+BATCH_SIZE],\n",
    "                                encoder,\n",
    "                                decoder,\n",
    "                                optimizer,\n",
    "                                dec_tokenizer)\n",
    "    \n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "    \n",
    "    test_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_val.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm(idx_list)\n",
    "\n",
    "    for (test_batch, idx) in enumerate(t):\n",
    "        test_batch_loss = eval_step(enc_val[idx:idx+BATCH_SIZE],\n",
    "                                    dec_val[idx:idx+BATCH_SIZE],\n",
    "                                    encoder,\n",
    "                                    decoder,\n",
    "                                    dec_tokenizer)\n",
    "    \n",
    "        test_loss += test_batch_loss\n",
    "\n",
    "        t.set_description_str('Test Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Test Loss %.4f' % (test_loss.numpy() / (test_batch + 1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f3aaca2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, encoder, decoder):\n",
    "    attention = np.zeros((dec_train.shape[-1], enc_train.shape[-1]))\n",
    "    \n",
    "    sentence = preprocess_sentence(sentence)\n",
    "    inputs = enc_tokenizer.texts_to_sequences([sentence.split()])\n",
    "    inputs = tf.keras.preprocessing.sequence.pad_sequences(inputs,\n",
    "                                                           maxlen=enc_train.shape[-1],\n",
    "                                                           padding='post')\n",
    "\n",
    "    result = ''\n",
    "\n",
    "    enc_out = encoder(inputs)\n",
    "\n",
    "    dec_hidden = enc_out[:, -1]\n",
    "    dec_input = tf.expand_dims([dec_tokenizer.word_index['<start>']], 0)\n",
    "\n",
    "    for t in range(dec_train.shape[-1]):\n",
    "        predictions, dec_hidden, attention_weights = decoder(dec_input,\n",
    "                                                             dec_hidden,\n",
    "                                                             enc_out)\n",
    "\n",
    "        attention_weights = tf.reshape(attention_weights, (-1, ))\n",
    "        attention[t] = attention_weights.numpy()\n",
    "\n",
    "        predicted_id = \\\n",
    "        tf.argmax(tf.math.softmax(predictions, axis=-1)[0]).numpy()\n",
    "\n",
    "        result += dec_tokenizer.index_word[predicted_id] + ' '\n",
    "\n",
    "        if dec_tokenizer.index_word[predicted_id] == '<end>':\n",
    "            return result, sentence, attention\n",
    "\n",
    "        dec_input = tf.expand_dims([predicted_id], 0)\n",
    "\n",
    "    return result, sentence, attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9e3a43d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_attention(attention, sentence, predicted_sentence):\n",
    "    fig = plt.figure(figsize=(10,10))\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    ax.matshow(attention, cmap='viridis')\n",
    "\n",
    "    fontdict = {'fontsize': 14}\n",
    "\n",
    "    ax.set_xticklabels([''] + sentence, fontdict=fontdict, rotation=90)\n",
    "    ax.set_yticklabels([''] + predicted_sentence, fontdict=fontdict)\n",
    "\n",
    "    ax.xaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "    ax.yaxis.set_major_locator(ticker.MultipleLocator(1))\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "def translate(sentence, encoder, decoder):\n",
    "    result, sentence, attention = evaluate(sentence, encoder, decoder)\n",
    "\n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "    \n",
    "    attention = attention[:len(result.split()), :len(sentence.split())]\n",
    "    plot_attention(attention, sentence.split(), result.split(' '))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ba3c952",
   "metadata": {},
   "outputs": [],
   "source": [
    "translate(\"일곱 명의 사망자가 발생했다.\", encoder, decoder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00bff37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 이슈사항 정리\n",
    "- 1. 번역 결과는 '>>>>>>>'과 같은 형태로 나옴\n",
    "\n",
    "- 2. 어텐션 스코어가 대부분 동일한 값을 가지고 있음\n",
    "[0.03348084]\n",
    "[0.03345907]\n",
    "[0.03342958]\n",
    "[0.03339833]\n",
    "[0.03337143]\n",
    "[0.03335103]\n",
    "[0.03333669]\n",
    "[0.03332708]\n",
    "[0.03332083]\n",
    "[0.03331683]\n",
    "[0.03331429]\n",
    "[0.03331268]\n",
    "[0.03331165]\n",
    "[0.03331099]\n",
    "[0.03331057]\n",
    "[0.0333103 ]\n",
    "[0.03331012]...\n",
    "- 3. 토크나이저 내부 단어집합을 살펴보니 특수토큰이 별도로 지정되지 않고 분리되어 있음\n",
    "[(1, '<'),\n",
    "(2, '>'),\n",
    "(3, 'the'),\n",
    "(4, ','),\n",
    "(5, '.'),\n",
    "(6, 'end'),\n",
    "(7, 'start'),\n",
    " ...\n",
    " \n",
    " - 취합하면 토크나이징 방법에 문제가 있을 수 있고 모델 구조도 함께 살펴볼 예정"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
