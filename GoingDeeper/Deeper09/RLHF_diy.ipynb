{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4f013233",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\n!pip install colossalai==0.2.7\\n\\n!git clone https://github.com/airobotlab/KoChatGPT.git\\n\\n%cd KoChatGPT/colossalai_ChatGPT_230319/\\n!pip install .\\n\\n# 설치 확인\\ntry:\\n    from chatgpt.models.base import RewardModel  # 예시로 임포트 테스트\\n    print(\"chatgpt 모듈 설치 성공!\")\\nexcept ImportError as e:\\n    print(\"chatgpt 모듈 설치 실패:\", e)\\n    '"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''\n",
    "!pip install colossalai==0.2.7\n",
    "\n",
    "!git clone https://github.com/airobotlab/KoChatGPT.git\n",
    "\n",
    "%cd KoChatGPT/colossalai_ChatGPT_230319/\n",
    "!pip install .\n",
    "\n",
    "# 설치 확인\n",
    "try:\n",
    "    from chatgpt.models.base import RewardModel  # 예시로 임포트 테스트\n",
    "    print(\"chatgpt 모듈 설치 성공!\")\n",
    "except ImportError as e:\n",
    "    print(\"chatgpt 모듈 설치 실패:\", e)\n",
    "    '''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c77298",
   "metadata": {},
   "source": [
    "### 1. 데이터 확인"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "86e8262c",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Special tokens have been added in the vocabulary, make sure the associated word embeddings are fine-tuned or trained.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "import pandas as pd\n",
    "import numpy\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model_name = \"skt/kogpt2-base-v2\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "bccb2ef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab1841c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'gpt2': 1024,\n",
       " 'gpt2-medium': 1024,\n",
       " 'gpt2-large': 1024,\n",
       " 'gpt2-xl': 1024,\n",
       " 'distilgpt2': 1024}"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.max_model_input_sizes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "43b8cbd9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '불고기용 고기 한우에요?',\n",
       "  'completion': \"'저는 인공지능 챗봇이며, 직접적으로 식품에 관한 정보를 가지고 있지 않습니다. 하지만 일반적으로 불고기용 고기는 한우, 쇠고기, 돼지고기 등 다양한 종류의 고기를 사용합니다. 하지만 한우는 대표적인 고급 육류로 알려져 있기 때문에, 한우를 사용하는 경우도 많습니다. 알러지나 개별 건강 상태에 따라 다를 수 있으니 충분한 정보 수집 후에 선택해 주시기 바랍니다.\",\n",
       "  'tokens': 193},\n",
       " {'prompt': '쓰던 앱이 유료로 전환됐어',\n",
       "  'completion': \"'어떤 앱인지 모르기 때문에 정확한 답변을 드리기 어렵습니다. 하지만, 일반적으로 유료 전환된 앱은 기존 무료 앱에서 추가적인 기능이나 서비스를 제공하는 경우가 많습니다. 이 경우, 유료 전환 이전 가격이 매우 저렴하거나 무료인 경우가 많으므로, 앱을 계속 사용하려면 추가적인 비용을 지불해야 할 수 있습니다. 하지만, 유료 전환 이후에는 기존 사용자 또는 새로운 사용자 모두가 추가 기능과 높은 품질의 서비스를 이용할 수 있게 됩니다. 따라서, 앱 개발자는 유료 모델로 전환함으로써 앱의 수익을 증가시키고 더 나은 서비스를 제공할 수 있습니다.\",\n",
       "  'tokens': 288},\n",
       " {'prompt': '여친이랑 다툼',\n",
       "  'completion': \"'저는 인공지능이기 때문에 여친과의 다툼에 대해서는 조언을 드릴 수는 없습니다. 하지만 다투게 되면 상대방의 말을 경청하는 태도와 서로의 감정을 존중하는 것이 중요합니다. 논리적이고 차분한 대화를 통해 문제를 해결해보세요. 그리고 서로를 이해하고 서로의 의견을 수용하는 것이 중요합니다.\",\n",
       "  'tokens': 153}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json \n",
    "data_path_1_SFT = './data_kochatgpt/kochatgpt_1_SFT.jsonl' \n",
    "with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "19715897",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "12000\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'prompt': '번디는 자신이 탐정잡지, 범죄소설 그리고 성범죄 관련 실제 범죄 다큐멘터리들을 탐독했다고 누구에게 말했나?'},\n",
       " {'prompt': '개포주공아파트는 몇 단지로 이루어져 있나?'},\n",
       " {'prompt': '김영삼의 후보 시절 지역표심을 겨냥한 발언을 문제삼은 후보는?'}]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_path_3_PPO = './data_kochatgpt/kochatgpt_3_PPO.jsonl'\n",
    "with open(data_path_3_PPO, \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "print(len(list_data_dict))\n",
    "list_data_dict[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aab5ad38",
   "metadata": {},
   "source": [
    "### 2. SFT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "aff08693",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.optim import Adam\n",
    "from datasets import load_dataset\n",
    "import transformers\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, pipeline\n",
    "from transformers import Trainer, TrainingArguments\n",
    "from copy import deepcopy\n",
    "import copy\n",
    "import logging\n",
    "import json\n",
    "from dataclasses import dataclass\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0e7d7e7c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GPT2TokenizerFast(name_or_path='skt/kogpt2-base-v2', vocab_size=51200, model_max_length=512, is_fast=True, padding_side='right', truncation_side='right', special_tokens={'bos_token': '</s>', 'eos_token': '</s>', 'unk_token': '</s>', 'pad_token': '</s>'}, clean_up_tokenization_spaces=True)\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "print(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "842936b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from typing import Optional, Dict, Sequence\n",
    "\n",
    "class SFT_dataset(Dataset):\n",
    "\n",
    "    def __init__(self, data_path_1_SFT: str, tokenizer: transformers.PreTrainedTokenizer, verbose=False):\n",
    "        super(SFT_dataset, self).__init__()\n",
    "        logging.warning(\"Loading data...\")\n",
    "\n",
    "        pattern_instruction = 'prompt'  # instruction\n",
    "        pattern_output = 'completion'  # response\n",
    "\n",
    "        with open(data_path_1_SFT, \"r\", encoding='utf-8-sig') as json_file:\n",
    "            list_data_dict = json.load(json_file)\n",
    "\n",
    "        PROMPT_DICT = {\n",
    "            \"prompt_input\": (\n",
    "                \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "            )\n",
    "        }\n",
    "\n",
    "        prompt_input = PROMPT_DICT[\"prompt_input\"]\n",
    "\n",
    "        sources = []\n",
    "        for example in list_data_dict:\n",
    "            tmp = prompt_input.format_map(example)\n",
    "            sources.append(tmp)\n",
    "\n",
    "        targets = []\n",
    "        for example in list_data_dict:\n",
    "            targets.append(f\"{example[pattern_output]}{tokenizer.eos_token}\")\n",
    "        examples = [s + t for s, t in zip(sources, targets)]\n",
    "\n",
    "        sources_tokenized = self._tokenize_fn(sources, tokenizer)  # source\n",
    "        examples_tokenized = self._tokenize_fn(examples, tokenizer)  # source + target\n",
    "\n",
    "        input_ids = examples_tokenized[\"input_ids\"]\n",
    "        labels = copy.deepcopy(input_ids)\n",
    "        for label, source_len in zip(labels, sources_tokenized[\"input_ids_lens\"]):\n",
    "            label[:source_len] = -100\n",
    "\n",
    "        data_dict = dict(input_ids=input_ids, labels=labels)\n",
    "\n",
    "        self.input_ids = data_dict[\"input_ids\"]\n",
    "        self.labels = data_dict[\"labels\"]\n",
    "        logging.warning(\"Loading data done!!: %d\"%(len(self.labels)))\n",
    "\n",
    "\n",
    "    def _tokenize_fn(self, strings: Sequence[str], tokenizer: transformers.PreTrainedTokenizer) -> Dict:\n",
    "        tokenized_list = [\n",
    "            tokenizer(\n",
    "                text,\n",
    "                return_tensors=\"pt\",\n",
    "                padding=\"longest\",\n",
    "                max_length=tokenizer.model_max_length,\n",
    "                truncation=True,\n",
    "            )\n",
    "            for text in strings\n",
    "        ]\n",
    "        input_ids = labels = [tokenized.input_ids[0] for tokenized in tokenized_list]\n",
    "        input_ids_lens = labels_lens = [\n",
    "            tokenized.input_ids.ne(tokenizer.pad_token_id).sum().item() for tokenized in tokenized_list\n",
    "        ]\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            input_ids_lens=input_ids_lens,\n",
    "            labels_lens=labels_lens,\n",
    "        )\n",
    "\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.input_ids)\n",
    "\n",
    "\n",
    "    def __getitem__(self, i) -> Dict[str, torch.Tensor]:\n",
    "        return dict(input_ids=self.input_ids[i], labels=self.labels[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8c857e36",
   "metadata": {},
   "outputs": [],
   "source": [
    "@dataclass\n",
    "class DataCollatorForSupervisedDataset(object): \n",
    "\n",
    "    tokenizer: transformers.PreTrainedTokenizer\n",
    "\n",
    "    def __call__(self, instances: Sequence[Dict]) -> Dict[str, torch.Tensor]:\n",
    "        input_ids, labels = tuple([instance[key] for instance in instances] for key in (\"input_ids\", \"labels\"))\n",
    "        input_ids = torch.nn.utils.rnn.pad_sequence(\n",
    "            input_ids, batch_first=True, padding_value=self.tokenizer.pad_token_id\n",
    "        )\n",
    "        labels = torch.nn.utils.rnn.pad_sequence(labels, batch_first=True, padding_value= -100)\n",
    "        return dict(\n",
    "            input_ids=input_ids,\n",
    "            labels=labels,\n",
    "            attention_mask=input_ids.ne(self.tokenizer.pad_token_id),\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4da385ed",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Loading data...\n",
      "WARNING:root:Loading data done!!: 12000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input : tensor([  739,   378,   378,   378, 14659, 13394, 37091, 10651,   383, 25841,\n",
      "         8006, 14914,   375,  7673, 20479,  8091, 22311,  9036, 30902, 13675,\n",
      "          375,   378,   378,   378, 41951,   454,  9549, 20549,   383,  8142,\n",
      "         7192, 14914,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n",
      "output: tensor([ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
      "         -100,  -100,   382, 37767, 13753,  8263,  7166,   739,  8352,  7659,\n",
      "         9594, 25585, 13600,  8022,  9378, 11532,  9887, 11218,  9111, 16691,\n",
      "        10351, 10561,  9128, 20479,  8091,  9065,  9446,  9036, 28420, 26521,\n",
      "        10163, 26367,  6958,  9030,  9882, 12317, 25882,  9209, 37194, 10351,\n",
      "         9036, 12168, 10529, 15989,  9719, 15434, 10552, 11188, 13362,  9036,\n",
      "        15805, 11300, 11846,  9146, 16691,  9181,  7397, 15806, 13480, 11342,\n",
      "        17596,  9161, 19996,  9025, 25006, 18595,  9966, 12592, 10751, 11814,\n",
      "         8711,  9046, 12450,  9117,  7377, 12521,     1])\n"
     ]
    }
   ],
   "source": [
    "train_dataset = SFT_dataset(data_path_1_SFT='./data_kochatgpt/kochatgpt_1_SFT.jsonl', tokenizer=tokenizer)\n",
    "data_collator = DataCollatorForSupervisedDataset(tokenizer=tokenizer)\n",
    "\n",
    "print('input : %s'%train_dataset.input_ids[0])\n",
    "print('output: %s'%train_dataset.labels[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d9d06018",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./test_output\",\n",
    "    overwrite_output_dir=True,\n",
    "    num_train_epochs=3,\n",
    "    per_device_train_batch_size=8,\n",
    "    per_device_eval_batch_size=8,\n",
    "    warmup_steps=5,\n",
    "    prediction_loss_only=True,\n",
    "    fp16 = True\n",
    "    )\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=train_dataset\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c5a559f9",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='4500' max='4500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4500/4500 18:49, Epoch 3/3]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>3.007800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1000</td>\n",
       "      <td>2.850000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1500</td>\n",
       "      <td>2.795700</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2000</td>\n",
       "      <td>2.251900</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2500</td>\n",
       "      <td>2.262400</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3000</td>\n",
       "      <td>2.240000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3500</td>\n",
       "      <td>1.827800</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4000</td>\n",
       "      <td>1.840500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4500</td>\n",
       "      <td>1.823100</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "trainer.train()\n",
    "model.save_pretrained('output_1_SFT')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "cac0b02d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAmEAAAFNCAYAAABIc7ibAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjQuMywgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/MnkTPAAAACXBIWXMAAAsTAAALEwEAmpwYAAA4lUlEQVR4nO3deXxU1f3/8deHsIRNFkFUtoBaEAGhxBW1qJW6fatttaJBxaVItVJXEHHBBaSKota64IbWqFi3n1Wrtoql1boERUAQRRYBURZFQJD18/vj3MgQJiEJmdyZzPv5eMwjd+49M/M5mVTePefec83dEREREZHqVSvuAkRERESykUKYiIiISAwUwkRERERioBAmIiIiEgOFMBEREZEYKISJiIiIxEAhTERSxsyuNLMHqrqtiEhNoBAmkmHM7BAze9vMvjOzb8zsLTPbLzo2wMw2mdnqhMddJZ5vNrO1Cc8LSrz/PxKObTCz9QnP761Ire4+yt3Preq2FWVmbmZ7puK9y/HZx5vZe2b2vZktN7NCM2tTjZ9/gplNMbOVZrbMzN4wsw7RsRFm9lh11SIiW6sddwEiUn5mthPwIvB74CmgLnAosC6h2f/c/ZASL/1DwnvMA851938l+wx3Pyah7XhgobtflaSW2u6+sXI9yQ5mdhLwEDAIeA5oAowC/mtmPd392yr8rG2+jyh4Pgr8GngDaAT0BTZV1eeKSOVpJEwks/wEwN2fcPdN7r7W3V9z96nV8eHRiNIFZvYZ8Fm07w4zWxCNtEw2s0MT2v840mJmedHrzzSzL6JRmeGVbFvfzB4xs2/NbKaZDTGzhZXoTxMze9TMlprZfDO7ysxqRcf2NLN/RyOOy8xsQrTfzGysmS2J+jzNzLomeW8DbgVudPfHo+/qK+BcYDVwsZnVM7MVia83s5bRSOUu0fPjo5GsFdEIaPeEtvPMbKiZTQW+N7OS/8e6BzDX3V/3YJW7P+PuX5jZ0cCVwCnRKOdHCb+TB81ssZktMrMbzSwnOjbAwsjrXdHv5RMzOzKhngFmNsfMVpnZ3JKjrCKyNYUwkczyKbApCiDHmFmzGGo4ETgA6BI9f5/wj31z4HHgb2aWW8brDwE6AUcC15jZ3pVoey2QB3QEjgL6V6IfAH8mjE51BH4GnAGcFR27AXgNaAa0idpCGEk6jBCImwC/BZYnee9OQDvgb4k73X0z8AxwlLuvA54FTk1o8lvg3+6+xMx6EkbSzgN2Bu4DXjCzegntTwWOA5omGZn8AOgchcbDzaxRQh2vEEblJrh7I3ffNzo0HtgI7An0jPqbOE18APA50ILwPTxrZs3NrCFwJ3CMuzcGDgamJPm9iEhEIUwkg7j7SkIwceB+YKmZvWBmrRKaHRiNmhQ/DqziMm5y92/cfW1U02PuvtzdN7r7rUA9QgApzXXRqNBHwEfAvpVo+1tglLt/6+4LCf/4V0g0utMPGBaNEM0jjFydHjXZALQHdnf3H9z9vwn7GwOdAXP3me6+OMlHtIh+Jju2OOH441EdxU6L9gEMBO5z93ejkc9HCFPPid/pne6+oPj7SOTuc4A+QGvC9PUyMxufGMYSRX9HxwIXufv37r4EGFuiviXA7e6+wd0nALMIIRBgM9DVzOq7+2J3/zjZ54hIoBAmkmGif/QHuHsboCuwO3B7QpN33L1pwuOdKi5hQeITM7ssmhL8zsxWEEaHWiR9ZfBVwvYawnlKFW27e4k6tqqpnFoAdYD5CfvmEwILwBDAgPfM7GMzOxvA3d8A7gL+Aiwxs3EWztUraVn0c7ckx3ZLOD4RaGBmB5hZHmFU8bnoWHvg0sRQDbQl9L9YmX1393fc/bfu3pJw/uBhwPBSmrcn/E4WJ3zefcAuCW0WubsnPJ9PCKrfA6cQzn9bbGYvmVnnsmoTyXYKYSIZzN0/IUwfbXNOUio/tnjDwvlfQwgjU83cvSnwHSG8pNJiwhRhsbaVeI9lbBntKtYOWATg7l+5++/cfXfCdODdFl1h6e53unsvwpTsT4DLk7z/LGAhcHLizuics98Ar0fvtYkwSnVq9HjR3VdFzRcAI0uE6gbu/kTCWyYGojK5+/uE6c/iv5eSr11AGGlrkfB5O7n7PgltWkfnuxVrB3wZvf+r7n4UIWR+QhitFZFSKISJZBAz62xml1q0xIGZtSX8w13Vo13l1Zhw/tBSoLaZXQMkGxWqak8Bw8ysmZm1JuHqzzLUNbPc4kfC+4w0s8Zm1h64BCi+OOBk27KUxLeEwLLZzPaLRq3qAN8DPxCm4bYSjRZdBlxlZqdFn7sr8ADhdzQ2ofnjhFGkArZMRUIIMYOizzMza2hmx5lZ43L0t3g5k98lnOTfGfglW/5evgbyomBINK36GnCrme1kZrXMbA8z+1nC2+4CDDazOmZ2MrA38LKZtbKwHEZDQpBbnez3IiJbKISJZJZVhBOj3zWz7wn/mE4HLo2pnleBVwgXDMwnBJLKTA1W1PWEUaa5wL+Ap9l6mY5kPgbWJjzOAi4kBKk5wH8JAeihqP1+hN/zauAF4I/ROVY7EcLRt4Q+LwduSfaB0TlTpwMXR+1mAPWB3u6+PKHdu1EduwP/SNhfBPyOMP35LTAbGLCdfiZaQQhd06J+vEKY6rw5Ol580cByM/sg2j6DsPTJjOgzn2brKdV3gb0II4kjgZOivtQihNgvgW8IFzr8vgK1imQd23pqX0Qk85jZ74F+7v6z7TaWSjOzAYQ15kquQycilaCRMBHJOGa2m5n1jqbLOhFGAp/b3utERNKJVswXkUxUl3DVXgfClNuTwN1xFiQiUlGajhQRERGJgaYjRURERGKgECYiIiISg4w7J6xFixael5cXdxkiIiIi2zV58uRl0R0rtpFxISwvL4+ioqK4yxARERHZLjObX9oxTUeKiIiIxEAhTERERCQGCmEiIiIiMci4c8JEREQk2LBhAwsXLuSHH36Iu5Ssl5ubS5s2bahTp065X6MQJiIikqEWLlxI48aNycvLw8ziLidruTvLly9n4cKFdOjQodyv03SkiIhIhvrhhx/YeeedFcBiZmbsvPPOFR6RVAgTERHJYApg6aEy34NCWEmFhZCXB7VqhZ+FhXFXJCIiknaWL19Ojx496NGjB7vuuiutW7f+8fn69evLfG1RURGDBw/e7mccfPDBVVLrm2++yfHHH18l71WVUnZOmJnlApOAetHnPO3u15ZoUw94FOgFLAdOcfd5qappuwoLYeBAWLMmPJ8/PzwHKCiIrSwREZF0s/POOzNlyhQARowYQaNGjbjssst+PL5x40Zq104eM/Lz88nPz9/uZ7z99ttVUmu6SuVI2DrgCHffF+gBHG1mB5Zocw7wrbvvCYwF/pTCerZv+PAtAazYmjVhv4iISKZL8WzPgAEDGDRoEAcccABDhgzhvffe46CDDqJnz54cfPDBzJo1C9h6ZGrEiBGcffbZ9OnTh44dO3LnnXf++H6NGjX6sX2fPn046aST6Ny5MwUFBbg7AC+//DKdO3emV69eDB48uEIjXk888QTdunWja9euDB06FIBNmzYxYMAAunbtSrdu3Rg7diwAd955J126dKF79+7069dvx39ZpHAkzMNvZ3X0tE708BLNTgBGRNtPA3eZmXnxb7a6ffFFxfaLiIhkimqa7Vm4cCFvv/02OTk5rFy5kv/85z/Url2bf/3rX1x55ZU888wz27zmk08+YeLEiaxatYpOnTrx+9//fpulHj788EM+/vhjdt99d3r37s1bb71Ffn4+5513HpMmTaJDhw6ceuqp5a7zyy+/ZOjQoUyePJlmzZrRt29fnn/+edq2bcuiRYuYPn06ACtWrABg9OjRzJ07l3r16v24b0eldIkKM8sBJgN7An9x93dLNGkNLABw941m9h2wM7AslXWVql278EdZUpMmsHYt1K9f/TWJiIiUx0UXQTQ9mNQ778C6dVvvW7MGzjkH7r8/+Wt69IDbb69QGSeffDI5OTkAfPfdd5x55pl89tlnmBkbNmxI+prjjjuOevXqUa9ePXbZZRe+/vpr2rRps1Wb/fff/8d9PXr0YN68eTRq1IiOHTv+uCzEqaeeyrhx48pV5/vvv0+fPn1o2TLcW7ugoIBJkyZx9dVXM2fOHC688EKOO+44+vbtC0D37t0pKCjgxBNP5MQTT6zQ76Q0KT0x3903uXsPoA2wv5l1rcz7mNlAMysys6KlS5dWaY1bGTkSGjTYel+tWrBiBey5J9xzD2znZEMREZG0VDKAbW9/JTVs2PDH7auvvprDDz+c6dOn8/e//73UJRzq1av343ZOTg4bN26sVJuq0KxZMz766CP69OnDvffey7nnngvASy+9xAUXXMAHH3zAfvvtVyWfXy2Ltbr7CjObCBwNTE84tAhoCyw0s9pAE8IJ+iVfPw4YB5Cfn5+6qcri4djhw8MUZLt2IZi1aRP2nX8+3HwzjBgR2pZywqGIiEi1296IVV5e8tme9u3hzTdTUFAYCWvdujUA48ePr/L379SpE3PmzGHevHnk5eUxYcKEcr92//33Z/DgwSxbtoxmzZrxxBNPcOGFF7Js2TLq1q3Lb37zGzp16kT//v3ZvHkzCxYs4PDDD+eQQw7hySefZPXq1TRt2nSH6k/ZSJiZtTSzptF2feAo4JMSzV4Azoy2TwLeiO18sGIFBTBvHmzeHH4WFMDPfgb/+Q+8/DI0bw4DBkC3bvDUU6GdiIhIuks229OgQdifIkOGDGHYsGH07NkzJSNX9evX5+677+boo4+mV69eNG7cmCZNmiRt+/rrr9OmTZsfH/PmzWP06NEcfvjh7LvvvvTq1YsTTjiBRYsW0adPH3r06EH//v256aab2LRpE/3796dbt2707NmTwYMH73AAA7BUZR4z6w48AuQQwt5T7n69mV0PFLn7C9EyFn8FegLfAP3cfU5Z75ufn+9FRUUpqblc3OG55+Dqq2HGDNh3X7jxRjjuONCCeSIiUo1mzpzJ3nvvXf4XFBZuO9uT4UswrV69mkaNGuHuXHDBBey1115cfPHFsdSS7Psws8nunnQ9jpSFsFSJPYQV27QJnngCrr0W5syBAw8Mf8xHHBF3ZSIikiUqHMJqoLFjx/LII4+wfv16evbsyf3330+DkiN+1aSiIUwr5ldWTg707w+ffAL33QcLF8KRR4bH//4Xd3UiIiJZ4eKLL2bKlCnMmDGDwsLC2AJYZSiE7ag6dcI6K599BmPHwrRpcPDBcPzxZV8qLCIiIllNIayq5OaGNVrmzIFRo+Ctt6BnT/jtb8NomYiISApk2mlFNVVlvgeFsKrWqBEMGwZz58JVV8E//gH77BOuqJw7N+7qRESkBsnNzWX58uUKYjFzd5YvX05ubm6FXqcT81Nt6VIYPRr+8pdwMv+554ZwFq2bIiIiUlkbNmxg4cKFpS6CKtUnNzeXNm3abHO7JV0dmQ4WLQpLWTzwQFjk9fzz4YorILpdgoiIiNQ8ujoyHbRuHW579OmncMopYWXjjh3DemNVdCNQERERyRwKYdWtQwcYPx6mT4djjgmjYx06wE03werVcVcnIiIi1UQhLC577x1ue/TBB9C7N1x5JeyxRxgh09y+iIhIjacQFreePeHFF+Htt8NVlBdfDHvtBePGwYYNcVcnIiIiKaIQli4OOgjeeANefx3atIHzzgujZY89Fq6qFBERkRpFISzdHHFEGBX7+9/DmmOnnw7du8Ozz4abh4uIiEiNoBCWjszCbY8++AAmTAgjYb/5DeTnh8VfFcZEREQynkJYOqtVK9z2aPp0ePhh+OYbOPZYOPRQ+Pe/465OREREdoBCWCaoXTvc9mjWLLj77nB/yj59oG9feO+9uKsTERGRSlAIyyR168Lvfw+ffw5jxsCHH8IBB8CJJ8LUqXFXJyIiIhWgEJaJ6teHSy8NI2LXXw8TJ0KPHnDqqWFFfhEREUl7CmGZrHHjcNujuXPDfShfeAG6dIFzzoH58+OuTkRERMqgEFYTNG8Oo0aFkbE//CGsLbbXXnDhhbB4cdzViYiISBIKYTVJq1bhtkezZ4cT+e+5J9wKacgQWL487upEREQkgUJYTdS2bbjt0SefwK9/HU7i79ABRoyAlSvjrk5ERERQCKvZ9twzTE1OmwZHHQXXXRfC2M03w5o1cVcnIiKS1RTCssE++8Azz0BREey/PwwdGqYp77oL1q2LuzoREZGspBCWTXr1Crc9mjQJfvKTcOL+T34CDz4IGzdCYSHk5YWV+vPywnMRERFJCYWwbHToofDmm/Dqq+Fk/nPPhTZt4Oyzw9IW7uHnwIEKYiIiIimiEJatzMJtj959F55/PtyXcv36rdusWQPDh8dSnoiISE1XO+4CJGZmcMIJYToymfnzw/Hu3cOjW7ewBllOTvXWKSIiUsMohEnQrl3yVfYbNIDPPoMXX4TNm8O+3Nxwsn9xKCsOaC1bVm/NIiIiGUwhTIKRI8M5YIlLVzRoENYbKyiAH36AmTPDjcKnTg3LXrz8Mjz88Jb2u+66dSjr3h323hvq1av+/oiIiKQ5hTAJCgrCz+HD4YsvwsjYyJFb9ufmQs+e4ZHo669DIJs2bUtAS1z6IicHOnXaejqze/ewoKxZ9fVPREQkzZi7x11DheTn53tRUVHcZUhZNm4Mt04qDmXFI2fz5m1p06TJttOZXbuGm5KLiIjUEGY22d3zkx5TCJNq8913MH361qNmU6fCqlVb2nTosO2o2Z576kIAERHJSGWFME1HSvVp0gR69w6PYu5h+rPkqNnf/771hQBdu249atatmy4EEBGRjJaykTAzaws8CrQCHBjn7neUaNMEeAxoRwiEY9z94ZLvlUgjYVli7dotFwIkjpwtWbKlza67bjtqpgsBREQkjcQyHWlmuwG7ufsHZtYYmAyc6O4zEtpcCTRx96Fm1hKYBezq7uuTv6tCWNYrvhAgcdTs44+3vhCgc+dtr9Js00YXAoiISLWLZTrS3RcDi6PtVWY2E2gNzEhsBjQ2MwMaAd8ApawaKkK4zVKrVvDzn2/Zt3FjWMsscdTsf/+DJ5/c0qb4QoDEkbPECwEKC0u/MlRERCQFquXEfDPLAyYBXd19ZcL+xsALQGegMXCKu79U1ntpJEzKrfhCgJLnmyVeCNCxIzRtGvZv2LBlf+IaaSIiIpUU69WRZtYI+Dcw0t2fLXHsJKA3cAmwB/BPYN/EoBa1GwgMBGjXrl2v+clWdhcpj+KbkyeOmj37bPLbNrVvv/WyGiIiIhUUWwgzszrAi8Cr7n5bkuMvAaPd/T/R8zeAK9z9vdLeUyNhUuVq1QrhrCSzLVdoioiIVEJZIaxWCj/UgAeBmckCWOQL4MiofSugEzAnVTWJJNWuXcX2i4iIVIGUhTDCNOPpwBFmNiV6HGtmg8xsUNTmBuBgM5sGvA4MdfdlKaxJZFsjR4ZzwBKZwfXXx1OPiIhkhVReHflfoMw1Adz9S6BvqmoQKZeS981s0QKWLg0n9ouIiKRIKkfCRDJHQUE4CX/z5rAW2ZFHwnXXwYoVcVcmIiI1lEKYSElmcOut8M03YapSREQkBRTCRJLZd18YMADuvBPmzo27GhERqYEUwkRKc+ONULs2XHFF3JWIiEgNpBAmUprdd4fLL4enngq3QRIREalCCmEiZbn8cthtN7jkkuQLuoqIiFSSQphIWRo2DNOS77wDf/tb3NWIiEgNohAmsj1nngndu8PQofDDD3FXIyIiNYRCmMj25OSEJSvmzYM//znuakREpIZQCBMpj5//HI47Lqwbtkx31hIRkR2nECZSXrfcAqtXh5X0RUREdpBCmEh57b03DBwI994Ls2bFXY2IiGQ4hTCRihgxAurXhyFD4q5EREQynEKYSEXssgtceSW88AK8+Wbc1YiISAZTCBOpqIsugnbtwgKumzfHXY2IiGQohTCRisrNhZtugg8/hL/+Ne5qREQkQymEiVRGv36w334wfDisWRN3NSIikoEUwkQqo1YtuO02WLQoLOQqIiJSQQphIpV1yCHwm9/An/4EixfHXY2IiGQYhTCRHfGnP8H69XDNNXFXIiIiGUYhTGRH7LEH/OEP8NBDMG1a3NWIiEgGUQgT2VFXXQVNmsBll8VdiYiIZBCFMJEd1bx5mI587TV45ZW4qxERkQyhECZSFc4/H/bcEy69FDZujLsaERHJAAphIlWhbt1wkv6MGfDgg3FXIyIiGUAhTKSq/OpXcOihYWpy5cq4qxERkTSnECZSVczCwq1LloRRMRERkTIohIlUpf32g4KCsJr+ggVxVyMiImlMIUykqo0aFX5eeWW8dYiISFpTCBOpau3awcUXw2OPQVFR3NWIiEiaUggTSYUrroBddglLVrjHXY2IiKQhhTCRVNhpJ7juOpg0CZ5/Pu5qREQkDSmEiaTKuedCly4wZEi4ybeIiEiClIUwM2trZhPNbIaZfWxmfyylXR8zmxK1+Xeq6hGpdrVrwy23wOzZcM89cVcjIiJpJpUjYRuBS929C3AgcIGZdUlsYGZNgbuBX7r7PsDJKaxHpPodcwwcdRRcfz18+23c1YiISBpJWQhz98Xu/kG0vQqYCbQu0ew04Fl3/yJqtyRV9YjEwgzGjAkB7MYb465GRETSSLWcE2ZmeUBP4N0Sh34CNDOzN81sspmdUcrrB5pZkZkVLV26NMXVilSx7t3h7LPhz3+Gzz+PuxoREUkTKQ9hZtYIeAa4yN1L3lCvNtALOA74BXC1mf2k5Hu4+zh3z3f3/JYtW6a6ZJGqd8MN4SbfV1wRdyUiIpImUhrCzKwOIYAVuvuzSZosBF519+/dfRkwCdg3lTWJxGK33cJVkk8/DW+9FXc1IiKSBlJ5daQBDwIz3f22Upr9P+AQM6ttZg2AAwjnjonUPJdeCrvvDpdcAps3x12NiIjELJUjYb2B04EjoiUoppjZsWY2yMwGAbj7TOAVYCrwHvCAu09PYU0i8WnYEEaOhPfegwkT4q5GRERiZp5ht1TJz8/3It2PTzLV5s3Qq1e4WvKTTyA3N+6KREQkhcxssrvnJzumFfNFqlOtWnDrrTB/PtxxR9zViIhIjBTCRKrbEUfA//0fjBoFWnJFRCRrKYSJxOHmm+H772HEiLgrERGRmCiEicShc2cYNAjuuw9m6oJgEZFspBAmEpdrrw1XTA4ZEnclIiISA4Uwkbi0bAnDh8OLL8Lrr8ddjYiIVDOFMJE4DR4M7duHhVw3bYq7GhERqUYKYSJxys2F0aPho4/g0UfjrkZERKqRQphI3E45BQ44IExNfv993NWIiEg1UQgTiZsZ3HYbLF4MY8bEXY2IiFQThTCRdHDwwXDyyWH9sC+/jLsaERGpBgphIuli9GjYuBGuvjruSkREpBoohImki44d4cIL4eGHw4n6IiJSoymEiaST4cOhWbOwZIV73NWIiEgKKYSJpJNmzcJK+q+/Di+/HHc1IiKSQgphIulm0CDYay+4/PJwjpiIiNRICmEi6aZu3XCV5MyZcP/9cVcjIiIpohAmko5OOAEOOyxMTa5cGXc1IiKSAgphIumoeAHXpUvhppvirkZERFJAIUwkXfXqBaefDmPHwvz5cVcjIiJVTCFMJJ2NHBlGxa68Mu5KRESkiimEiaSztm3DmmGPPw7vvRd3NSIiUoUUwkTS3dCh0KoVXHKJFnAVEalBFMJE0l3jxnD99fDWW/Dss3FXIyIiVaRcIczMGppZrWj7J2b2SzOrk9rSRORHZ58N++wTRsXWr4+7GhERqQLlHQmbBOSaWWvgNeB0YHyqihKREmrXhjFj4PPP4S9/ibsaERGpAuUNYebua4BfA3e7+8nAPqkrS0S2cfTR8ItfwA03wDffxF2NiIjsoHKHMDM7CCgAXor25aSmJBEp1Zgx8N13IYiJiEhGK28IuwgYBjzn7h+bWUdgYsqqEpHkunaFc86Bu+6Czz6LuxoREdkB5hW85D06Qb+Ru8dyQ7v8/HwvKiqK46NF0sNXX8Fee8FRR+lqSRGRNGdmk909P9mx8l4d+biZ7WRmDYHpwAwzu7wqixSRctp113CV5HPPwaRJcVcjIiKVVN7pyC7RyNeJwD+ADoQrJEUkDpdcAq1bh9X0N2+OuxoREamE8oawOtG6YCcCL7j7BqDMeUwza2tmE81shpl9bGZ/LKPtfma20cxOKnflItmsQQMYNQqKiuCJJ+KuRkREKqG8Iew+YB7QEJhkZu2B7Z0TthG41N27AAcCF5hZl5KNzCwH+BNh/TERKa/+/eGnP4Vhw2Dt2rirERGRCipXCHP3O929tbsf68F84PDtvGaxu38Qba8CZgKtkzS9EHgGWFKx0kWyXK1acOutsGAB3H573NWIiEgFlffE/CZmdpuZFUWPWwmjYuViZnlAT+DdEvtbA78C7il/ySLyoz594IQT4Kab4Ouv465GREQqoLzTkQ8Bq4DfRo+VwMPleaGZNSKMdF2UZFmL24Gh7l7mmcVmNrA4AC5durScJYtkiZtvDtOR114bdyUiIlIB5VonzMymuHuP7e1L8ro6wIvAq+5+W5LjcwGLnrYA1gAD3f350t5T64SJJDF4cLin5NSp4UbfIiKSFnZ4nTBgrZkdkvCGvYEyzwQ2MwMeBGYmC2AA7t7B3fPcPQ94Gji/rAAmIqW45hpo3Bgu1/J9IiKZonY52w0CHjWzJtHzb4Ezt/Oa3oS1xKaZ2ZRo35VAOwB3v7dipYpIqVq0gKuuCiHsn/8Mq+mLiEhaq9Bti8xsJwB3X2lmF7n77akqrDSajhQpxbp1sPfe0KgRfPgh5OTEXZGISNariulIIISvhJPrL9nhykSk6tSrB6NHw7RpMH583NWIiMh2VCiElWDbbyIi1erkk+Ggg8LU5OrVcVcjIiJl2JEQVv55TBGpHmZw223w1Vdwyy1xVyMiImUoM4SZ2SozW5nksQrYvZpqFJGKOPBAOOWUEMIWLoy7GhERKUWZIczdG7v7Tkkejd29vFdWikh1u+km2LQpTEuKiEha2pHpSBFJVx06wB//CI8+Gq6UFBGRtKMQJlJTXXklNG8Ol14KFViKRkREqodCmEhN1bQpjBgBEyfCiy/GXY2IiJSgECZSk513HnTqFFbS37Ah7mpERCSBQphITVanDtx8M8yaBePGxV2NiIgkUAgTqen+7//g8MPD1OSKFXFXIyIiEYUwkZrODG69FZYvh1Gj4q5GREQiCmEi2aBnTzjjDLjjDpg7N+5qREQEhTCR7DFyJOTkwLBhcVciIiIohIlkj9at4bLLYMIEeOeduKsREcl6CmEi2WTIENh1V7jkEi3gKiISM4UwkWzSqBHccAP873/w9NNxVyMiktUUwkSyzVlnQbduMHQorFsXdzUiIllLIUwk2+TkwJgx4SrJu+6KuxoRkaylECaSjfr2hWOOCVOTy5bFXY2ISFZSCBPJVrfcAqtWwfXXx12JiEhWUggTyVb77AO/+x3ccw98+mnc1cSnsBDy8qBWrfCzsDDuikQkSyiEiWSz666D3NywdEU2KiyEgQNh/vywZMf8+eG5gpiIVIPacRcgIjFq1SqsoD98eFg/bMkSaNcurK5fUBB3dbBpE6xdGx4//LBlu+SjtGPbe820abBx49afuWZNGCF8/XXYeedtH82bb9muVy+e34uI1AgKYSLZbrfdwk2+v/46PC8eDYKtg9jmzZULOjvymg0bKt+vOnWgfv3wyM3dsl2/PjRoEELUhx8mf+3atfDaa+Gm5z/8UPpnNGxYekArLbw1bRqmPkUk65ln2KrZ+fn5XlRUFHcZIjVHXl4IXiXl5ECLFluC0fr1lf+MOnW2DUKlBaSq2p+TU/m+t28P8+aF7TVrQhgrfnzzzdbPk+3/9tsQWpOpVQuaNSt/eCs+3qBBZX/7ZSssDCOhX3yRXqOgIjWEmU129/xkxzQSJpLtvvgi+f5Nm+CXv0weeCoShnJzoXaa/qdm5Mgw6rdmzZZ9DRqE/YnPGzSAtm3L/76bN8OKFdsPbsuXw6JFMHVq2P7++9LfMzd3+9OjJR/NmpUdRovPiSvuf2mjoCKSEhoJE8l25RkNqsnSaSRo3bryBbeSxzZtKv09mzYtPbzdfnsYtSspW757kWpQ1kiYQphItis5GgJh5GfcOI2GZAJ3WLmy4sFt5crS39Os9OlUEakQTUeKSOmKg1a6jAZJxZhBkybh0bFj+V+3YUNov3Dhtsfatau6+kSkVLpER0RC4Jo3L4x+zJunAJYN6tSB0aO3PeG/fv2tz4kTkZRRCBMRyVYFBWHauX37MKJmFs4R7Ncv7spEsoJCmIhINkscBX34YZg5E268Me6qRLJCykKYmbU1s4lmNsPMPjazPyZpU2BmU81smpm9bWb7pqoeERHZjjPPhNNPDzd1//e/465GpMZL5UjYRuBSd+8CHAhcYGZdSrSZC/zM3bsBNwDjUliPiIhsz913w557wmmnwbJlcVcjUqOlLIS5+2J3/yDaXgXMBFqXaPO2uxcvUvMO0CZV9YiISDk0agQTJoRlLAYMCEtgiEhKVMs5YWaWB/QE3i2j2TnAP6qjHhERKUOPHnDrrfDSSzB2bNzViNRYKQ9hZtYIeAa4yN2Trg5oZocTQtjQUo4PNLMiMytaunRp6ooVEZHg/PPhV7+CK66A99+PuxqRGimlK+abWR3gReBVd7+tlDbdgeeAY9z90+29p1bMFxGpJt9+G0bFcnLgww/DgrAiUiFlrZifyqsjDXgQmFlGAGsHPAucXp4AJiIi1ahZM3jiiXAnhYEDdX6YSBVL5XRkb+B04AgzmxI9jjWzQWY2KGpzDbAzcHd0XENcIiLp5OCDw7phTz0FDzwQdzUiNYpu4C0iImXbvBmOPhr+859wfljXrnFXJJIxYpmOFBGRGqJWLfjrX8M5YaecAmvWxF2RSI2gECYiItvXqhU89li4rdHgwXFXI1IjKISJiEj5/PznMGwYPPhgOGFfRHaIQpiIiJTfdddB795w3nkwe3bc1YhkNIUwEREpv9q14fHHw89+/WDdurgrEslYCmEiIlIx7drBQw/B5MlhRX0RqRSFMBERqbgTT4QLL4Tbb4e//z3uakQykkKYiIhUzi23QM+eMGAALFgQdzUiGUchTEREKqdePZgwAdavh9NOg40b465IJKMohImISOXttRfcey/8979w/fVxVyOSURTCRERkxxQUwFlnhXtMvvFG3NWIZAyFMBER2XF//jN06hQC2ZIlcVcjkhEUwkREZMc1bBjOD/v2WzjjjHDTbxEpk0KYiIhUje7dw5IVr74KY8bEXY1I2lMIExGRqnPeeXDSSTB8OLzzTtzViKQ1hTAREak6ZnD//dCmDZx6KqxYEXdFImlLIUxERKpW06bw5JOwcCGcey64x12RSFpSCBMRkap3wAEwahQ880xYR0xEtqEQJiIiqXHppXDMMXDxxfDRR3FXI5J2FMJERCQ1atWCRx6B5s3hlFNg9eq4KxJJKwphIiKSOi1bQmEhfPopXHhh3NWIpBWFMBERSa3DD4erroLx4+Gxx+KuRiRtKISJiEjqXXMNHHooDBoURsVERCFMRESqQe3a8PjjkJsbzg/74Ye4KxKJnUKYiIhUjzZtwpTklClw+eVxVyMSO4UwERGpPscfH5asuOsueO65uKsRiZVCmIiIVK+bboJeveDss2H+/LirEYmNQpiIiFSvevVgwgTYtAlOOw02bIi7IpFYKISJiEj122MPGDcO3n4brr027mpEYqEQJiIi8ejXL9zge/Ro+Oc/465GpNophImISHzuuAO6dIH+/eGrr+KuRqRaKYSJiEh8GjQI54etWgWnnw6bN8ddkUi1UQgTEZF47bNPGBH717/gT3+KuxqRapOyEGZmbc1sopnNMLOPzeyPSdqYmd1pZrPNbKqZ/TRV9YiISBo799ywkv7VV8Nbb8VdjUi1SOVI2EbgUnfvAhwIXGBmXUq0OQbYK3oMBO5JYT0iIpKuzMLVku3bw6mnwjffxF2RSMqlLIS5+2J3/yDaXgXMBFqXaHYC8KgH7wBNzWy3VNUkIiJpbKedwvlhX30F55wD7nFXJJJS1XJOmJnlAT2Bd0scag0sSHi+kG2DmoiIZIv8/HBe2PPPh1sbidRgKQ9hZtYIeAa4yN1XVvI9BppZkZkVLV26tGoLFBGR9HLRRXDccXDZZfDhh3FXI5IyKQ1hZlaHEMAK3f3ZJE0WAW0TnreJ9m3F3ce5e76757ds2TI1xYqISHowg/HjoWXLcLL+qlVxVySSEqm8OtKAB4GZ7n5bKc1eAM6IrpI8EPjO3RenqiYREckQLVrA44/D55/D+efr/DCpkVI5EtYbOB04wsymRI9jzWyQmQ2K2rwMzAFmA/cD56ewHhERySSHHRbuK/nYY/DII3FXI1LlzDPs/13k5+d7UVFR3GWIiEh12LQJfv5zeO89KCqCvfeOuyKRCjGzye6en+yYVswXEZH0lZMDhYXh9kb9+sHatXFXJFJlFMJERCS97b47PPooTJ0Kl14adzUiVUYhTERE0t8xx4QlK+65B555Ju5qRKqEQpiIiGSGkSNh//3Davpz58ZdjcgOUwgTEZHMULcuPPlkWK7i1FNhw4a4KxLZIQphIiKSOTp0gAcegHffheHD465GZIcohImISGY5+WQ47zy45RZ45ZW4qxGpNIUwERHJPGPHQrducMYZ8OWXcVcjUikKYSIiknnq14cJE+D776F//7Coq0iGUQgTEZHMtPfecNddMHEijBoVdzUiFaYQJiIimWvAADjtNBgxAiZNirsakQpRCBMRkcxlBvfeCx07hjC2bFncFYmUm0KYiIhktsaNw/lhS5fCWWeFdcREMoBCmIiIZL6f/jQsWfHii3DHHXFXI1IuCmEiIlIzXHghnHACDBkCRUVxVyOyXQphIiJSM5jBQw/BrrtCv36wcmXcFYmUSSFMRERqjubN4fHHYd68sKq+zg+TNKYQJiIiNcshh8B114WbfT/0UNzViJRKIUxERGqeK66AI48M54l9/HHc1YgkpRAmIiI1T04OPPZYWL7ilFNgzZq4KxLZhkKYiIjUTLvuCn/9axgJu+iiuKsR2YZCmIiI1Fx9+8LQoXD//WFBV5E0ohAmIiI12w03wEEHwe9+B59/Hnc1Ij9SCBMRkZqtTh144olwnli/frB+fdwViQAKYSIikg3at4cHHwwr6Q8bFnc1IoBCmIiIZItf/xouuABuuw1eeinuauJVWAh5eVCrVvhZWBh3RVlJIUxERLLHmDGw775w5pmwcGHc1cSjsBAGDoT588MdBebPD88VxKqdeYbd0iE/P9+LdGNWERGprFmzoFcvaNMG1q6FBQugXTsYORIKCuKuLrn168NaZ99/Hx6J2yWfb2+7qAg2bNj2M+rVCwvcNmkSHjvttPXP0rbr1Kn+30cGMbPJ7p6f7Fjt6i5GREQkVp06Qf/+cN99W/YVjwZB5YLYhg1lB6CKBqWSzzdurFg9tWtDw4bQoEH4mbidLIABrFsHX30Fn34K330XHuW5iKF+/e0HtdK2i382bBhuwF5dCgth+HD44otYA7hCmIiIZJ9XXtl235o14TZH8+dXPCiVFmxKk5OzbTgq3m7RovRjybaTHatbt/TPzssLfSypfXuYPHnrfevWbQlkK1eWf3vx4i3bq1Zt//dRq1blg1zidu1yxJri6djiuyjsaADfAZqOFBGR7FOrVjgfqqzjZYWc8oah0trVqVO9Iz+JSoYQCPWNG5eaELJpE6xeXfEgV3K7PEG3QYPtT6PeeSd8++22r23fHubNq/LuazpSREQkUbt2yUeD2rSB2bPDSFJcISnVioNWdU3H5eRsCUCV5V75UblFi7Zsr15d+md88UXl66uklIUwM3sIOB5Y4u5dkxxvAjwGtIvqGOPuD6eqHhERkR+NHJl8NGj06HCCek1XUJC+FyEkYwa5ueHRqlXl32fTpjAdm+zK2HbtKv++lZTKJSrGA0eXcfwCYIa77wv0AW41szImsUVERKpIQUGYfmvfPvwD37596qbjJH3k5ISg3aDB1vsbNAjBvJqlbCTM3SeZWV5ZTYDGZmZAI+AboIKXf4iIiFRSpo0GSdWo7unYMsR5TthdwAvAl0Bj4BR33xxjPSIiIpIN0iSAx7li/i+AKcDuQA/gLjPbKVlDMxtoZkVmVrR06dLqq1BEREQkReIMYWcBz3owG5gLdE7W0N3HuXu+u+e3bNmyWosUERERSYU4Q9gXwJEAZtYK6ATMibEeERERkWqTyiUqniBc9djCzBYC1wJ1ANz9XuAGYLyZTQMMGOruy1JVj4iIiEg6SeXVkadu5/iXQN9Ufb6IiIhIOotzOlJEREQkaymEiYiIiMRAIUxEREQkBuZl3UU+DZnZUiDJXVerXAsgWy8UUN+zVzb3P5v7Dtndf/U9e1VH/9u7e9L1tTIuhFUXMyty9/y464iD+p6dfYfs7n829x2yu//qe3b2HeLvv6YjRURERGKgECYiIiISA4Ww0o2Lu4AYqe/ZK5v7n819h+zuv/qevWLtv84JExEREYmBRsJEREREYpC1IczM5pnZNDObYmZF0b7mZvZPM/ss+tks2m9mdqeZzTazqWb203irrzgze8jMlpjZ9IR9Fe6vmZ0Ztf/MzM6Moy8VVUrfR5jZouj7n2JmxyYcGxb1fZaZ/SJh/9HRvtlmdkV196MyzKytmU00sxlm9rGZ/THaX+O/+zL6ni3ffa6ZvWdmH0X9vy7a38HM3o36MsHM6kb760XPZ0fH8xLeK+nvJV2V0ffxZjY34bvvEe2vMX/3xcwsx8w+NLMXo+c1/ntPlKT/6fndu3tWPoB5QIsS+24Groi2rwD+FG0fC/yDcKPxA4F3466/Ev09DPgpML2y/QWaA3Oin82i7WZx962SfR8BXJakbRfgI6Ae0AH4HMiJHp8DHYG6UZsucfetHH3fDfhptN0Y+DTqY43/7svoe7Z89wY0irbrAO9G3+lTQL9o/73A76Pt84F7o+1+wISyfi9x96+SfR8PnJSkfY35u0/o0yXA48CL0fMa/71vp/9p+d1n7UhYKU4AHom2HwFOTNj/qAfvAE3NbLcY6qs0d58EfFNid0X7+wvgn+7+jbt/C/wTODrlxe+gUvpemhOAJ919nbvPBWYD+0eP2e4+x93XA09GbdOauy929w+i7VXATKA1WfDdl9H30tS0797dfXX0tE70cOAI4Olof8nvvvhv4mngSDMzSv+9pK0y+l6aGvN3D2BmbYDjgAei50YWfO/FSvZ/O2L97rM5hDnwmplNNrOB0b5W7r442v4KaBVttwYWJLx2IWX/xzxTVLS/Ne338Ido+Pkhi6bjqMF9j6YZehJGBbLquy/Rd8iS7z6akpkCLCH8I/I5sMLdN0ZNEvvyYz+j498BO5Oh/S/Zd3cv/u5HRt/9WDOrF+2rad/97cAQYHP0fGey5HuP3M7W/S+Wdt99NoewQ9z9p8AxwAVmdljiQQ/jkVlz6Wi29Re4B9gD6AEsBm6NtZoUM7NGwDPARe6+MvFYTf/uk/Q9a757d9/k7j2ANoRRjM7xVlR9SvbdzLoCwwi/g/0I00xD46swNczseGCJu0+Ou5Y4lNH/tPzuszaEufui6OcS4DnCf6C+Lp5mjH4uiZovAtomvLxNtC/TVbS/Neb34O5fR/+R3gzcz5Zh9hrXdzOrQwghhe7+bLQ7K777ZH3Ppu++mLuvACYCBxGmW2pHhxL78mM/o+NNgOVkeP8T+n50NEXt7r4OeJia+d33Bn5pZvMIU+dHAHeQPd/7Nv03s8fS9bvPyhBmZg3NrHHxNtAXmA68ABRfAXEm8P+i7ReAM6KrKA4EvkuYyslkFe3vq0BfM2sWTeH0jfZlnBLn9P2K8P1D6Hu/6IqhDsBewHvA+8Be0RVGdQknsL5QnTVXRnRux4PATHe/LeFQjf/uS+t7Fn33Lc2sabRdHziKcF7cROCkqFnJ7774b+Ik4I1olLS030vaKqXvnyT8Hw8jnBOV+N3XiL97dx/m7m3cPY/wt/qGuxeQBd87lNr//mn73XsaXMVQ3Q/CVU4fRY+PgeHR/p2B14HPgH8BzaP9BvyFcD7FNCA/7j5Uos9PEKZeNhDmts+pTH+BswknaM4Gzoq7XzvQ979GfZtK+B/hbgnth0d9nwUck7D/WMIVdp8X/82k+wM4hDDVOBWYEj2OzYbvvoy+Z8t33x34MOrndOCaaH9Hwj+ms4G/AfWi/bnR89nR8Y7b+72k66OMvr8RfffTgcfYcgVljfm7L/F76MOWqwNr/Pe+nf6n5XevFfNFREREYpCV05EiIiIicVMIExEREYmBQpiIiIhIDBTCRERERGKgECYiIiISA4UwEYmNme1sZlOix1dmtijhed3tvDbfzO4sx2e8XUW1NjCzQjObZmbTzey/ZtbIzJqa2flV8Rkikl20RIWIpAUzGwGsdvcxCftq+5b73cXKzIYBLd39kuh5J2AesBthLaKuMZYnIhlII2EiklbMbLyZ3Wtm7wI3m9n+ZvY/M/vQzN6Owg9m1sfMXoy2R1i4GfebZjbHzAYnvN/qhPZvmtnTZvZJNKpl0bFjo32TzezO4vctYTcSblvi7rM83AJlNLBHNHp3S/R+l5vZ+xZuFnxdtC8v4XNnRnU0iI6NNrMZUfsxST5bRGqg2ttvIiJS7doAB7v7JjPbCTjU3Tea2c+BUcBvkrymM3A40BiYZWb3uPuGEm16AvsAXwJvAb3NrAi4DzjM3eea2ROl1PQQ8JqZnUS428Aj7v4ZcAXQ1cPNojGzvoRbvOxPWI37BTM7DPgC6ASc4+5vmdlDwPlm9jDh9kmd3d2Lb7cjIjWfRsJEJB39zd03RdtNgL+Z2XRgLCFEJfOSu69z92WEG5K3StLmPXdf6OHm3VOAPEJ4m+Puc6M2SUOYu08h3PrlFqA58L6Z7Z2kad/o8SHwQfT+e0XHFrj7W9H2Y4RbK30H/AA8aGa/BtaU0j8RqWEUwkQkHX2fsH0DMDE65+r/CPe6S2ZdwvYmko/0l6dNqdx9tbs/6+7nE0LUsUmaGXCTu/eIHnu6+4PFb7HtW/pGwqjZ08DxwCsVqUlEMpdCmIikuyZsORdrQArefxbQ0czyouenJGtkZr3NrFm0XRfoAswHVhGmQIu9CpxtZo2itq3NbJfoWDszOyjaPg34b9Suibu/DFwM7FtlPRORtKZzwkQk3d0MPGJmVwEvVfWbu/vaaImJV8zse+D9UpruAdwTncxfK6rlmeg8rrei6dJ/uPvl0TTl/6Lz/lcD/Qkjb7OAC6LzwWYA9xBC5v8zs1zCKNolVd1HEUlPWqJCRLKemTVy99VRwPoL8Jm7j63iz8hDS1mISAJNR4qIwO/MbArwMWFk6r54yxGRbKCRMBEREZEYaCRMREREJAYKYSIiIiIxUAgTERERiYFCmIiIiEgMFMJEREREYqAQJiIiIhKD/w/6ZzCpgL/eCAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<Figure size 720x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "log_history = trainer.state.log_history\n",
    "\n",
    "# Step과 Loss 데이터 추출\n",
    "steps = [log[\"step\"] for log in log_history if \"loss\" in log]\n",
    "losses = [log[\"loss\"] for log in log_history if \"loss\" in log]\n",
    "\n",
    "# 그래프 그리기\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(steps, losses, marker='o', linestyle='-', color='red', label=\"Training Loss\")\n",
    "plt.title(\"SFT Training Loss Over Steps\")\n",
    "plt.xlabel(\"Training Steps\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "fd75aa79",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.9/site-packages/transformers/generation/utils.py:1219: UserWarning: You have modified the pretrained model configuration to control generation. This is a deprecated strategy to control generation and will be removed soon, in a future version. Please use a generation configuration file (see https://huggingface.co/docs/transformers/main_classes/text_generation)\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 실제로 고기를 먹을 수 없습니다. 하지만 일반적으로 쇠고기는 건강에 좋은 재료로 많이 사용됩니다. 따라서 불고기용 고기는 건강에 좋지 않으므로, 구매 전에 건강 상태와 안전성을 확인하시는 것이 좋습니다.\\n\\n만약 \"불고기용 고기의 한\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'리처드 닉슨은 41대 부통령직을 수행했습니다. \"리처드 닉슨\"은 1952년 대선에서 공화당 후보로 출마한 리처드 닉슨이 출마하면서 처음 등장했습니다.\\n\\n1960년대 초반, 리처드 닉슨은 39대 부통령직을 맡았습니다.\\n출처: 클린턴\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어?\n",
      "\n",
      "### Response(응답):'시카고 오 헤어 국제공항은 미국 일리노이주 시카고에 위치해 있습니다.恩國際都市報道報道報道)에 따르면, 시카고는 미국 일리노이주 시카고에서 가장 중요한 국제공항 중 하나입니다.恩國濟州報道報道)의 보도에 따르면, 시카고는 미국에서 가장 중요한 도시 중 하나로 인정받고 있습니다.\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'미세먼지 농도는 어제와 비교해서 나아졌지만, 여전히 나쁜 수준입니다. 미세먼지 농도가 높은 날에는 실외 활동을 자제하는 것이 좋습니다. 또한, 미세먼지 농도를 줄이기 위해 공기청정기나 마스크를 사용하는 것이 좋습니다. 외출 후에는 반드시 마스크를 착용하고, 실내 공기 질을 유지하기 위한 노력을 기울여야\n"
     ]
    }
   ],
   "source": [
    "generator = pipeline('text-generation', model='output_1_SFT', tokenizer=tokenizer)\n",
    "\n",
    "generation_args = dict(   \n",
    "    num_beams=4,\n",
    "    repetition_penalty=2.0,\n",
    "    no_repeat_ngram_size=4,\n",
    "    eos_token_id=375, # \\n   \n",
    "    max_new_tokens=64,\n",
    "    do_sample=True,\n",
    "    top_k=50,\n",
    "    early_stopping=True\n",
    ")\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = ['불고기용 고기 한우에요?',\n",
    "               '리처드 닉슨이 43대 부통령직을 수행한 년도는?',\n",
    "               '시카고 오헤어 국제공항은 어디에 있어?',\n",
    "               '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt' : tmp}) for tmp in list_prompt]\n",
    "\n",
    "list_result = generator(list_prompt, **generation_args)   \n",
    "for prompt, result in zip(list_prompt, list_result):\n",
    "    print()\n",
    "    print((result[0]['generated_text']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e51ee4",
   "metadata": {},
   "source": [
    "### 3. RM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "d7ef9521",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from typing import Optional\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.optim import Adam\n",
    "from chatgpt.dataset import RewardDataset\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.trainer import RewardModelTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, AutoModel, AutoConfig\n",
    "from transformers.models.gpt2.configuration_gpt2 import GPT2Config\n",
    "from transformers.models.gpt2.modeling_gpt2 import GPT2Model\n",
    "import loralib as lora"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "e0bc5732",
   "metadata": {},
   "outputs": [],
   "source": [
    "class GPTRM_custom(RewardModel):\n",
    "\n",
    "    def __init__(self,\n",
    "                 pretrained: Optional[str] = None,\n",
    "                 config: Optional[GPT2Config] = None,\n",
    "                 checkpoint: bool = False,\n",
    "                 lora_rank: int = 0,\n",
    "                 lora_train_bias: str = 'none',\n",
    "                 tokenizer=None) -> None:\n",
    "        if pretrained is not None:\n",
    "            model = GPT2Model.from_pretrained(pretrained)\n",
    "            model.resize_token_embeddings(len(tokenizer))\n",
    "        elif config is not None:\n",
    "            model = GPT2Model(config)\n",
    "        else:\n",
    "            model = GPT2Model(GPT2Config())\n",
    "        if checkpoint:\n",
    "            model.gradient_checkpointing_enable()\n",
    "\n",
    "        value_head = nn.Linear(model.config.n_embd, 1)\n",
    "        super().__init__(model, value_head, lora_rank, lora_train_bias)\n",
    "\n",
    "        if pretrained is not None:\n",
    "            self.model = model\n",
    "            self.pretrained = pretrained\n",
    "\n",
    "\n",
    "    def save_pretrained(self, dir):\n",
    "        if self.pretrained is not None:\n",
    "            self.model.save_pretrained(dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "94a3717d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at skt/kogpt2-base-v2 were not used when initializing GPT2Model: ['lm_head.weight']\n",
      "- This IS expected if you are initializing GPT2Model from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing GPT2Model from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "model = AutoModelForCausalLM.from_pretrained('skt/kogpt2-base-v2')\n",
    "tokenizer = AutoTokenizer.from_pretrained(\n",
    "    'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "    padding_side=\"right\",\n",
    "    model_max_length=512,\n",
    ")\n",
    "\n",
    "with NaiveStrategy().model_init_context():\n",
    "        model = GPTRM_custom(pretrained='skt/kogpt2-base-v2', lora_rank=0, tokenizer=tokenizer).cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "5cacea99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "before data num: 10220\n",
      "after  data num: 30660\n",
      "data example: \n",
      "{'prompt': '애플은 리사를 어떻게 처리했어', 'chosen': '애플이 누구인지 명확히 알 수 없어서, 리사가 누구인지와 어떤 상황에서 처리되었는지에 대한 추가적인 정보가 필요합니다. 따라서, 보다 정확한 답변을 제공할 수 없습니다.', 'rejected': '애플은 리사를 위해 고객 서비스 부서에서 고객 다양한 컴퓨터 관련 문제에 대해 응답하는 데 필요한 모든 지원을 제공했습니다. 사용자가 하드웨어 문제를 경험할 때, 전문가들은 필요한 수리(수리, 추가 부품 제공, 소프트웨어 업그레이드 등)을 제공해 드릴 수 있습니다. 또한, 사용자가 사용 방법 문제나 기타 문제를 경험할 때, 대화 상대로 사용자를 지원할 수 있는 전문 고객 서비스 직원들이 사용자에게 상담하고 도움을 주는 데 도움이 될 수 있는 정보를 제공합니다. 또한, 인터넷에서 제공되는 정보를 통해 문제를 해결하거나 고객 서비스 웹 사이트를 통해 자신의 문제를 진단할 수 있도록 하는 등 다양한 방법으로 리사를 처리해 왔습니다.'}\n"
     ]
    }
   ],
   "source": [
    "with open('./data_kochatgpt/kochatgpt_2_RM.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "\n",
    "total_data_ranking2chosen = []\n",
    "for tmp in list_data_dict:\n",
    "    one_data_ranking2chosen = []\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][1]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][0] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_0']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_0']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "    data = {}\n",
    "    data['prompt'] = tmp['prompt']\n",
    "    if tmp['ranking'][1] < tmp['ranking'][2]:\n",
    "        data['chosen'] = tmp['completion_1']\n",
    "        data['rejected'] = tmp['completion_2']\n",
    "    else:\n",
    "        data['chosen'] = tmp['completion_2']\n",
    "        data['rejected'] = tmp['completion_1']\n",
    "    one_data_ranking2chosen.append(data)\n",
    "\n",
    "\n",
    "\n",
    "    total_data_ranking2chosen.extend(one_data_ranking2chosen)\n",
    "\n",
    "print('before data num: %d'%(len(list_data_dict)))\n",
    "print('after  data num: %d'%(len(total_data_ranking2chosen)))\n",
    "print('data example: \\n%s'%total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1b04ce21",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'prompt': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은?', 'chosen': '유아인이 류승완 감독을 만나 영화 베테랑의 시나리오를 받았던 곳은 류승완의 사무실입니다.', 'rejected': '대구 영화사옥'}\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "random.seed(230319)\n",
    "random.shuffle(total_data_ranking2chosen)\n",
    "print(total_data_ranking2chosen[45])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b9941245",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = total_data_ranking2chosen[:1000] \n",
    "eval_data = total_data_ranking2chosen[1000:1200]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "75f74a4d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1000\n",
      "200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1317.37it/s]\n",
      "100%|██████████| 200/200 [00:00<00:00, 1323.07it/s]\n"
     ]
    }
   ],
   "source": [
    "print(len(train_data))\n",
    "print(len(eval_data))\n",
    "\n",
    "train_dataset = RewardDataset(train_data, tokenizer, 512)\n",
    "eval_dataset = RewardDataset(eval_data, tokenizer, 512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "25a8fb96",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "######################################################################\n",
      "## prompt ##\n",
      "흑고래의 무게는 어느 정도야\n",
      "######################################################################\n",
      "## chosen ##\n",
      "흑고래의 평균 몸무게는 약 25~40톤 정도이지만, 최대 몸무게는 50톤 이상에 이를 수 있습니다.\n",
      "######################################################################\n",
      "## rejected ##\n",
      "흑고래의 무게는 매우 다양하게 달라집니다. 약 200kg에서 10톤까지 달라질 수 있습니다.\n"
     ]
    }
   ],
   "source": [
    "idx = 1\n",
    "print('#'*70)\n",
    "print('## prompt ##')\n",
    "print(train_data[idx]['prompt'])\n",
    "print('#'*70)\n",
    "print('## chosen ##')\n",
    "print(train_data[idx]['chosen'])\n",
    "print('#'*70)\n",
    "print('## rejected ##')\n",
    "print(train_data[idx]['rejected'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "460681c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = RewardModelTrainer(model=model,\n",
    "                             strategy=NaiveStrategy(),\n",
    "                             optim=Adam(model.parameters(), lr=5e-5),\n",
    "                             train_dataset=train_dataset,\n",
    "                             eval_dataset=eval_dataset,\n",
    "                             batch_size=4,\n",
    "                             max_epochs=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "df5bd7fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Train epoch:   0%|          | 0/3 [00:00<?, ?it/s]\n",
      "Train step of epoch 0:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:45,  1.14s/it]\u001b[A\n",
      "Train step of epoch 0:   0%|          | 1/250 [00:01<04:45,  1.14s/it, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:07,  1.00it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 2/250 [00:02<04:07,  1.00it/s, loss=0.768]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:52,  1.06it/s, loss=0.768]\u001b[A\n",
      "Train step of epoch 0:   1%|          | 3/250 [00:02<03:52,  1.06it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:45,  1.09it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 4/250 [00:03<03:45,  1.09it/s, loss=0.53] \u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:41,  1.11it/s, loss=0.53]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 5/250 [00:04<03:41,  1.11it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:38,  1.11it/s, loss=0.725]\u001b[A\n",
      "Train step of epoch 0:   2%|▏         | 6/250 [00:05<03:38,  1.11it/s, loss=0.289]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.289]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.341]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:35,  1.12it/s, loss=0.341]\u001b[A\n",
      "Train step of epoch 0:   3%|▎         | 8/250 [00:07<03:35,  1.12it/s, loss=2.05] \u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:34,  1.13it/s, loss=2.05]\u001b[A\n",
      "Train step of epoch 0:   4%|▎         | 9/250 [00:08<03:34,  1.13it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:33,  1.13it/s, loss=0.473]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 10/250 [00:09<03:33,  1.13it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:09<03:31,  1.13it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:   4%|▍         | 11/250 [00:10<03:31,  1.13it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:30,  1.13it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:   5%|▍         | 12/250 [00:10<03:30,  1.13it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:30,  1.13it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 0:   5%|▌         | 13/250 [00:11<03:30,  1.13it/s, loss=0.0754]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:29,  1.13it/s, loss=0.0754]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 14/250 [00:12<03:29,  1.13it/s, loss=1.42]  \u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:28,  1.12it/s, loss=1.42]\u001b[A\n",
      "Train step of epoch 0:   6%|▌         | 15/250 [00:13<03:28,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:28,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 0:   6%|▋         | 16/250 [00:14<03:28,  1.12it/s, loss=1.58] \u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:27,  1.12it/s, loss=1.58]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 17/250 [00:15<03:27,  1.12it/s, loss=0.746]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:26,  1.12it/s, loss=0.746]\u001b[A\n",
      "Train step of epoch 0:   7%|▋         | 18/250 [00:16<03:26,  1.12it/s, loss=0.489]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:25,  1.12it/s, loss=0.489]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 19/250 [00:17<03:25,  1.12it/s, loss=0.683]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:25,  1.12it/s, loss=0.683]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 20/250 [00:18<03:25,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:24,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 0:   8%|▊         | 21/250 [00:18<03:24,  1.12it/s, loss=0.7]  \u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:24,  1.11it/s, loss=0.7]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 22/250 [00:19<03:24,  1.11it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:23,  1.11it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 0:   9%|▉         | 23/250 [00:20<03:23,  1.11it/s, loss=1.19] \u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:23,  1.11it/s, loss=1.19]\u001b[A\n",
      "Train step of epoch 0:  10%|▉         | 24/250 [00:21<03:23,  1.11it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:22,  1.11it/s, loss=0.705]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 25/250 [00:22<03:22,  1.11it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:21,  1.11it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 0:  10%|█         | 26/250 [00:23<03:21,  1.11it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:21,  1.11it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 27/250 [00:24<03:21,  1.11it/s, loss=0.67] \u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:20,  1.11it/s, loss=0.67]\u001b[A\n",
      "Train step of epoch 0:  11%|█         | 28/250 [00:25<03:20,  1.11it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:19,  1.11it/s, loss=0.398]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 29/250 [00:26<03:19,  1.11it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:19,  1.10it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 30/250 [00:27<03:19,  1.10it/s, loss=0.45] \u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:18,  1.10it/s, loss=0.45]\u001b[A\n",
      "Train step of epoch 0:  12%|█▏        | 31/250 [00:27<03:18,  1.10it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:17,  1.10it/s, loss=0.359]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 32/250 [00:28<03:17,  1.10it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:17,  1.10it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 0:  13%|█▎        | 33/250 [00:29<03:17,  1.10it/s, loss=0.409]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:30<03:16,  1.10it/s, loss=0.409]\u001b[A\n",
      "Train step of epoch 0:  14%|█▎        | 34/250 [00:30<03:16,  1.10it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:31<03:16,  1.10it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 35/250 [00:31<03:16,  1.10it/s, loss=0.86] \u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:32<03:15,  1.10it/s, loss=0.86]\u001b[A\n",
      "Train step of epoch 0:  14%|█▍        | 36/250 [00:32<03:15,  1.10it/s, loss=2.2] \u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:14,  1.09it/s, loss=2.2]\u001b[A\n",
      "Train step of epoch 0:  15%|█▍        | 37/250 [00:33<03:14,  1.09it/s, loss=0.876]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:14,  1.09it/s, loss=0.876]\u001b[A\n",
      "Train step of epoch 0:  15%|█▌        | 38/250 [00:34<03:14,  1.09it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:13,  1.09it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 39/250 [00:35<03:13,  1.09it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:12,  1.09it/s, loss=0.717]\u001b[A\n",
      "Train step of epoch 0:  16%|█▌        | 40/250 [00:36<03:12,  1.09it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:11,  1.09it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 0:  16%|█▋        | 41/250 [00:37<03:11,  1.09it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:11,  1.09it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 42/250 [00:38<03:11,  1.09it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:38<03:10,  1.09it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 0:  17%|█▋        | 43/250 [00:38<03:10,  1.09it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:39<03:09,  1.09it/s, loss=0.855]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 44/250 [00:39<03:09,  1.09it/s, loss=1.31] \u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:40<03:08,  1.09it/s, loss=1.31]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 45/250 [00:40<03:08,  1.09it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:41<03:07,  1.09it/s, loss=0.719]\u001b[A\n",
      "Train step of epoch 0:  18%|█▊        | 46/250 [00:41<03:07,  1.09it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:42<03:06,  1.09it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 47/250 [00:42<03:06,  1.09it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:43<03:05,  1.09it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  19%|█▉        | 48/250 [00:43<03:05,  1.09it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:04,  1.09it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 0:  20%|█▉        | 49/250 [00:44<03:04,  1.09it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:02,  1.09it/s, loss=0.562]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 50/250 [00:45<03:02,  1.09it/s, loss=0.828]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<03:01,  1.10it/s, loss=0.828]\u001b[A\n",
      "Train step of epoch 0:  20%|██        | 51/250 [00:46<03:01,  1.10it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<03:00,  1.10it/s, loss=0.429]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 52/250 [00:47<03:00,  1.10it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:59,  1.10it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  21%|██        | 53/250 [00:48<02:59,  1.10it/s, loss=0.45]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:48<02:58,  1.10it/s, loss=0.45]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 54/250 [00:49<02:58,  1.10it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:49<02:56,  1.10it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 55/250 [00:49<02:56,  1.10it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:50<02:55,  1.11it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  22%|██▏       | 56/250 [00:50<02:55,  1.11it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:51<02:54,  1.11it/s, loss=0.575]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 57/250 [00:51<02:54,  1.11it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:52<02:53,  1.11it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 0:  23%|██▎       | 58/250 [00:52<02:53,  1.11it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:53<02:52,  1.11it/s, loss=0.442]\u001b[A\n",
      "Train step of epoch 0:  24%|██▎       | 59/250 [00:53<02:52,  1.11it/s, loss=0.58] \u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:50,  1.11it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 60/250 [00:54<02:50,  1.11it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:49,  1.11it/s, loss=0.248]\u001b[A\n",
      "Train step of epoch 0:  24%|██▍       | 61/250 [00:55<02:49,  1.11it/s, loss=0.903]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:48,  1.11it/s, loss=0.903]\u001b[A\n",
      "Train step of epoch 0:  25%|██▍       | 62/250 [00:56<02:48,  1.11it/s, loss=0.215]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:47,  1.11it/s, loss=0.215]\u001b[A\n",
      "Train step of epoch 0:  25%|██▌       | 63/250 [00:57<02:47,  1.11it/s, loss=0.975]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.975]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.36] \u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.36]\u001b[A\n",
      "Train step of epoch 0:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.788]\u001b[A\n",
      "Train step of epoch 0:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:00<02:43,  1.12it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 67/250 [01:00<02:43,  1.12it/s, loss=0.849]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:01<02:42,  1.12it/s, loss=0.849]\u001b[A\n",
      "Train step of epoch 0:  27%|██▋       | 68/250 [01:01<02:42,  1.12it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:02<02:41,  1.12it/s, loss=0.322]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 69/250 [01:02<02:41,  1.12it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 70/250 [01:03<02:40,  1.12it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:39,  1.12it/s, loss=0.757]\u001b[A\n",
      "Train step of epoch 0:  28%|██▊       | 71/250 [01:04<02:39,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:38,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 72/250 [01:05<02:38,  1.12it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:05<02:37,  1.12it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  29%|██▉       | 73/250 [01:06<02:37,  1.12it/s, loss=0.873]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:06<02:36,  1.12it/s, loss=0.873]\u001b[A\n",
      "Train step of epoch 0:  30%|██▉       | 74/250 [01:06<02:36,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:07<02:35,  1.12it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 75/250 [01:07<02:35,  1.12it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:08<02:34,  1.13it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 0:  30%|███       | 76/250 [01:08<02:34,  1.13it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:09<02:33,  1.13it/s, loss=0.752]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 77/250 [01:09<02:33,  1.13it/s, loss=0.405]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:10<02:32,  1.12it/s, loss=0.405]\u001b[A\n",
      "Train step of epoch 0:  31%|███       | 78/250 [01:10<02:32,  1.12it/s, loss=0.69] \u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:11<02:31,  1.13it/s, loss=0.69]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 79/250 [01:11<02:31,  1.13it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:30,  1.13it/s, loss=0.493]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 80/250 [01:12<02:30,  1.13it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:29,  1.13it/s, loss=0.722]\u001b[A\n",
      "Train step of epoch 0:  32%|███▏      | 81/250 [01:13<02:29,  1.13it/s, loss=0.361]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:13<02:28,  1.13it/s, loss=0.361]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 82/250 [01:13<02:28,  1.13it/s, loss=0.851]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:14<02:27,  1.13it/s, loss=0.851]\u001b[A\n",
      "Train step of epoch 0:  33%|███▎      | 83/250 [01:14<02:27,  1.13it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:15<02:27,  1.13it/s, loss=0.559]\u001b[A\n",
      "Train step of epoch 0:  34%|███▎      | 84/250 [01:15<02:27,  1.13it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:16<02:26,  1.13it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 85/250 [01:16<02:26,  1.13it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:17<02:25,  1.13it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  34%|███▍      | 86/250 [01:17<02:25,  1.13it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:18<02:24,  1.13it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 0:  35%|███▍      | 87/250 [01:18<02:24,  1.13it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:19<02:23,  1.13it/s, loss=0.533]\u001b[A\n",
      "Train step of epoch 0:  35%|███▌      | 88/250 [01:19<02:23,  1.13it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:20<02:22,  1.13it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 89/250 [01:20<02:22,  1.13it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:21,  1.13it/s, loss=0.787]\u001b[A\n",
      "Train step of epoch 0:  36%|███▌      | 90/250 [01:21<02:21,  1.13it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:21<02:20,  1.13it/s, loss=0.718]\u001b[A\n",
      "Train step of epoch 0:  36%|███▋      | 91/250 [01:21<02:20,  1.13it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:22<02:19,  1.13it/s, loss=0.574]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 92/250 [01:22<02:19,  1.13it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:23<02:18,  1.13it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 0:  37%|███▋      | 93/250 [01:23<02:18,  1.13it/s, loss=0.915]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:24<02:18,  1.13it/s, loss=0.915]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 94/250 [01:24<02:18,  1.13it/s, loss=0.837]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:25<02:17,  1.13it/s, loss=0.837]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 95/250 [01:25<02:17,  1.13it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:26<02:16,  1.13it/s, loss=0.475]\u001b[A\n",
      "Train step of epoch 0:  38%|███▊      | 96/250 [01:26<02:16,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:27<02:15,  1.13it/s, loss=0.676]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 97/250 [01:27<02:15,  1.13it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:28<02:14,  1.13it/s, loss=0.785]\u001b[A\n",
      "Train step of epoch 0:  39%|███▉      | 98/250 [01:28<02:14,  1.13it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:14,  1.13it/s, loss=0.731]\u001b[A\n",
      "Train step of epoch 0:  40%|███▉      | 99/250 [01:29<02:14,  1.13it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:29<02:13,  1.13it/s, loss=0.511]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 100/250 [01:29<02:13,  1.13it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:30<02:12,  1.13it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  40%|████      | 101/250 [01:30<02:12,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:31<02:11,  1.13it/s, loss=0.756]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 102/250 [01:31<02:11,  1.13it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:32<02:10,  1.13it/s, loss=0.668]\u001b[A\n",
      "Train step of epoch 0:  41%|████      | 103/250 [01:32<02:10,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:33<02:09,  1.13it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 104/250 [01:33<02:09,  1.13it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:34<02:08,  1.13it/s, loss=0.749]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 105/250 [01:34<02:08,  1.13it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:35<02:07,  1.13it/s, loss=0.646]\u001b[A\n",
      "Train step of epoch 0:  42%|████▏     | 106/250 [01:35<02:07,  1.13it/s, loss=0.76] \u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:36<02:07,  1.13it/s, loss=0.76]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 107/250 [01:36<02:07,  1.13it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:37<02:06,  1.13it/s, loss=0.761]\u001b[A\n",
      "Train step of epoch 0:  43%|████▎     | 108/250 [01:37<02:06,  1.13it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:37<02:05,  1.12it/s, loss=0.601]\u001b[A\n",
      "Train step of epoch 0:  44%|████▎     | 109/250 [01:37<02:05,  1.12it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:38<02:04,  1.12it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 110/250 [01:38<02:04,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:39<02:03,  1.12it/s, loss=0.744]\u001b[A\n",
      "Train step of epoch 0:  44%|████▍     | 111/250 [01:39<02:03,  1.12it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:40<02:02,  1.12it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 0:  45%|████▍     | 112/250 [01:40<02:02,  1.12it/s, loss=0.736]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:41<02:01,  1.12it/s, loss=0.736]\u001b[A\n",
      "Train step of epoch 0:  45%|████▌     | 113/250 [01:41<02:01,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:42<02:01,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 114/250 [01:42<02:01,  1.12it/s, loss=0.908]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:43<02:00,  1.12it/s, loss=0.908]\u001b[A\n",
      "Train step of epoch 0:  46%|████▌     | 115/250 [01:43<02:00,  1.12it/s, loss=0.78] \u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:44<01:59,  1.12it/s, loss=0.78]\u001b[A\n",
      "Train step of epoch 0:  46%|████▋     | 116/250 [01:44<01:59,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:45<01:58,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 117/250 [01:45<01:58,  1.12it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:45<01:57,  1.12it/s, loss=0.613]\u001b[A\n",
      "Train step of epoch 0:  47%|████▋     | 118/250 [01:45<01:57,  1.12it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:46<01:56,  1.12it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 119/250 [01:46<01:56,  1.12it/s, loss=0.454]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=0.454]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.778]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:50<01:53,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 0:  49%|████▉     | 123/250 [01:50<01:53,  1.12it/s, loss=0.56] \u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:51<01:52,  1.12it/s, loss=0.56]\u001b[A\n",
      "Train step of epoch 0:  50%|████▉     | 124/250 [01:51<01:52,  1.12it/s, loss=0.793]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:52<01:51,  1.12it/s, loss=0.793]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 125/250 [01:52<01:51,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:53<01:51,  1.12it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  50%|█████     | 126/250 [01:53<01:51,  1.12it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:53<01:50,  1.12it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 127/250 [01:54<01:50,  1.12it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=0.593]\u001b[A\n",
      "Train step of epoch 0:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=0.672]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=0.57] \u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=0.57]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 0:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=0.976]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:58<01:46,  1.11it/s, loss=0.976]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 132/250 [01:58<01:46,  1.11it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:59<01:45,  1.11it/s, loss=0.615]\u001b[A\n",
      "Train step of epoch 0:  53%|█████▎    | 133/250 [01:59<01:45,  1.11it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:00<01:44,  1.11it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▎    | 134/250 [02:00<01:44,  1.11it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:01<01:43,  1.11it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 135/250 [02:01<01:43,  1.11it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:02<01:42,  1.11it/s, loss=0.689]\u001b[A\n",
      "Train step of epoch 0:  54%|█████▍    | 136/250 [02:02<01:42,  1.11it/s, loss=0.715]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:02<01:41,  1.11it/s, loss=0.715]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▍    | 137/250 [02:02<01:41,  1.11it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:03<01:40,  1.11it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 0:  55%|█████▌    | 138/250 [02:03<01:40,  1.11it/s, loss=0.62] \u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:04<01:39,  1.11it/s, loss=0.62]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 139/250 [02:04<01:39,  1.11it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:05<01:38,  1.11it/s, loss=0.517]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▌    | 140/250 [02:05<01:38,  1.11it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:06<01:37,  1.11it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  56%|█████▋    | 141/250 [02:06<01:37,  1.11it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:07<01:36,  1.11it/s, loss=0.723]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 142/250 [02:07<01:36,  1.11it/s, loss=0.79] \u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:08<01:36,  1.11it/s, loss=0.79]\u001b[A\n",
      "Train step of epoch 0:  57%|█████▋    | 143/250 [02:08<01:36,  1.11it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:09<01:35,  1.11it/s, loss=0.568]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 144/250 [02:09<01:35,  1.11it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:10<01:34,  1.11it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 145/250 [02:10<01:34,  1.11it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:11<01:33,  1.11it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 0:  58%|█████▊    | 146/250 [02:11<01:33,  1.11it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:11<01:32,  1.11it/s, loss=0.623]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 147/250 [02:11<01:32,  1.11it/s, loss=0.373]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:12<01:31,  1.11it/s, loss=0.373]\u001b[A\n",
      "Train step of epoch 0:  59%|█████▉    | 148/250 [02:12<01:31,  1.11it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:13<01:30,  1.11it/s, loss=0.639]\u001b[A\n",
      "Train step of epoch 0:  60%|█████▉    | 149/250 [02:13<01:30,  1.11it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:14<01:29,  1.11it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 150/250 [02:14<01:29,  1.11it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:15<01:28,  1.11it/s, loss=0.595]\u001b[A\n",
      "Train step of epoch 0:  60%|██████    | 151/250 [02:15<01:28,  1.11it/s, loss=0.32] \u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:16<01:28,  1.11it/s, loss=0.32]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 152/250 [02:16<01:28,  1.11it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:17<01:27,  1.11it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  61%|██████    | 153/250 [02:17<01:27,  1.11it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:18<01:26,  1.11it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 154/250 [02:18<01:26,  1.11it/s, loss=1.01] \u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:19<01:25,  1.11it/s, loss=1.01]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 155/250 [02:19<01:25,  1.11it/s, loss=0.92]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:20<01:24,  1.11it/s, loss=0.92]\u001b[A\n",
      "Train step of epoch 0:  62%|██████▏   | 156/250 [02:20<01:24,  1.11it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:20<01:23,  1.11it/s, loss=0.468]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 157/250 [02:20<01:23,  1.11it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:21<01:22,  1.11it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  63%|██████▎   | 158/250 [02:21<01:22,  1.11it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.681]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.43] \u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:24<01:19,  1.11it/s, loss=0.43]\u001b[A\n",
      "Train step of epoch 0:  64%|██████▍   | 161/250 [02:24<01:19,  1.11it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:25<01:18,  1.11it/s, loss=0.698]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▍   | 162/250 [02:25<01:18,  1.11it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:26<01:17,  1.12it/s, loss=0.484]\u001b[A\n",
      "Train step of epoch 0:  65%|██████▌   | 163/250 [02:26<01:17,  1.12it/s, loss=0.319]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:27<01:17,  1.12it/s, loss=0.319]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 164/250 [02:27<01:17,  1.12it/s, loss=0.61] \u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:28<01:16,  1.12it/s, loss=0.61]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▌   | 165/250 [02:28<01:16,  1.12it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=1.14]\u001b[A\n",
      "Train step of epoch 0:  66%|██████▋   | 166/250 [02:29<01:15,  1.12it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.436]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.436]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:32<01:11,  1.12it/s, loss=0.392]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 170/250 [02:32<01:11,  1.12it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:33<01:10,  1.12it/s, loss=0.896]\u001b[A\n",
      "Train step of epoch 0:  68%|██████▊   | 171/250 [02:33<01:10,  1.12it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:34<01:09,  1.12it/s, loss=0.806]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 172/250 [02:34<01:09,  1.12it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:35<01:08,  1.12it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 0:  69%|██████▉   | 173/250 [02:35<01:08,  1.12it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:36<01:07,  1.12it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:  70%|██████▉   | 174/250 [02:36<01:07,  1.12it/s, loss=0.33] \u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:37<01:06,  1.12it/s, loss=0.33]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 175/250 [02:37<01:06,  1.12it/s, loss=0.63]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.63]\u001b[A\n",
      "Train step of epoch 0:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.412]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.414]\u001b[A\n",
      "Train step of epoch 0:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:40<01:03,  1.12it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 179/250 [02:40<01:03,  1.12it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:41<01:02,  1.12it/s, loss=0.536]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 180/250 [02:41<01:02,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:42<01:01,  1.12it/s, loss=0.684]\u001b[A\n",
      "Train step of epoch 0:  72%|███████▏  | 181/250 [02:42<01:01,  1.12it/s, loss=0.316]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:43<01:00,  1.12it/s, loss=0.316]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 182/250 [02:43<01:00,  1.12it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:44<00:59,  1.12it/s, loss=0.804]\u001b[A\n",
      "Train step of epoch 0:  73%|███████▎  | 183/250 [02:44<00:59,  1.12it/s, loss=1.17] \u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:45<00:58,  1.12it/s, loss=1.17]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▎  | 184/250 [02:45<00:58,  1.12it/s, loss=0.93]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:45<00:57,  1.12it/s, loss=0.93]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 185/250 [02:45<00:57,  1.12it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:46<00:56,  1.12it/s, loss=0.543]\u001b[A\n",
      "Train step of epoch 0:  74%|███████▍  | 186/250 [02:46<00:56,  1.12it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.343]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.357]\u001b[A\n",
      "Train step of epoch 0:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:49<00:54,  1.12it/s, loss=0.542]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 189/250 [02:49<00:54,  1.12it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:50<00:53,  1.12it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▌  | 190/250 [02:50<00:53,  1.12it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:51<00:52,  1.12it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 0:  76%|███████▋  | 191/250 [02:51<00:52,  1.12it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:52<00:51,  1.12it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 192/250 [02:52<00:51,  1.12it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:53<00:50,  1.12it/s, loss=0.594]\u001b[A\n",
      "Train step of epoch 0:  77%|███████▋  | 193/250 [02:53<00:50,  1.12it/s, loss=1.06] \u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:53<00:49,  1.12it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 194/250 [02:53<00:49,  1.12it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.387]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=0.387]\u001b[A\n",
      "Train step of epoch 0:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=1.06] \u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 0:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.51] \u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:58<00:45,  1.12it/s, loss=0.51]\u001b[A\n",
      "Train step of epoch 0:  80%|███████▉  | 199/250 [02:58<00:45,  1.12it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:59<00:44,  1.12it/s, loss=0.467]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 200/250 [02:59<00:44,  1.12it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:00<00:43,  1.12it/s, loss=0.443]\u001b[A\n",
      "Train step of epoch 0:  80%|████████  | 201/250 [03:00<00:43,  1.12it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:01<00:42,  1.12it/s, loss=0.492]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 202/250 [03:01<00:42,  1.12it/s, loss=0.799]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:01<00:41,  1.12it/s, loss=0.799]\u001b[A\n",
      "Train step of epoch 0:  81%|████████  | 203/250 [03:01<00:41,  1.12it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:02<00:40,  1.12it/s, loss=0.618]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 204/250 [03:02<00:40,  1.12it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 0:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.798]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:06<00:37,  1.12it/s, loss=0.534]\u001b[A\n",
      "Train step of epoch 0:  83%|████████▎ | 208/250 [03:06<00:37,  1.12it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:07<00:36,  1.12it/s, loss=0.637]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▎ | 209/250 [03:07<00:36,  1.12it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:08<00:35,  1.12it/s, loss=0.729]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 210/250 [03:08<00:35,  1.12it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:09<00:34,  1.12it/s, loss=0.596]\u001b[A\n",
      "Train step of epoch 0:  84%|████████▍ | 211/250 [03:09<00:34,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▍ | 212/250 [03:10<00:33,  1.12it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:10<00:32,  1.12it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 0:  85%|████████▌ | 213/250 [03:10<00:32,  1.12it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=1.02] \u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=1.02]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 0:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:15<00:28,  1.12it/s, loss=0.834]\u001b[A\n",
      "Train step of epoch 0:  87%|████████▋ | 218/250 [03:15<00:28,  1.12it/s, loss=0.577]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:16<00:27,  1.12it/s, loss=0.577]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 219/250 [03:16<00:27,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:17<00:26,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 220/250 [03:17<00:26,  1.12it/s, loss=0.58] \u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:18<00:25,  1.12it/s, loss=0.58]\u001b[A\n",
      "Train step of epoch 0:  88%|████████▊ | 221/250 [03:18<00:25,  1.12it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:18<00:24,  1.12it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 222/250 [03:18<00:24,  1.12it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.358]\u001b[A\n",
      "Train step of epoch 0:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.72] \u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 0:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=0.774]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.774]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.633]\u001b[A\n",
      "Train step of epoch 0:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.68] \u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:23<00:20,  1.12it/s, loss=0.68]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 227/250 [03:23<00:20,  1.12it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:24<00:19,  1.12it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  91%|█████████ | 228/250 [03:24<00:19,  1.12it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:25<00:18,  1.12it/s, loss=0.494]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 229/250 [03:25<00:18,  1.12it/s, loss=0.769]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:26<00:17,  1.12it/s, loss=0.769]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 230/250 [03:26<00:17,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.671]\u001b[A\n",
      "Train step of epoch 0:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.415]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.8]  \u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.8]\u001b[A\n",
      "Train step of epoch 0:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.588]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=0.935]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:31<00:12,  1.12it/s, loss=0.935]\u001b[A\n",
      "Train step of epoch 0:  94%|█████████▍| 236/250 [03:31<00:12,  1.12it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:32<00:11,  1.12it/s, loss=0.529]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▍| 237/250 [03:32<00:11,  1.12it/s, loss=0.569]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:33<00:10,  1.12it/s, loss=0.569]\u001b[A\n",
      "Train step of epoch 0:  95%|█████████▌| 238/250 [03:33<00:10,  1.12it/s, loss=0.774]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:34<00:09,  1.12it/s, loss=0.774]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 239/250 [03:34<00:09,  1.12it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.714]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.617]\u001b[A\n",
      "Train step of epoch 0:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.531]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.877]\u001b[A\n",
      "Train step of epoch 0:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.716]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:40<00:03,  1.12it/s, loss=0.716]\u001b[A\n",
      "Train step of epoch 0:  98%|█████████▊| 246/250 [03:40<00:03,  1.12it/s, loss=0.85] \u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:41<00:02,  1.12it/s, loss=0.85]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 247/250 [03:41<00:02,  1.12it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:42<00:01,  1.12it/s, loss=0.419]\u001b[A\n",
      "Train step of epoch 0:  99%|█████████▉| 248/250 [03:42<00:01,  1.12it/s, loss=1.25] \u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:43<00:00,  1.12it/s, loss=1.25]\u001b[A\n",
      "Train step of epoch 0: 100%|█████████▉| 249/250 [03:43<00:00,  1.12it/s, loss=0.733]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:43<00:00,  1.12it/s, loss=0.733]\u001b[A\n",
      "Train epoch:  33%|███▎      | 1/3 [03:58<07:57, 238.63s/it]0,  1.12it/s, loss=0.509]\u001b[A\n",
      "Train step of epoch 0: 100%|██████████| 250/250 [03:58<00:00,  1.05it/s, loss=0.618, dist_mean=0.302]\u001b[A\n",
      "\n",
      "Train step of epoch 1:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 1:   0%|          | 1/250 [00:00<03:37,  1.15it/s]\u001b[A\n",
      "Train step of epoch 1:   0%|          | 1/250 [00:00<03:37,  1.15it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 1:   1%|          | 2/250 [00:01<03:40,  1.13it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 1:   1%|          | 2/250 [00:01<03:40,  1.13it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 1:   1%|          | 3/250 [00:02<03:39,  1.13it/s, loss=0.435]\u001b[A\n",
      "Train step of epoch 1:   1%|          | 3/250 [00:02<03:39,  1.13it/s, loss=0.57] \u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 4/250 [00:03<03:38,  1.12it/s, loss=0.57]\u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 4/250 [00:03<03:38,  1.12it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 5/250 [00:04<03:38,  1.12it/s, loss=0.586]\u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 5/250 [00:04<03:38,  1.12it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 6/250 [00:05<03:38,  1.12it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 1:   2%|▏         | 6/250 [00:05<03:38,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 1:   3%|▎         | 8/250 [00:07<03:36,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 1:   3%|▎         | 8/250 [00:07<03:36,  1.12it/s, loss=0.59] \u001b[A\n",
      "Train step of epoch 1:   4%|▎         | 9/250 [00:08<03:35,  1.12it/s, loss=0.59]\u001b[A\n",
      "Train step of epoch 1:   4%|▎         | 9/250 [00:08<03:35,  1.12it/s, loss=0.499]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 10/250 [00:08<03:34,  1.12it/s, loss=0.499]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 10/250 [00:08<03:34,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 11/250 [00:09<03:33,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   4%|▍         | 11/250 [00:09<03:33,  1.12it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:   5%|▍         | 12/250 [00:10<03:32,  1.12it/s, loss=0.416]\u001b[A\n",
      "Train step of epoch 1:   5%|▍         | 12/250 [00:10<03:32,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   5%|▌         | 13/250 [00:11<03:32,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:   5%|▌         | 13/250 [00:11<03:32,  1.12it/s, loss=0.181]\u001b[A\n",
      "Train step of epoch 1:   6%|▌         | 14/250 [00:12<03:31,  1.12it/s, loss=0.181]\u001b[A\n",
      "Train step of epoch 1:   6%|▌         | 14/250 [00:12<03:31,  1.12it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 1:   6%|▌         | 15/250 [00:13<03:30,  1.12it/s, loss=0.789]\u001b[A\n",
      "Train step of epoch 1:   6%|▌         | 15/250 [00:13<03:30,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 1:   6%|▋         | 16/250 [00:14<03:29,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 1:   6%|▋         | 16/250 [00:14<03:29,  1.12it/s, loss=0.616]\u001b[A\n",
      "Train step of epoch 1:   7%|▋         | 17/250 [00:15<03:28,  1.12it/s, loss=0.616]\u001b[A\n",
      "Train step of epoch 1:   7%|▋         | 17/250 [00:15<03:28,  1.12it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 1:   7%|▋         | 18/250 [00:16<03:27,  1.12it/s, loss=0.556]\u001b[A\n",
      "Train step of epoch 1:   7%|▋         | 18/250 [00:16<03:27,  1.12it/s, loss=0.233]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 19/250 [00:16<03:27,  1.12it/s, loss=0.233]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 19/250 [00:17<03:27,  1.12it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 20/250 [00:17<03:26,  1.12it/s, loss=0.732]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 20/250 [00:17<03:26,  1.12it/s, loss=0.186]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 21/250 [00:18<03:25,  1.12it/s, loss=0.186]\u001b[A\n",
      "Train step of epoch 1:   8%|▊         | 21/250 [00:18<03:25,  1.12it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 1:   9%|▉         | 22/250 [00:19<03:24,  1.12it/s, loss=0.905]\u001b[A\n",
      "Train step of epoch 1:   9%|▉         | 22/250 [00:19<03:24,  1.12it/s, loss=1.11] \u001b[A\n",
      "Train step of epoch 1:   9%|▉         | 23/250 [00:20<03:23,  1.12it/s, loss=1.11]\u001b[A\n",
      "Train step of epoch 1:   9%|▉         | 23/250 [00:20<03:23,  1.12it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 1:  10%|▉         | 24/250 [00:21<03:22,  1.12it/s, loss=0.728]\u001b[A\n",
      "Train step of epoch 1:  10%|▉         | 24/250 [00:21<03:22,  1.12it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 1:  10%|█         | 25/250 [00:22<03:21,  1.12it/s, loss=0.656]\u001b[A\n",
      "Train step of epoch 1:  10%|█         | 25/250 [00:22<03:21,  1.12it/s, loss=0.33] \u001b[A\n",
      "Train step of epoch 1:  10%|█         | 26/250 [00:23<03:20,  1.12it/s, loss=0.33]\u001b[A\n",
      "Train step of epoch 1:  10%|█         | 26/250 [00:23<03:20,  1.12it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 1:  11%|█         | 27/250 [00:24<03:19,  1.12it/s, loss=0.759]\u001b[A\n",
      "Train step of epoch 1:  11%|█         | 27/250 [00:24<03:19,  1.12it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 1:  11%|█         | 28/250 [00:25<03:18,  1.12it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 1:  11%|█         | 28/250 [00:25<03:18,  1.12it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 29/250 [00:25<03:17,  1.12it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 29/250 [00:25<03:17,  1.12it/s, loss=0.305]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 30/250 [00:26<03:16,  1.12it/s, loss=0.305]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 30/250 [00:26<03:16,  1.12it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 31/250 [00:27<03:16,  1.12it/s, loss=0.278]\u001b[A\n",
      "Train step of epoch 1:  12%|█▏        | 31/250 [00:27<03:16,  1.12it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 1:  13%|█▎        | 32/250 [00:28<03:15,  1.12it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 1:  13%|█▎        | 32/250 [00:28<03:15,  1.12it/s, loss=1.09] \u001b[A\n",
      "Train step of epoch 1:  13%|█▎        | 33/250 [00:29<03:14,  1.12it/s, loss=1.09]\u001b[A\n",
      "Train step of epoch 1:  13%|█▎        | 33/250 [00:29<03:14,  1.12it/s, loss=0.299]\u001b[A\n",
      "Train step of epoch 1:  14%|█▎        | 34/250 [00:30<03:13,  1.12it/s, loss=0.299]\u001b[A\n",
      "Train step of epoch 1:  14%|█▎        | 34/250 [00:30<03:13,  1.12it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 1:  14%|█▍        | 35/250 [00:31<03:12,  1.12it/s, loss=0.585]\u001b[A\n",
      "Train step of epoch 1:  14%|█▍        | 35/250 [00:31<03:12,  1.12it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 1:  14%|█▍        | 36/250 [00:32<03:11,  1.12it/s, loss=0.491]\u001b[A\n",
      "Train step of epoch 1:  14%|█▍        | 36/250 [00:32<03:11,  1.12it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 1:  15%|█▍        | 37/250 [00:33<03:10,  1.12it/s, loss=0.629]\u001b[A\n",
      "Train step of epoch 1:  15%|█▍        | 37/250 [00:33<03:10,  1.12it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 1:  15%|█▌        | 38/250 [00:33<03:10,  1.12it/s, loss=0.784]\u001b[A\n",
      "Train step of epoch 1:  15%|█▌        | 38/250 [00:34<03:10,  1.12it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.377]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 40/250 [00:35<03:08,  1.12it/s, loss=0.687]\u001b[A\n",
      "Train step of epoch 1:  16%|█▌        | 40/250 [00:35<03:08,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 1:  16%|█▋        | 41/250 [00:36<03:07,  1.12it/s, loss=0.465]\u001b[A\n",
      "Train step of epoch 1:  16%|█▋        | 41/250 [00:36<03:07,  1.12it/s, loss=0.221]\u001b[A\n",
      "Train step of epoch 1:  17%|█▋        | 42/250 [00:37<03:06,  1.12it/s, loss=0.221]\u001b[A\n",
      "Train step of epoch 1:  17%|█▋        | 42/250 [00:37<03:06,  1.12it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 1:  17%|█▋        | 43/250 [00:38<03:05,  1.12it/s, loss=0.518]\u001b[A\n",
      "Train step of epoch 1:  17%|█▋        | 43/250 [00:38<03:05,  1.12it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 44/250 [00:39<03:04,  1.12it/s, loss=0.458]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 44/250 [00:39<03:04,  1.12it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 45/250 [00:40<03:03,  1.12it/s, loss=0.773]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 45/250 [00:40<03:03,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 46/250 [00:41<03:02,  1.11it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 1:  18%|█▊        | 46/250 [00:41<03:02,  1.11it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 1:  19%|█▉        | 47/250 [00:42<03:01,  1.12it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 1:  19%|█▉        | 47/250 [00:42<03:01,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 1:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 1:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.839]\u001b[A\n",
      "Train step of epoch 1:  20%|█▉        | 49/250 [00:43<03:00,  1.12it/s, loss=0.839]\u001b[A\n",
      "Train step of epoch 1:  20%|█▉        | 49/250 [00:43<03:00,  1.12it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 1:  20%|██        | 50/250 [00:44<02:59,  1.12it/s, loss=0.507]\u001b[A\n",
      "Train step of epoch 1:  20%|██        | 50/250 [00:44<02:59,  1.12it/s, loss=0.92] \u001b[A\n",
      "Train step of epoch 1:  20%|██        | 51/250 [00:45<02:58,  1.12it/s, loss=0.92]\u001b[A\n",
      "Train step of epoch 1:  20%|██        | 51/250 [00:45<02:58,  1.12it/s, loss=0.327]\u001b[A\n",
      "Train step of epoch 1:  21%|██        | 52/250 [00:46<02:57,  1.12it/s, loss=0.327]\u001b[A\n",
      "Train step of epoch 1:  21%|██        | 52/250 [00:46<02:57,  1.12it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 1:  21%|██        | 53/250 [00:47<02:56,  1.12it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 1:  21%|██        | 53/250 [00:47<02:56,  1.12it/s, loss=0.347]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 54/250 [00:48<02:55,  1.12it/s, loss=0.347]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 54/250 [00:48<02:55,  1.12it/s, loss=0.151]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 55/250 [00:49<02:54,  1.12it/s, loss=0.151]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 55/250 [00:49<02:54,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 56/250 [00:50<02:53,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 1:  22%|██▏       | 56/250 [00:50<02:53,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  23%|██▎       | 57/250 [00:51<02:52,  1.12it/s, loss=0.603]\u001b[A\n",
      "Train step of epoch 1:  23%|██▎       | 57/250 [00:51<02:52,  1.12it/s, loss=0.451]\u001b[A\n",
      "Train step of epoch 1:  23%|██▎       | 58/250 [00:51<02:51,  1.12it/s, loss=0.451]\u001b[A\n",
      "Train step of epoch 1:  23%|██▎       | 58/250 [00:51<02:51,  1.12it/s, loss=0.64] \u001b[A\n",
      "Train step of epoch 1:  24%|██▎       | 59/250 [00:52<02:51,  1.12it/s, loss=0.64]\u001b[A\n",
      "Train step of epoch 1:  24%|██▎       | 59/250 [00:52<02:51,  1.12it/s, loss=0.297]\u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 60/250 [00:53<02:50,  1.12it/s, loss=0.297]\u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 60/250 [00:53<02:50,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 61/250 [00:54<02:49,  1.12it/s, loss=0.426]\u001b[A\n",
      "Train step of epoch 1:  24%|██▍       | 61/250 [00:54<02:49,  1.12it/s, loss=0.72] \u001b[A\n",
      "Train step of epoch 1:  25%|██▍       | 62/250 [00:55<02:48,  1.12it/s, loss=0.72]\u001b[A\n",
      "Train step of epoch 1:  25%|██▍       | 62/250 [00:55<02:48,  1.12it/s, loss=0.271]\u001b[A\n",
      "Train step of epoch 1:  25%|██▌       | 63/250 [00:56<02:47,  1.12it/s, loss=0.271]\u001b[A\n",
      "Train step of epoch 1:  25%|██▌       | 63/250 [00:56<02:47,  1.12it/s, loss=0.854]\u001b[A\n",
      "Train step of epoch 1:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.854]\u001b[A\n",
      "Train step of epoch 1:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 1:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.308]\u001b[A\n",
      "Train step of epoch 1:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 1:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.634]\u001b[A\n",
      "Train step of epoch 1:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.063]\u001b[A\n",
      "Train step of epoch 1:  27%|██▋       | 67/250 [00:59<02:44,  1.12it/s, loss=0.063]\u001b[A\n",
      "Train step of epoch 1:  27%|██▋       | 67/250 [00:59<02:44,  1.12it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 1:  27%|██▋       | 68/250 [01:00<02:42,  1.12it/s, loss=0.643]\u001b[A\n",
      "Train step of epoch 1:  27%|██▋       | 68/250 [01:00<02:42,  1.12it/s, loss=0.15] \u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 69/250 [01:01<02:42,  1.12it/s, loss=0.15]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 69/250 [01:01<02:42,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 70/250 [01:02<02:41,  1.12it/s, loss=0.833]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 70/250 [01:02<02:41,  1.12it/s, loss=0.735]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 71/250 [01:03<02:40,  1.12it/s, loss=0.735]\u001b[A\n",
      "Train step of epoch 1:  28%|██▊       | 71/250 [01:03<02:40,  1.12it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 1:  29%|██▉       | 72/250 [01:04<02:39,  1.12it/s, loss=0.422]\u001b[A\n",
      "Train step of epoch 1:  29%|██▉       | 72/250 [01:04<02:39,  1.12it/s, loss=0.994]\u001b[A\n",
      "Train step of epoch 1:  29%|██▉       | 73/250 [01:05<02:38,  1.12it/s, loss=0.994]\u001b[A\n",
      "Train step of epoch 1:  29%|██▉       | 73/250 [01:05<02:38,  1.12it/s, loss=1.12] \u001b[A\n",
      "Train step of epoch 1:  30%|██▉       | 74/250 [01:06<02:37,  1.12it/s, loss=1.12]\u001b[A\n",
      "Train step of epoch 1:  30%|██▉       | 74/250 [01:06<02:37,  1.12it/s, loss=0.206]\u001b[A\n",
      "Train step of epoch 1:  30%|███       | 75/250 [01:07<02:36,  1.12it/s, loss=0.206]\u001b[A\n",
      "Train step of epoch 1:  30%|███       | 75/250 [01:07<02:36,  1.12it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 1:  30%|███       | 76/250 [01:08<02:35,  1.12it/s, loss=0.726]\u001b[A\n",
      "Train step of epoch 1:  30%|███       | 76/250 [01:08<02:35,  1.12it/s, loss=0.911]\u001b[A\n",
      "Train step of epoch 1:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.911]\u001b[A\n",
      "Train step of epoch 1:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.292]\u001b[A\n",
      "Train step of epoch 1:  31%|███       | 78/250 [01:09<02:33,  1.12it/s, loss=0.292]\u001b[A\n",
      "Train step of epoch 1:  31%|███       | 78/250 [01:09<02:33,  1.12it/s, loss=0.54] \u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 79/250 [01:10<02:33,  1.12it/s, loss=0.54]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 79/250 [01:10<02:33,  1.12it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 80/250 [01:11<02:31,  1.12it/s, loss=0.389]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 80/250 [01:11<02:31,  1.12it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 81/250 [01:12<02:30,  1.12it/s, loss=0.561]\u001b[A\n",
      "Train step of epoch 1:  32%|███▏      | 81/250 [01:12<02:30,  1.12it/s, loss=0.3]  \u001b[A\n",
      "Train step of epoch 1:  33%|███▎      | 82/250 [01:13<02:30,  1.12it/s, loss=0.3]\u001b[A\n",
      "Train step of epoch 1:  33%|███▎      | 82/250 [01:13<02:30,  1.12it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 1:  33%|███▎      | 83/250 [01:14<02:29,  1.12it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 1:  33%|███▎      | 83/250 [01:14<02:29,  1.12it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 1:  34%|███▎      | 84/250 [01:15<02:28,  1.12it/s, loss=0.396]\u001b[A\n",
      "Train step of epoch 1:  34%|███▎      | 84/250 [01:15<02:28,  1.12it/s, loss=0.604]\u001b[A\n",
      "Train step of epoch 1:  34%|███▍      | 85/250 [01:16<02:27,  1.12it/s, loss=0.604]\u001b[A\n",
      "Train step of epoch 1:  34%|███▍      | 85/250 [01:16<02:27,  1.12it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 1:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.498]\u001b[A\n",
      "Train step of epoch 1:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.59] \u001b[A\n",
      "Train step of epoch 1:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.59]\u001b[A\n",
      "Train step of epoch 1:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.42]\u001b[A\n",
      "Train step of epoch 1:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.42]\u001b[A\n",
      "Train step of epoch 1:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.89]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 89/250 [01:19<02:24,  1.12it/s, loss=0.89]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 89/250 [01:19<02:24,  1.12it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 90/250 [01:20<02:23,  1.12it/s, loss=0.581]\u001b[A\n",
      "Train step of epoch 1:  36%|███▌      | 90/250 [01:20<02:23,  1.12it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 1:  36%|███▋      | 91/250 [01:21<02:22,  1.12it/s, loss=0.645]\u001b[A\n",
      "Train step of epoch 1:  36%|███▋      | 91/250 [01:21<02:22,  1.12it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 1:  37%|███▋      | 92/250 [01:22<02:21,  1.12it/s, loss=0.644]\u001b[A\n",
      "Train step of epoch 1:  37%|███▋      | 92/250 [01:22<02:21,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 1:  37%|███▋      | 93/250 [01:23<02:20,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 1:  37%|███▋      | 93/250 [01:23<02:20,  1.12it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 94/250 [01:24<02:19,  1.12it/s, loss=0.638]\u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 94/250 [01:24<02:19,  1.12it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 95/250 [01:25<02:18,  1.12it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 95/250 [01:25<02:18,  1.12it/s, loss=0.37] \u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 96/250 [01:25<02:17,  1.12it/s, loss=0.37]\u001b[A\n",
      "Train step of epoch 1:  38%|███▊      | 96/250 [01:25<02:17,  1.12it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 1:  39%|███▉      | 97/250 [01:26<02:16,  1.12it/s, loss=0.564]\u001b[A\n",
      "Train step of epoch 1:  39%|███▉      | 97/250 [01:26<02:16,  1.12it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 1:  39%|███▉      | 98/250 [01:27<02:15,  1.12it/s, loss=0.657]\u001b[A\n",
      "Train step of epoch 1:  39%|███▉      | 98/250 [01:27<02:15,  1.12it/s, loss=0.706]\u001b[A\n",
      "Train step of epoch 1:  40%|███▉      | 99/250 [01:28<02:14,  1.12it/s, loss=0.706]\u001b[A\n",
      "Train step of epoch 1:  40%|███▉      | 99/250 [01:28<02:14,  1.12it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 100/250 [01:29<02:14,  1.12it/s, loss=0.584]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 100/250 [01:29<02:14,  1.12it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 101/250 [01:30<02:13,  1.12it/s, loss=0.663]\u001b[A\n",
      "Train step of epoch 1:  40%|████      | 101/250 [01:30<02:13,  1.12it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 1:  41%|████      | 102/250 [01:31<02:12,  1.12it/s, loss=0.655]\u001b[A\n",
      "Train step of epoch 1:  41%|████      | 102/250 [01:31<02:12,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 1:  41%|████      | 103/250 [01:32<02:11,  1.12it/s, loss=0.669]\u001b[A\n",
      "Train step of epoch 1:  41%|████      | 103/250 [01:32<02:11,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 104/250 [01:33<02:10,  1.12it/s, loss=0.636]\u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 104/250 [01:33<02:10,  1.12it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 105/250 [01:33<02:09,  1.12it/s, loss=0.801]\u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 105/250 [01:33<02:09,  1.12it/s, loss=0.52] \u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 106/250 [01:34<02:08,  1.12it/s, loss=0.52]\u001b[A\n",
      "Train step of epoch 1:  42%|████▏     | 106/250 [01:34<02:08,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 1:  43%|████▎     | 107/250 [01:35<02:07,  1.12it/s, loss=0.614]\u001b[A\n",
      "Train step of epoch 1:  43%|████▎     | 107/250 [01:35<02:07,  1.12it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 1:  43%|████▎     | 108/250 [01:36<02:07,  1.12it/s, loss=0.641]\u001b[A\n",
      "Train step of epoch 1:  43%|████▎     | 108/250 [01:36<02:07,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 1:  44%|████▎     | 109/250 [01:37<02:06,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 1:  44%|████▎     | 109/250 [01:37<02:06,  1.12it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 110/250 [01:38<02:05,  1.12it/s, loss=0.664]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 110/250 [01:38<02:05,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 111/250 [01:39<02:04,  1.12it/s, loss=0.527]\u001b[A\n",
      "Train step of epoch 1:  44%|████▍     | 111/250 [01:39<02:04,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  45%|████▍     | 112/250 [01:40<02:03,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  45%|████▍     | 112/250 [01:40<02:03,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 1:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 1:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 1:  46%|████▌     | 114/250 [01:41<02:01,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 1:  46%|████▌     | 114/250 [01:42<02:01,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 1:  46%|████▌     | 115/250 [01:42<02:00,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 1:  46%|████▌     | 115/250 [01:42<02:00,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 1:  46%|████▋     | 116/250 [01:43<01:59,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 1:  46%|████▋     | 116/250 [01:43<01:59,  1.12it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 1:  47%|████▋     | 117/250 [01:44<01:58,  1.12it/s, loss=0.326]\u001b[A\n",
      "Train step of epoch 1:  47%|████▋     | 117/250 [01:44<01:58,  1.12it/s, loss=0.48] \u001b[A\n",
      "Train step of epoch 1:  47%|████▋     | 118/250 [01:45<01:58,  1.12it/s, loss=0.48]\u001b[A\n",
      "Train step of epoch 1:  47%|████▋     | 118/250 [01:45<01:58,  1.12it/s, loss=0.341]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 119/250 [01:46<01:57,  1.12it/s, loss=0.341]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 119/250 [01:46<01:57,  1.12it/s, loss=0.67] \u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=0.67]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=1.41]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=1.41]\u001b[A\n",
      "Train step of epoch 1:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 1:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.694]\u001b[A\n",
      "Train step of epoch 1:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 1:  49%|████▉     | 123/250 [01:50<01:53,  1.12it/s, loss=0.678]\u001b[A\n",
      "Train step of epoch 1:  49%|████▉     | 123/250 [01:50<01:53,  1.12it/s, loss=0.26] \u001b[A\n",
      "Train step of epoch 1:  50%|████▉     | 124/250 [01:50<01:52,  1.12it/s, loss=0.26]\u001b[A\n",
      "Train step of epoch 1:  50%|████▉     | 124/250 [01:50<01:52,  1.12it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 1:  50%|█████     | 125/250 [01:51<01:51,  1.12it/s, loss=0.659]\u001b[A\n",
      "Train step of epoch 1:  50%|█████     | 125/250 [01:51<01:51,  1.12it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 1:  50%|█████     | 126/250 [01:52<01:50,  1.12it/s, loss=0.693]\u001b[A\n",
      "Train step of epoch 1:  50%|█████     | 126/250 [01:52<01:50,  1.12it/s, loss=0.92] \u001b[A\n",
      "Train step of epoch 1:  51%|█████     | 127/250 [01:53<01:50,  1.12it/s, loss=0.92]\u001b[A\n",
      "Train step of epoch 1:  51%|█████     | 127/250 [01:53<01:50,  1.12it/s, loss=0.302]\u001b[A\n",
      "Train step of epoch 1:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=0.302]\u001b[A\n",
      "Train step of epoch 1:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=0.982]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=0.982]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=0.449]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=0.822]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=0.822]\u001b[A\n",
      "Train step of epoch 1:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 1:  53%|█████▎    | 132/250 [01:58<01:45,  1.12it/s, loss=0.599]\u001b[A\n",
      "Train step of epoch 1:  53%|█████▎    | 132/250 [01:58<01:45,  1.12it/s, loss=0.59] \u001b[A\n",
      "Train step of epoch 1:  53%|█████▎    | 133/250 [01:58<01:44,  1.12it/s, loss=0.59]\u001b[A\n",
      "Train step of epoch 1:  53%|█████▎    | 133/250 [01:59<01:44,  1.12it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.609]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.591]\u001b[A\n",
      "Train step of epoch 1:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 1:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.471]\u001b[A\n",
      "Train step of epoch 1:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 1:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.602]\u001b[A\n",
      "Train step of epoch 1:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.49] \u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 139/250 [02:04<01:39,  1.12it/s, loss=0.49]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 139/250 [02:04<01:39,  1.12it/s, loss=0.35]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 140/250 [02:05<01:38,  1.12it/s, loss=0.35]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▌    | 140/250 [02:05<01:38,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▋    | 141/250 [02:06<01:37,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 1:  56%|█████▋    | 141/250 [02:06<01:37,  1.12it/s, loss=0.445]\u001b[A\n",
      "Train step of epoch 1:  57%|█████▋    | 142/250 [02:07<01:36,  1.12it/s, loss=0.445]\u001b[A\n",
      "Train step of epoch 1:  57%|█████▋    | 142/250 [02:07<01:36,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 1:  57%|█████▋    | 143/250 [02:07<01:35,  1.12it/s, loss=0.701]\u001b[A\n",
      "Train step of epoch 1:  57%|█████▋    | 143/250 [02:07<01:35,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 144/250 [02:08<01:34,  1.12it/s, loss=0.439]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 144/250 [02:08<01:34,  1.12it/s, loss=0.409]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 145/250 [02:09<01:33,  1.12it/s, loss=0.409]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 145/250 [02:09<01:33,  1.12it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 146/250 [02:10<01:32,  1.12it/s, loss=0.501]\u001b[A\n",
      "Train step of epoch 1:  58%|█████▊    | 146/250 [02:10<01:32,  1.12it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 1:  59%|█████▉    | 147/250 [02:11<01:32,  1.12it/s, loss=0.469]\u001b[A\n",
      "Train step of epoch 1:  59%|█████▉    | 147/250 [02:11<01:32,  1.12it/s, loss=0.329]\u001b[A\n",
      "Train step of epoch 1:  59%|█████▉    | 148/250 [02:12<01:31,  1.12it/s, loss=0.329]\u001b[A\n",
      "Train step of epoch 1:  59%|█████▉    | 148/250 [02:12<01:31,  1.12it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 1:  60%|█████▉    | 149/250 [02:13<01:30,  1.12it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 1:  60%|█████▉    | 149/250 [02:13<01:30,  1.12it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 150/250 [02:14<01:29,  1.12it/s, loss=0.286]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 150/250 [02:14<01:29,  1.12it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 151/250 [02:15<01:28,  1.12it/s, loss=0.546]\u001b[A\n",
      "Train step of epoch 1:  60%|██████    | 151/250 [02:15<01:28,  1.12it/s, loss=0.15] \u001b[A\n",
      "Train step of epoch 1:  61%|██████    | 152/250 [02:15<01:27,  1.12it/s, loss=0.15]\u001b[A\n",
      "Train step of epoch 1:  61%|██████    | 152/250 [02:15<01:27,  1.12it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 1:  61%|██████    | 153/250 [02:16<01:26,  1.12it/s, loss=1.07]\u001b[A\n",
      "Train step of epoch 1:  61%|██████    | 153/250 [02:16<01:26,  1.12it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 154/250 [02:17<01:25,  1.12it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 154/250 [02:17<01:25,  1.12it/s, loss=1.2]  \u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 155/250 [02:18<01:24,  1.12it/s, loss=1.2]\u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 155/250 [02:18<01:24,  1.12it/s, loss=1.26]\u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 156/250 [02:19<01:24,  1.12it/s, loss=1.26]\u001b[A\n",
      "Train step of epoch 1:  62%|██████▏   | 156/250 [02:19<01:24,  1.12it/s, loss=0.25]\u001b[A\n",
      "Train step of epoch 1:  63%|██████▎   | 157/250 [02:20<01:23,  1.12it/s, loss=0.25]\u001b[A\n",
      "Train step of epoch 1:  63%|██████▎   | 157/250 [02:20<01:23,  1.12it/s, loss=0.432]\u001b[A\n",
      "Train step of epoch 1:  63%|██████▎   | 158/250 [02:21<01:22,  1.12it/s, loss=0.432]\u001b[A\n",
      "Train step of epoch 1:  63%|██████▎   | 158/250 [02:21<01:22,  1.12it/s, loss=1.18] \u001b[A\n",
      "Train step of epoch 1:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=1.18]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 161/250 [02:24<01:19,  1.12it/s, loss=0.371]\u001b[A\n",
      "Train step of epoch 1:  64%|██████▍   | 161/250 [02:24<01:19,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 1:  65%|██████▍   | 162/250 [02:24<01:18,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 1:  65%|██████▍   | 162/250 [02:24<01:18,  1.12it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 1:  65%|██████▌   | 163/250 [02:25<01:17,  1.12it/s, loss=0.462]\u001b[A\n",
      "Train step of epoch 1:  65%|██████▌   | 163/250 [02:25<01:17,  1.12it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▌   | 164/250 [02:26<01:16,  1.12it/s, loss=0.356]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▌   | 164/250 [02:26<01:16,  1.12it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▌   | 165/250 [02:27<01:15,  1.12it/s, loss=0.353]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▌   | 165/250 [02:27<01:15,  1.12it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=0.888]\u001b[A\n",
      "Train step of epoch 1:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=0.3]  \u001b[A\n",
      "Train step of epoch 1:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.3]\u001b[A\n",
      "Train step of epoch 1:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.261]\u001b[A\n",
      "Train step of epoch 1:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.261]\u001b[A\n",
      "Train step of epoch 1:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.315]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.315]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.29] \u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 170/250 [02:32<01:11,  1.12it/s, loss=0.29]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 170/250 [02:32<01:11,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.488]\u001b[A\n",
      "Train step of epoch 1:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 1:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 1:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 1:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.563]\u001b[A\n",
      "Train step of epoch 1:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.577]\u001b[A\n",
      "Train step of epoch 1:  70%|██████▉   | 174/250 [02:35<01:07,  1.12it/s, loss=0.577]\u001b[A\n",
      "Train step of epoch 1:  70%|██████▉   | 174/250 [02:35<01:07,  1.12it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 1:  70%|███████   | 175/250 [02:36<01:07,  1.12it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 1:  70%|███████   | 175/250 [02:36<01:07,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 1:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.503]\u001b[A\n",
      "Train step of epoch 1:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.355]\u001b[A\n",
      "Train step of epoch 1:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.355]\u001b[A\n",
      "Train step of epoch 1:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 1:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 1:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 179/250 [02:40<01:03,  1.12it/s, loss=0.191]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 179/250 [02:40<01:03,  1.12it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 180/250 [02:41<01:02,  1.12it/s, loss=0.425]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 180/250 [02:41<01:02,  1.12it/s, loss=0.217]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.217]\u001b[A\n",
      "Train step of epoch 1:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.217]\u001b[A\n",
      "Train step of epoch 1:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=0.217]\u001b[A\n",
      "Train step of epoch 1:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 1:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=0.812]\u001b[A\n",
      "Train step of epoch 1:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▍  | 185/250 [02:45<00:58,  1.12it/s, loss=0.692]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▍  | 185/250 [02:45<00:58,  1.12it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 1:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.139]\u001b[A\n",
      "Train step of epoch 1:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.139]\u001b[A\n",
      "Train step of epoch 1:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 1:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.251]\u001b[A\n",
      "Train step of epoch 1:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 189/250 [02:49<00:54,  1.12it/s, loss=0.506]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 189/250 [02:49<00:54,  1.12it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.103]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.103]\u001b[A\n",
      "Train step of epoch 1:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 1:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.111]\u001b[A\n",
      "Train step of epoch 1:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 1:  77%|███████▋  | 193/250 [02:52<00:50,  1.12it/s, loss=0.545]\u001b[A\n",
      "Train step of epoch 1:  77%|███████▋  | 193/250 [02:52<00:50,  1.12it/s, loss=1.03] \u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 194/250 [02:53<00:50,  1.12it/s, loss=1.03]\u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 194/250 [02:53<00:50,  1.12it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.797]\u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=0.236]\u001b[A\n",
      "Train step of epoch 1:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 1:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=0.682]\u001b[A\n",
      "Train step of epoch 1:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.446]\u001b[A\n",
      "Train step of epoch 1:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.275]\u001b[A\n",
      "Train step of epoch 1:  80%|███████▉  | 199/250 [02:57<00:45,  1.12it/s, loss=0.275]\u001b[A\n",
      "Train step of epoch 1:  80%|███████▉  | 199/250 [02:58<00:45,  1.12it/s, loss=0.238]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.238]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 1:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 1:  81%|████████  | 202/250 [03:00<00:43,  1.12it/s, loss=0.364]\u001b[A\n",
      "Train step of epoch 1:  81%|████████  | 202/250 [03:00<00:43,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 1:  81%|████████  | 203/250 [03:01<00:42,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 1:  81%|████████  | 203/250 [03:01<00:42,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 204/250 [03:02<00:41,  1.12it/s, loss=0.444]\u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 204/250 [03:02<00:41,  1.12it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.338]\u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.34] \u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.34]\u001b[A\n",
      "Train step of epoch 1:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.832]\u001b[A\n",
      "Train step of epoch 1:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.832]\u001b[A\n",
      "Train step of epoch 1:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 1:  83%|████████▎ | 208/250 [03:06<00:37,  1.12it/s, loss=0.223]\u001b[A\n",
      "Train step of epoch 1:  83%|████████▎ | 208/250 [03:06<00:37,  1.12it/s, loss=0.85] \u001b[A\n",
      "Train step of epoch 1:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=0.85]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.502]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.323]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.323]\u001b[A\n",
      "Train step of epoch 1:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.2]  \u001b[A\n",
      "Train step of epoch 1:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.2]\u001b[A\n",
      "Train step of epoch 1:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 1:  85%|████████▌ | 213/250 [03:10<00:33,  1.12it/s, loss=0.928]\u001b[A\n",
      "Train step of epoch 1:  85%|████████▌ | 213/250 [03:10<00:33,  1.12it/s, loss=0.2]  \u001b[A\n",
      "Train step of epoch 1:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=0.2]\u001b[A\n",
      "Train step of epoch 1:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=0.548]\u001b[A\n",
      "Train step of epoch 1:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=0.548]\u001b[A\n",
      "Train step of epoch 1:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 1:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.129]\u001b[A\n",
      "Train step of epoch 1:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 1:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 1:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 1:  87%|████████▋ | 218/250 [03:14<00:28,  1.12it/s, loss=0.612]\u001b[A\n",
      "Train step of epoch 1:  87%|████████▋ | 218/250 [03:15<00:28,  1.12it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.532]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.153]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 1:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 1:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.276]\u001b[A\n",
      "Train step of epoch 1:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.0718]\u001b[A\n",
      "Train step of epoch 1:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.0718]\u001b[A\n",
      "Train step of epoch 1:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.988] \u001b[A\n",
      "Train step of epoch 1:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=0.988]\u001b[A\n",
      "Train step of epoch 1:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 1:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.665]\u001b[A\n",
      "Train step of epoch 1:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 1:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 1:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 1:  91%|█████████ | 227/250 [03:23<00:20,  1.12it/s, loss=0.587]\u001b[A\n",
      "Train step of epoch 1:  91%|█████████ | 227/250 [03:23<00:20,  1.12it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 1:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.466]\u001b[A\n",
      "Train step of epoch 1:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.709]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.709]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 231/250 [03:26<00:17,  1.12it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 1:  92%|█████████▏| 231/250 [03:26<00:17,  1.12it/s, loss=0.0978]\u001b[A\n",
      "Train step of epoch 1:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.0978]\u001b[A\n",
      "Train step of epoch 1:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.47]  \u001b[A\n",
      "Train step of epoch 1:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.47]\u001b[A\n",
      "Train step of epoch 1:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 1:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.135]\u001b[A\n",
      "Train step of epoch 1:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.127]\u001b[A\n",
      "Train step of epoch 1:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=0.127]\u001b[A\n",
      "Train step of epoch 1:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=1.4]  \u001b[A\n",
      "Train step of epoch 1:  94%|█████████▍| 236/250 [03:31<00:12,  1.12it/s, loss=1.4]\u001b[A\n",
      "Train step of epoch 1:  94%|█████████▍| 236/250 [03:31<00:12,  1.12it/s, loss=0.48]\u001b[A\n",
      "Train step of epoch 1:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.48]\u001b[A\n",
      "Train step of epoch 1:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 1:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.481]\u001b[A\n",
      "Train step of epoch 1:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.525]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.783]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.51] \u001b[A\n",
      "Train step of epoch 1:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.51]\u001b[A\n",
      "Train step of epoch 1:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 1:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.241]\u001b[A\n",
      "Train step of epoch 1:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.27] \u001b[A\n",
      "Train step of epoch 1:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.27]\u001b[A\n",
      "Train step of epoch 1:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.351]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.461]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 246/250 [03:40<00:03,  1.12it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 1:  98%|█████████▊| 246/250 [03:40<00:03,  1.12it/s, loss=0.716]\u001b[A\n",
      "Train step of epoch 1:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.716]\u001b[A\n",
      "Train step of epoch 1:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 1:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 1:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=1.39] \u001b[A\n",
      "Train step of epoch 1: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=1.39]\u001b[A\n",
      "Train step of epoch 1: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=0.495]\u001b[A\n",
      "Train step of epoch 1: 100%|██████████| 250/250 [03:43<00:00,  1.12it/s, loss=0.495]\u001b[A\n",
      "Train epoch:  67%|██████▋   | 2/3 [07:56<03:58, 238.44s/it]0,  1.12it/s, loss=0.224]\u001b[A\n",
      "Train step of epoch 1: 100%|██████████| 250/250 [03:58<00:00,  1.05it/s, loss=0.794, dist_mean=0.497]\u001b[A\n",
      "\n",
      "Train step of epoch 2:   0%|          | 0/250 [00:00<?, ?it/s]\u001b[A\n",
      "Train step of epoch 2:   0%|          | 1/250 [00:00<03:37,  1.14it/s]\u001b[A\n",
      "Train step of epoch 2:   0%|          | 1/250 [00:00<03:37,  1.14it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 2:   1%|          | 2/250 [00:01<03:40,  1.13it/s, loss=0.268]\u001b[A\n",
      "Train step of epoch 2:   1%|          | 2/250 [00:01<03:40,  1.13it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 2:   1%|          | 3/250 [00:02<03:39,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 2:   1%|          | 3/250 [00:02<03:39,  1.12it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 4/250 [00:03<03:39,  1.12it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 4/250 [00:03<03:39,  1.12it/s, loss=0.361]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 5/250 [00:04<03:38,  1.12it/s, loss=0.361]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 5/250 [00:04<03:38,  1.12it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 6/250 [00:05<03:37,  1.12it/s, loss=0.573]\u001b[A\n",
      "Train step of epoch 2:   2%|▏         | 6/250 [00:05<03:37,  1.12it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 2:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.324]\u001b[A\n",
      "Train step of epoch 2:   3%|▎         | 7/250 [00:06<03:36,  1.12it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 2:   3%|▎         | 8/250 [00:07<03:36,  1.12it/s, loss=0.695]\u001b[A\n",
      "Train step of epoch 2:   3%|▎         | 8/250 [00:07<03:36,  1.12it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 2:   4%|▎         | 9/250 [00:08<03:35,  1.12it/s, loss=0.378]\u001b[A\n",
      "Train step of epoch 2:   4%|▎         | 9/250 [00:08<03:35,  1.12it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 10/250 [00:08<03:34,  1.12it/s, loss=0.418]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 10/250 [00:08<03:34,  1.12it/s, loss=0.194]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 11/250 [00:09<03:33,  1.12it/s, loss=0.194]\u001b[A\n",
      "Train step of epoch 2:   4%|▍         | 11/250 [00:09<03:33,  1.12it/s, loss=0.421]\u001b[A\n",
      "Train step of epoch 2:   5%|▍         | 12/250 [00:10<03:32,  1.12it/s, loss=0.421]\u001b[A\n",
      "Train step of epoch 2:   5%|▍         | 12/250 [00:10<03:32,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 2:   5%|▌         | 13/250 [00:11<03:31,  1.12it/s, loss=0.384]\u001b[A\n",
      "Train step of epoch 2:   5%|▌         | 13/250 [00:11<03:31,  1.12it/s, loss=0.113]\u001b[A\n",
      "Train step of epoch 2:   6%|▌         | 14/250 [00:12<03:30,  1.12it/s, loss=0.113]\u001b[A\n",
      "Train step of epoch 2:   6%|▌         | 14/250 [00:12<03:30,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 2:   6%|▌         | 15/250 [00:13<03:29,  1.12it/s, loss=0.648]\u001b[A\n",
      "Train step of epoch 2:   6%|▌         | 15/250 [00:13<03:29,  1.12it/s, loss=0.154]\u001b[A\n",
      "Train step of epoch 2:   6%|▋         | 16/250 [00:14<03:29,  1.12it/s, loss=0.154]\u001b[A\n",
      "Train step of epoch 2:   6%|▋         | 16/250 [00:14<03:29,  1.12it/s, loss=0.17] \u001b[A\n",
      "Train step of epoch 2:   7%|▋         | 17/250 [00:15<03:28,  1.12it/s, loss=0.17]\u001b[A\n",
      "Train step of epoch 2:   7%|▋         | 17/250 [00:15<03:28,  1.12it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 2:   7%|▋         | 18/250 [00:16<03:27,  1.12it/s, loss=0.237]\u001b[A\n",
      "Train step of epoch 2:   7%|▋         | 18/250 [00:16<03:27,  1.12it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 19/250 [00:16<03:26,  1.12it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 19/250 [00:16<03:26,  1.12it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 20/250 [00:17<03:25,  1.12it/s, loss=0.294]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 20/250 [00:17<03:25,  1.12it/s, loss=0.088]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 21/250 [00:18<03:24,  1.12it/s, loss=0.088]\u001b[A\n",
      "Train step of epoch 2:   8%|▊         | 21/250 [00:18<03:24,  1.12it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 2:   9%|▉         | 22/250 [00:19<03:24,  1.12it/s, loss=0.313]\u001b[A\n",
      "Train step of epoch 2:   9%|▉         | 22/250 [00:19<03:24,  1.12it/s, loss=0.182]\u001b[A\n",
      "Train step of epoch 2:   9%|▉         | 23/250 [00:20<03:23,  1.12it/s, loss=0.182]\u001b[A\n",
      "Train step of epoch 2:   9%|▉         | 23/250 [00:20<03:23,  1.12it/s, loss=0.0547]\u001b[A\n",
      "Train step of epoch 2:  10%|▉         | 24/250 [00:21<03:22,  1.12it/s, loss=0.0547]\u001b[A\n",
      "Train step of epoch 2:  10%|▉         | 24/250 [00:21<03:22,  1.12it/s, loss=0.104] \u001b[A\n",
      "Train step of epoch 2:  10%|█         | 25/250 [00:22<03:21,  1.12it/s, loss=0.104]\u001b[A\n",
      "Train step of epoch 2:  10%|█         | 25/250 [00:22<03:21,  1.12it/s, loss=0.0111]\u001b[A\n",
      "Train step of epoch 2:  10%|█         | 26/250 [00:23<03:20,  1.12it/s, loss=0.0111]\u001b[A\n",
      "Train step of epoch 2:  10%|█         | 26/250 [00:23<03:20,  1.12it/s, loss=0.156] \u001b[A\n",
      "Train step of epoch 2:  11%|█         | 27/250 [00:24<03:19,  1.12it/s, loss=0.156]\u001b[A\n",
      "Train step of epoch 2:  11%|█         | 27/250 [00:24<03:19,  1.12it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 2:  11%|█         | 28/250 [00:25<03:18,  1.12it/s, loss=0.128]\u001b[A\n",
      "Train step of epoch 2:  11%|█         | 28/250 [00:25<03:18,  1.12it/s, loss=0.0289]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 29/250 [00:25<03:17,  1.12it/s, loss=0.0289]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 29/250 [00:25<03:17,  1.12it/s, loss=0.000725]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 30/250 [00:26<03:16,  1.12it/s, loss=0.000725]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 30/250 [00:26<03:16,  1.12it/s, loss=0.022]   \u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 31/250 [00:27<03:16,  1.12it/s, loss=0.022]\u001b[A\n",
      "Train step of epoch 2:  12%|█▏        | 31/250 [00:27<03:16,  1.12it/s, loss=0.0316]\u001b[A\n",
      "Train step of epoch 2:  13%|█▎        | 32/250 [00:28<03:14,  1.12it/s, loss=0.0316]\u001b[A\n",
      "Train step of epoch 2:  13%|█▎        | 32/250 [00:28<03:14,  1.12it/s, loss=1.09]  \u001b[A\n",
      "Train step of epoch 2:  13%|█▎        | 33/250 [00:29<03:14,  1.12it/s, loss=1.09]\u001b[A\n",
      "Train step of epoch 2:  13%|█▎        | 33/250 [00:29<03:14,  1.12it/s, loss=0.117]\u001b[A\n",
      "Train step of epoch 2:  14%|█▎        | 34/250 [00:30<03:13,  1.12it/s, loss=0.117]\u001b[A\n",
      "Train step of epoch 2:  14%|█▎        | 34/250 [00:30<03:13,  1.12it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 2:  14%|█▍        | 35/250 [00:31<03:12,  1.12it/s, loss=0.786]\u001b[A\n",
      "Train step of epoch 2:  14%|█▍        | 35/250 [00:31<03:12,  1.12it/s, loss=0.0231]\u001b[A\n",
      "Train step of epoch 2:  14%|█▍        | 36/250 [00:32<03:11,  1.12it/s, loss=0.0231]\u001b[A\n",
      "Train step of epoch 2:  14%|█▍        | 36/250 [00:32<03:11,  1.12it/s, loss=0.367] \u001b[A\n",
      "Train step of epoch 2:  15%|█▍        | 37/250 [00:33<03:10,  1.12it/s, loss=0.367]\u001b[A\n",
      "Train step of epoch 2:  15%|█▍        | 37/250 [00:33<03:10,  1.12it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  15%|█▌        | 38/250 [00:33<03:09,  1.12it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  15%|█▌        | 38/250 [00:33<03:09,  1.12it/s, loss=0.00973]\u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.00973]\u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 39/250 [00:34<03:08,  1.12it/s, loss=0.0788] \u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 40/250 [00:35<03:08,  1.12it/s, loss=0.0788]\u001b[A\n",
      "Train step of epoch 2:  16%|█▌        | 40/250 [00:35<03:08,  1.12it/s, loss=0.102] \u001b[A\n",
      "Train step of epoch 2:  16%|█▋        | 41/250 [00:36<03:07,  1.12it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 2:  16%|█▋        | 41/250 [00:36<03:07,  1.12it/s, loss=0.0145]\u001b[A\n",
      "Train step of epoch 2:  17%|█▋        | 42/250 [00:37<03:06,  1.12it/s, loss=0.0145]\u001b[A\n",
      "Train step of epoch 2:  17%|█▋        | 42/250 [00:37<03:06,  1.12it/s, loss=0.174] \u001b[A\n",
      "Train step of epoch 2:  17%|█▋        | 43/250 [00:38<03:05,  1.12it/s, loss=0.174]\u001b[A\n",
      "Train step of epoch 2:  17%|█▋        | 43/250 [00:38<03:05,  1.12it/s, loss=0.0198]\u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 44/250 [00:39<03:04,  1.12it/s, loss=0.0198]\u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 44/250 [00:39<03:04,  1.12it/s, loss=0.18]  \u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 45/250 [00:40<03:03,  1.12it/s, loss=0.18]\u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 45/250 [00:40<03:03,  1.12it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 46/250 [00:41<03:02,  1.12it/s, loss=0.234]\u001b[A\n",
      "Train step of epoch 2:  18%|█▊        | 46/250 [00:41<03:02,  1.12it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 2:  19%|█▉        | 47/250 [00:42<03:01,  1.12it/s, loss=0.228]\u001b[A\n",
      "Train step of epoch 2:  19%|█▉        | 47/250 [00:42<03:01,  1.12it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 2:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.369]\u001b[A\n",
      "Train step of epoch 2:  19%|█▉        | 48/250 [00:42<03:00,  1.12it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 2:  20%|█▉        | 49/250 [00:43<02:59,  1.12it/s, loss=0.423]\u001b[A\n",
      "Train step of epoch 2:  20%|█▉        | 49/250 [00:43<02:59,  1.12it/s, loss=0.0169]\u001b[A\n",
      "Train step of epoch 2:  20%|██        | 50/250 [00:44<02:58,  1.12it/s, loss=0.0169]\u001b[A\n",
      "Train step of epoch 2:  20%|██        | 50/250 [00:44<02:58,  1.12it/s, loss=1.41]  \u001b[A\n",
      "Train step of epoch 2:  20%|██        | 51/250 [00:45<02:58,  1.12it/s, loss=1.41]\u001b[A\n",
      "Train step of epoch 2:  20%|██        | 51/250 [00:45<02:58,  1.12it/s, loss=0.0721]\u001b[A\n",
      "Train step of epoch 2:  21%|██        | 52/250 [00:46<02:57,  1.12it/s, loss=0.0721]\u001b[A\n",
      "Train step of epoch 2:  21%|██        | 52/250 [00:46<02:57,  1.12it/s, loss=0.0578]\u001b[A\n",
      "Train step of epoch 2:  21%|██        | 53/250 [00:47<02:56,  1.12it/s, loss=0.0578]\u001b[A\n",
      "Train step of epoch 2:  21%|██        | 53/250 [00:47<02:56,  1.12it/s, loss=0.0519]\u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 54/250 [00:48<02:55,  1.12it/s, loss=0.0519]\u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 54/250 [00:48<02:55,  1.12it/s, loss=0.175] \u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 55/250 [00:49<02:54,  1.12it/s, loss=0.175]\u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 55/250 [00:49<02:54,  1.12it/s, loss=0.0659]\u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 56/250 [00:50<02:53,  1.12it/s, loss=0.0659]\u001b[A\n",
      "Train step of epoch 2:  22%|██▏       | 56/250 [00:50<02:53,  1.12it/s, loss=0.133] \u001b[A\n",
      "Train step of epoch 2:  23%|██▎       | 57/250 [00:50<02:52,  1.12it/s, loss=0.133]\u001b[A\n",
      "Train step of epoch 2:  23%|██▎       | 57/250 [00:50<02:52,  1.12it/s, loss=0.0737]\u001b[A\n",
      "Train step of epoch 2:  23%|██▎       | 58/250 [00:51<02:51,  1.12it/s, loss=0.0737]\u001b[A\n",
      "Train step of epoch 2:  23%|██▎       | 58/250 [00:51<02:51,  1.12it/s, loss=0.0182]\u001b[A\n",
      "Train step of epoch 2:  24%|██▎       | 59/250 [00:52<02:50,  1.12it/s, loss=0.0182]\u001b[A\n",
      "Train step of epoch 2:  24%|██▎       | 59/250 [00:52<02:50,  1.12it/s, loss=0.0123]\u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 60/250 [00:53<02:49,  1.12it/s, loss=0.0123]\u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 60/250 [00:53<02:49,  1.12it/s, loss=0.134] \u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 61/250 [00:54<02:48,  1.12it/s, loss=0.134]\u001b[A\n",
      "Train step of epoch 2:  24%|██▍       | 61/250 [00:54<02:48,  1.12it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 2:  25%|██▍       | 62/250 [00:55<02:48,  1.12it/s, loss=0.765]\u001b[A\n",
      "Train step of epoch 2:  25%|██▍       | 62/250 [00:55<02:48,  1.12it/s, loss=0.0268]\u001b[A\n",
      "Train step of epoch 2:  25%|██▌       | 63/250 [00:56<02:47,  1.12it/s, loss=0.0268]\u001b[A\n",
      "Train step of epoch 2:  25%|██▌       | 63/250 [00:56<02:47,  1.12it/s, loss=0.256] \u001b[A\n",
      "Train step of epoch 2:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.256]\u001b[A\n",
      "Train step of epoch 2:  26%|██▌       | 64/250 [00:57<02:46,  1.12it/s, loss=0.00109]\u001b[A\n",
      "Train step of epoch 2:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.00109]\u001b[A\n",
      "Train step of epoch 2:  26%|██▌       | 65/250 [00:58<02:45,  1.12it/s, loss=0.674]  \u001b[A\n",
      "Train step of epoch 2:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.674]\u001b[A\n",
      "Train step of epoch 2:  26%|██▋       | 66/250 [00:59<02:44,  1.12it/s, loss=0.0065]\u001b[A\n",
      "Train step of epoch 2:  27%|██▋       | 67/250 [00:59<02:43,  1.12it/s, loss=0.0065]\u001b[A\n",
      "Train step of epoch 2:  27%|██▋       | 67/250 [00:59<02:43,  1.12it/s, loss=0.763] \u001b[A\n",
      "Train step of epoch 2:  27%|██▋       | 68/250 [01:00<02:42,  1.12it/s, loss=0.763]\u001b[A\n",
      "Train step of epoch 2:  27%|██▋       | 68/250 [01:00<02:42,  1.12it/s, loss=0.0816]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 69/250 [01:01<02:41,  1.12it/s, loss=0.0816]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 69/250 [01:01<02:41,  1.12it/s, loss=0.124] \u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 70/250 [01:02<02:40,  1.12it/s, loss=0.124]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 70/250 [01:02<02:40,  1.12it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 71/250 [01:03<02:40,  1.12it/s, loss=0.105]\u001b[A\n",
      "Train step of epoch 2:  28%|██▊       | 71/250 [01:03<02:40,  1.12it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 2:  29%|██▉       | 72/250 [01:04<02:39,  1.12it/s, loss=0.352]\u001b[A\n",
      "Train step of epoch 2:  29%|██▉       | 72/250 [01:04<02:39,  1.12it/s, loss=0.052]\u001b[A\n",
      "Train step of epoch 2:  29%|██▉       | 73/250 [01:05<02:38,  1.12it/s, loss=0.052]\u001b[A\n",
      "Train step of epoch 2:  29%|██▉       | 73/250 [01:05<02:38,  1.12it/s, loss=0.43] \u001b[A\n",
      "Train step of epoch 2:  30%|██▉       | 74/250 [01:06<02:37,  1.12it/s, loss=0.43]\u001b[A\n",
      "Train step of epoch 2:  30%|██▉       | 74/250 [01:06<02:37,  1.12it/s, loss=0.0889]\u001b[A\n",
      "Train step of epoch 2:  30%|███       | 75/250 [01:07<02:36,  1.12it/s, loss=0.0889]\u001b[A\n",
      "Train step of epoch 2:  30%|███       | 75/250 [01:07<02:36,  1.12it/s, loss=0.577] \u001b[A\n",
      "Train step of epoch 2:  30%|███       | 76/250 [01:07<02:35,  1.12it/s, loss=0.577]\u001b[A\n",
      "Train step of epoch 2:  30%|███       | 76/250 [01:07<02:35,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 2:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.348]\u001b[A\n",
      "Train step of epoch 2:  31%|███       | 77/250 [01:08<02:34,  1.12it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 2:  31%|███       | 78/250 [01:09<02:33,  1.12it/s, loss=0.184]\u001b[A\n",
      "Train step of epoch 2:  31%|███       | 78/250 [01:09<02:33,  1.12it/s, loss=0.072]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 79/250 [01:10<02:32,  1.12it/s, loss=0.072]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 79/250 [01:10<02:32,  1.12it/s, loss=0.0894]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 80/250 [01:11<02:32,  1.12it/s, loss=0.0894]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 80/250 [01:11<02:32,  1.12it/s, loss=0.157] \u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 81/250 [01:12<02:31,  1.12it/s, loss=0.157]\u001b[A\n",
      "Train step of epoch 2:  32%|███▏      | 81/250 [01:12<02:31,  1.12it/s, loss=0.088]\u001b[A\n",
      "Train step of epoch 2:  33%|███▎      | 82/250 [01:13<02:30,  1.12it/s, loss=0.088]\u001b[A\n",
      "Train step of epoch 2:  33%|███▎      | 82/250 [01:13<02:30,  1.12it/s, loss=2.18] \u001b[A\n",
      "Train step of epoch 2:  33%|███▎      | 83/250 [01:14<02:29,  1.12it/s, loss=2.18]\u001b[A\n",
      "Train step of epoch 2:  33%|███▎      | 83/250 [01:14<02:29,  1.12it/s, loss=0.0529]\u001b[A\n",
      "Train step of epoch 2:  34%|███▎      | 84/250 [01:15<02:28,  1.12it/s, loss=0.0529]\u001b[A\n",
      "Train step of epoch 2:  34%|███▎      | 84/250 [01:15<02:28,  1.12it/s, loss=0.309] \u001b[A\n",
      "Train step of epoch 2:  34%|███▍      | 85/250 [01:15<02:27,  1.12it/s, loss=0.309]\u001b[A\n",
      "Train step of epoch 2:  34%|███▍      | 85/250 [01:16<02:27,  1.12it/s, loss=0.173]\u001b[A\n",
      "Train step of epoch 2:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.173]\u001b[A\n",
      "Train step of epoch 2:  34%|███▍      | 86/250 [01:16<02:26,  1.12it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 2:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.344]\u001b[A\n",
      "Train step of epoch 2:  35%|███▍      | 87/250 [01:17<02:25,  1.12it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 2:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=0.477]\u001b[A\n",
      "Train step of epoch 2:  35%|███▌      | 88/250 [01:18<02:24,  1.12it/s, loss=1.17] \u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 89/250 [01:19<02:23,  1.12it/s, loss=1.17]\u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 89/250 [01:19<02:23,  1.12it/s, loss=0.73]\u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 90/250 [01:20<02:22,  1.12it/s, loss=0.73]\u001b[A\n",
      "Train step of epoch 2:  36%|███▌      | 90/250 [01:20<02:22,  1.12it/s, loss=0.598]\u001b[A\n",
      "Train step of epoch 2:  36%|███▋      | 91/250 [01:21<02:21,  1.12it/s, loss=0.598]\u001b[A\n",
      "Train step of epoch 2:  36%|███▋      | 91/250 [01:21<02:21,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 2:  37%|███▋      | 92/250 [01:22<02:20,  1.12it/s, loss=0.553]\u001b[A\n",
      "Train step of epoch 2:  37%|███▋      | 92/250 [01:22<02:20,  1.12it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 2:  37%|███▋      | 93/250 [01:23<02:20,  1.12it/s, loss=0.505]\u001b[A\n",
      "Train step of epoch 2:  37%|███▋      | 93/250 [01:23<02:20,  1.12it/s, loss=0.282]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 94/250 [01:24<02:19,  1.12it/s, loss=0.282]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 94/250 [01:24<02:19,  1.12it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 95/250 [01:24<02:18,  1.12it/s, loss=0.523]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 95/250 [01:24<02:18,  1.12it/s, loss=0.272]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 96/250 [01:25<02:17,  1.12it/s, loss=0.272]\u001b[A\n",
      "Train step of epoch 2:  38%|███▊      | 96/250 [01:25<02:17,  1.12it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 2:  39%|███▉      | 97/250 [01:26<02:16,  1.12it/s, loss=0.589]\u001b[A\n",
      "Train step of epoch 2:  39%|███▉      | 97/250 [01:26<02:16,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 2:  39%|███▉      | 98/250 [01:27<02:15,  1.12it/s, loss=0.437]\u001b[A\n",
      "Train step of epoch 2:  39%|███▉      | 98/250 [01:27<02:15,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 2:  40%|███▉      | 99/250 [01:28<02:14,  1.12it/s, loss=0.404]\u001b[A\n",
      "Train step of epoch 2:  40%|███▉      | 99/250 [01:28<02:14,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 100/250 [01:29<02:14,  1.12it/s, loss=0.408]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 100/250 [01:29<02:14,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 101/250 [01:30<02:13,  1.12it/s, loss=0.649]\u001b[A\n",
      "Train step of epoch 2:  40%|████      | 101/250 [01:30<02:13,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  41%|████      | 102/250 [01:31<02:12,  1.12it/s, loss=0.497]\u001b[A\n",
      "Train step of epoch 2:  41%|████      | 102/250 [01:31<02:12,  1.12it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 2:  41%|████      | 103/250 [01:32<02:11,  1.12it/s, loss=0.632]\u001b[A\n",
      "Train step of epoch 2:  41%|████      | 103/250 [01:32<02:11,  1.12it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 104/250 [01:32<02:10,  1.12it/s, loss=0.474]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 104/250 [01:32<02:10,  1.12it/s, loss=0.337]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 105/250 [01:33<02:09,  1.12it/s, loss=0.337]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 105/250 [01:33<02:09,  1.12it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 106/250 [01:34<02:08,  1.12it/s, loss=0.382]\u001b[A\n",
      "Train step of epoch 2:  42%|████▏     | 106/250 [01:34<02:08,  1.12it/s, loss=0.342]\u001b[A\n",
      "Train step of epoch 2:  43%|████▎     | 107/250 [01:35<02:07,  1.12it/s, loss=0.342]\u001b[A\n",
      "Train step of epoch 2:  43%|████▎     | 107/250 [01:35<02:07,  1.12it/s, loss=0.275]\u001b[A\n",
      "Train step of epoch 2:  43%|████▎     | 108/250 [01:36<02:06,  1.12it/s, loss=0.275]\u001b[A\n",
      "Train step of epoch 2:  43%|████▎     | 108/250 [01:36<02:06,  1.12it/s, loss=0.0958]\u001b[A\n",
      "Train step of epoch 2:  44%|████▎     | 109/250 [01:37<02:05,  1.12it/s, loss=0.0958]\u001b[A\n",
      "Train step of epoch 2:  44%|████▎     | 109/250 [01:37<02:05,  1.12it/s, loss=0.329] \u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 110/250 [01:38<02:05,  1.12it/s, loss=0.329]\u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 110/250 [01:38<02:05,  1.12it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 111/250 [01:39<02:04,  1.12it/s, loss=0.301]\u001b[A\n",
      "Train step of epoch 2:  44%|████▍     | 111/250 [01:39<02:04,  1.12it/s, loss=0.413]\u001b[A\n",
      "Train step of epoch 2:  45%|████▍     | 112/250 [01:40<02:03,  1.12it/s, loss=0.413]\u001b[A\n",
      "Train step of epoch 2:  45%|████▍     | 112/250 [01:40<02:03,  1.12it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 2:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.102]\u001b[A\n",
      "Train step of epoch 2:  45%|████▌     | 113/250 [01:41<02:02,  1.12it/s, loss=0.0988]\u001b[A\n",
      "Train step of epoch 2:  46%|████▌     | 114/250 [01:41<02:01,  1.12it/s, loss=0.0988]\u001b[A\n",
      "Train step of epoch 2:  46%|████▌     | 114/250 [01:41<02:01,  1.12it/s, loss=0.126] \u001b[A\n",
      "Train step of epoch 2:  46%|████▌     | 115/250 [01:42<02:00,  1.12it/s, loss=0.126]\u001b[A\n",
      "Train step of epoch 2:  46%|████▌     | 115/250 [01:42<02:00,  1.12it/s, loss=0.0855]\u001b[A\n",
      "Train step of epoch 2:  46%|████▋     | 116/250 [01:43<01:59,  1.12it/s, loss=0.0855]\u001b[A\n",
      "Train step of epoch 2:  46%|████▋     | 116/250 [01:43<01:59,  1.12it/s, loss=0.0541]\u001b[A\n",
      "Train step of epoch 2:  47%|████▋     | 117/250 [01:44<01:58,  1.12it/s, loss=0.0541]\u001b[A\n",
      "Train step of epoch 2:  47%|████▋     | 117/250 [01:44<01:58,  1.12it/s, loss=0.0647]\u001b[A\n",
      "Train step of epoch 2:  47%|████▋     | 118/250 [01:45<01:58,  1.12it/s, loss=0.0647]\u001b[A\n",
      "Train step of epoch 2:  47%|████▋     | 118/250 [01:45<01:58,  1.12it/s, loss=0.0299]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 119/250 [01:46<01:57,  1.12it/s, loss=0.0299]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 119/250 [01:46<01:57,  1.12it/s, loss=0.067] \u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=0.067]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 120/250 [01:47<01:56,  1.12it/s, loss=0.336]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=0.336]\u001b[A\n",
      "Train step of epoch 2:  48%|████▊     | 121/250 [01:48<01:55,  1.12it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 2:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.464]\u001b[A\n",
      "Train step of epoch 2:  49%|████▉     | 122/250 [01:49<01:54,  1.12it/s, loss=0.0506]\u001b[A\n",
      "Train step of epoch 2:  49%|████▉     | 123/250 [01:49<01:53,  1.12it/s, loss=0.0506]\u001b[A\n",
      "Train step of epoch 2:  49%|████▉     | 123/250 [01:49<01:53,  1.12it/s, loss=0.00502]\u001b[A\n",
      "Train step of epoch 2:  50%|████▉     | 124/250 [01:50<01:52,  1.12it/s, loss=0.00502]\u001b[A\n",
      "Train step of epoch 2:  50%|████▉     | 124/250 [01:50<01:52,  1.12it/s, loss=0.1]    \u001b[A\n",
      "Train step of epoch 2:  50%|█████     | 125/250 [01:51<01:51,  1.12it/s, loss=0.1]\u001b[A\n",
      "Train step of epoch 2:  50%|█████     | 125/250 [01:51<01:51,  1.12it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 2:  50%|█████     | 126/250 [01:52<01:50,  1.12it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 2:  50%|█████     | 126/250 [01:52<01:50,  1.12it/s, loss=0.0412]\u001b[A\n",
      "Train step of epoch 2:  51%|█████     | 127/250 [01:53<01:49,  1.12it/s, loss=0.0412]\u001b[A\n",
      "Train step of epoch 2:  51%|█████     | 127/250 [01:53<01:49,  1.12it/s, loss=0.19]  \u001b[A\n",
      "Train step of epoch 2:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=0.19]\u001b[A\n",
      "Train step of epoch 2:  51%|█████     | 128/250 [01:54<01:49,  1.12it/s, loss=1.19]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=1.19]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 129/250 [01:55<01:48,  1.12it/s, loss=0.101]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=0.101]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 130/250 [01:56<01:47,  1.12it/s, loss=1.5]  \u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=1.5]\u001b[A\n",
      "Train step of epoch 2:  52%|█████▏    | 131/250 [01:57<01:46,  1.12it/s, loss=0.0464]\u001b[A\n",
      "Train step of epoch 2:  53%|█████▎    | 132/250 [01:57<01:45,  1.12it/s, loss=0.0464]\u001b[A\n",
      "Train step of epoch 2:  53%|█████▎    | 132/250 [01:58<01:45,  1.12it/s, loss=0.258] \u001b[A\n",
      "Train step of epoch 2:  53%|█████▎    | 133/250 [01:58<01:44,  1.12it/s, loss=0.258]\u001b[A\n",
      "Train step of epoch 2:  53%|█████▎    | 133/250 [01:58<01:44,  1.12it/s, loss=0.0819]\u001b[A\n",
      "Train step of epoch 2:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.0819]\u001b[A\n",
      "Train step of epoch 2:  54%|█████▎    | 134/250 [01:59<01:43,  1.12it/s, loss=0.246] \u001b[A\n",
      "Train step of epoch 2:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.246]\u001b[A\n",
      "Train step of epoch 2:  54%|█████▍    | 135/250 [02:00<01:42,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 2:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.391]\u001b[A\n",
      "Train step of epoch 2:  54%|█████▍    | 136/250 [02:01<01:41,  1.12it/s, loss=0.0566]\u001b[A\n",
      "Train step of epoch 2:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.0566]\u001b[A\n",
      "Train step of epoch 2:  55%|█████▍    | 137/250 [02:02<01:40,  1.12it/s, loss=0.0414]\u001b[A\n",
      "Train step of epoch 2:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.0414]\u001b[A\n",
      "Train step of epoch 2:  55%|█████▌    | 138/250 [02:03<01:40,  1.12it/s, loss=0.287] \u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 139/250 [02:04<01:39,  1.12it/s, loss=0.287]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 139/250 [02:04<01:39,  1.12it/s, loss=0.167]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 140/250 [02:05<01:38,  1.12it/s, loss=0.167]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▌    | 140/250 [02:05<01:38,  1.12it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▋    | 141/250 [02:06<01:37,  1.12it/s, loss=0.264]\u001b[A\n",
      "Train step of epoch 2:  56%|█████▋    | 141/250 [02:06<01:37,  1.12it/s, loss=0.0532]\u001b[A\n",
      "Train step of epoch 2:  57%|█████▋    | 142/250 [02:06<01:36,  1.12it/s, loss=0.0532]\u001b[A\n",
      "Train step of epoch 2:  57%|█████▋    | 142/250 [02:06<01:36,  1.12it/s, loss=0.11]  \u001b[A\n",
      "Train step of epoch 2:  57%|█████▋    | 143/250 [02:07<01:35,  1.12it/s, loss=0.11]\u001b[A\n",
      "Train step of epoch 2:  57%|█████▋    | 143/250 [02:07<01:35,  1.12it/s, loss=1.96]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 144/250 [02:08<01:34,  1.12it/s, loss=1.96]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 144/250 [02:08<01:34,  1.12it/s, loss=0.044]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 145/250 [02:09<01:33,  1.12it/s, loss=0.044]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 145/250 [02:09<01:33,  1.12it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 146/250 [02:10<01:32,  1.12it/s, loss=0.539]\u001b[A\n",
      "Train step of epoch 2:  58%|█████▊    | 146/250 [02:10<01:32,  1.12it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 2:  59%|█████▉    | 147/250 [02:11<01:32,  1.12it/s, loss=0.165]\u001b[A\n",
      "Train step of epoch 2:  59%|█████▉    | 147/250 [02:11<01:32,  1.12it/s, loss=0.0177]\u001b[A\n",
      "Train step of epoch 2:  59%|█████▉    | 148/250 [02:12<01:31,  1.12it/s, loss=0.0177]\u001b[A\n",
      "Train step of epoch 2:  59%|█████▉    | 148/250 [02:12<01:31,  1.12it/s, loss=0.0841]\u001b[A\n",
      "Train step of epoch 2:  60%|█████▉    | 149/250 [02:13<01:30,  1.12it/s, loss=0.0841]\u001b[A\n",
      "Train step of epoch 2:  60%|█████▉    | 149/250 [02:13<01:30,  1.12it/s, loss=0.0765]\u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 150/250 [02:14<01:29,  1.12it/s, loss=0.0765]\u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 150/250 [02:14<01:29,  1.12it/s, loss=0.0385]\u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 151/250 [02:14<01:28,  1.12it/s, loss=0.0385]\u001b[A\n",
      "Train step of epoch 2:  60%|██████    | 151/250 [02:14<01:28,  1.12it/s, loss=0.0917]\u001b[A\n",
      "Train step of epoch 2:  61%|██████    | 152/250 [02:15<01:27,  1.12it/s, loss=0.0917]\u001b[A\n",
      "Train step of epoch 2:  61%|██████    | 152/250 [02:15<01:27,  1.12it/s, loss=0.11]  \u001b[A\n",
      "Train step of epoch 2:  61%|██████    | 153/250 [02:16<01:26,  1.12it/s, loss=0.11]\u001b[A\n",
      "Train step of epoch 2:  61%|██████    | 153/250 [02:16<01:26,  1.12it/s, loss=0.0925]\u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 154/250 [02:17<01:25,  1.12it/s, loss=0.0925]\u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 154/250 [02:17<01:25,  1.12it/s, loss=0.0588]\u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 155/250 [02:18<01:25,  1.12it/s, loss=0.0588]\u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 155/250 [02:18<01:25,  1.12it/s, loss=0.329] \u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 156/250 [02:19<01:24,  1.12it/s, loss=0.329]\u001b[A\n",
      "Train step of epoch 2:  62%|██████▏   | 156/250 [02:19<01:24,  1.12it/s, loss=0.0438]\u001b[A\n",
      "Train step of epoch 2:  63%|██████▎   | 157/250 [02:20<01:23,  1.12it/s, loss=0.0438]\u001b[A\n",
      "Train step of epoch 2:  63%|██████▎   | 157/250 [02:20<01:23,  1.12it/s, loss=0.0569]\u001b[A\n",
      "Train step of epoch 2:  63%|██████▎   | 158/250 [02:21<01:22,  1.12it/s, loss=0.0569]\u001b[A\n",
      "Train step of epoch 2:  63%|██████▎   | 158/250 [02:21<01:22,  1.12it/s, loss=0.606] \u001b[A\n",
      "Train step of epoch 2:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=0.606]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▎   | 159/250 [02:22<01:21,  1.12it/s, loss=0.0306]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.0306]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 160/250 [02:23<01:20,  1.12it/s, loss=0.131] \u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 161/250 [02:23<01:19,  1.12it/s, loss=0.131]\u001b[A\n",
      "Train step of epoch 2:  64%|██████▍   | 161/250 [02:23<01:19,  1.12it/s, loss=0.0112]\u001b[A\n",
      "Train step of epoch 2:  65%|██████▍   | 162/250 [02:24<01:18,  1.12it/s, loss=0.0112]\u001b[A\n",
      "Train step of epoch 2:  65%|██████▍   | 162/250 [02:24<01:18,  1.12it/s, loss=0.526] \u001b[A\n",
      "Train step of epoch 2:  65%|██████▌   | 163/250 [02:25<01:17,  1.12it/s, loss=0.526]\u001b[A\n",
      "Train step of epoch 2:  65%|██████▌   | 163/250 [02:25<01:17,  1.12it/s, loss=0.021]\u001b[A\n",
      "Train step of epoch 2:  66%|██████▌   | 164/250 [02:26<01:16,  1.12it/s, loss=0.021]\u001b[A\n",
      "Train step of epoch 2:  66%|██████▌   | 164/250 [02:26<01:16,  1.12it/s, loss=0.00718]\u001b[A\n",
      "Train step of epoch 2:  66%|██████▌   | 165/250 [02:27<01:16,  1.12it/s, loss=0.00718]\u001b[A\n",
      "Train step of epoch 2:  66%|██████▌   | 165/250 [02:27<01:16,  1.12it/s, loss=0.23]   \u001b[A\n",
      "Train step of epoch 2:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=0.23]\u001b[A\n",
      "Train step of epoch 2:  66%|██████▋   | 166/250 [02:28<01:15,  1.12it/s, loss=0.231]\u001b[A\n",
      "Train step of epoch 2:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.231]\u001b[A\n",
      "Train step of epoch 2:  67%|██████▋   | 167/250 [02:29<01:14,  1.12it/s, loss=0.0292]\u001b[A\n",
      "Train step of epoch 2:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.0292]\u001b[A\n",
      "Train step of epoch 2:  67%|██████▋   | 168/250 [02:30<01:13,  1.12it/s, loss=0.0145]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.0145]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 169/250 [02:31<01:12,  1.12it/s, loss=0.0291]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 170/250 [02:31<01:11,  1.12it/s, loss=0.0291]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 170/250 [02:31<01:11,  1.12it/s, loss=0.137] \u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.137]\u001b[A\n",
      "Train step of epoch 2:  68%|██████▊   | 171/250 [02:32<01:10,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 2:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.572]\u001b[A\n",
      "Train step of epoch 2:  69%|██████▉   | 172/250 [02:33<01:09,  1.12it/s, loss=0.108]\u001b[A\n",
      "Train step of epoch 2:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.108]\u001b[A\n",
      "Train step of epoch 2:  69%|██████▉   | 173/250 [02:34<01:08,  1.12it/s, loss=0.0793]\u001b[A\n",
      "Train step of epoch 2:  70%|██████▉   | 174/250 [02:35<01:08,  1.12it/s, loss=0.0793]\u001b[A\n",
      "Train step of epoch 2:  70%|██████▉   | 174/250 [02:35<01:08,  1.12it/s, loss=0.00182]\u001b[A\n",
      "Train step of epoch 2:  70%|███████   | 175/250 [02:36<01:07,  1.12it/s, loss=0.00182]\u001b[A\n",
      "Train step of epoch 2:  70%|███████   | 175/250 [02:36<01:07,  1.12it/s, loss=0.0616] \u001b[A\n",
      "Train step of epoch 2:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.0616]\u001b[A\n",
      "Train step of epoch 2:  70%|███████   | 176/250 [02:37<01:06,  1.12it/s, loss=0.000988]\u001b[A\n",
      "Train step of epoch 2:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.000988]\u001b[A\n",
      "Train step of epoch 2:  71%|███████   | 177/250 [02:38<01:05,  1.12it/s, loss=0.515]   \u001b[A\n",
      "Train step of epoch 2:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.515]\u001b[A\n",
      "Train step of epoch 2:  71%|███████   | 178/250 [02:39<01:04,  1.12it/s, loss=0.00711]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 179/250 [02:39<01:03,  1.12it/s, loss=0.00711]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 179/250 [02:40<01:03,  1.12it/s, loss=0.0674] \u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 180/250 [02:40<01:02,  1.12it/s, loss=0.0674]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 180/250 [02:40<01:02,  1.12it/s, loss=0.00661]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.00661]\u001b[A\n",
      "Train step of epoch 2:  72%|███████▏  | 181/250 [02:41<01:01,  1.12it/s, loss=0.478]  \u001b[A\n",
      "Train step of epoch 2:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=0.478]\u001b[A\n",
      "Train step of epoch 2:  73%|███████▎  | 182/250 [02:42<01:00,  1.12it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 2:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=0.146]\u001b[A\n",
      "Train step of epoch 2:  73%|███████▎  | 183/250 [02:43<00:59,  1.12it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 2:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 2:  74%|███████▎  | 184/250 [02:44<00:58,  1.12it/s, loss=0.33]  \u001b[A\n",
      "Train step of epoch 2:  74%|███████▍  | 185/250 [02:45<00:58,  1.12it/s, loss=0.33]\u001b[A\n",
      "Train step of epoch 2:  74%|███████▍  | 185/250 [02:45<00:58,  1.12it/s, loss=0.0767]\u001b[A\n",
      "Train step of epoch 2:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.0767]\u001b[A\n",
      "Train step of epoch 2:  74%|███████▍  | 186/250 [02:46<00:57,  1.12it/s, loss=0.017] \u001b[A\n",
      "Train step of epoch 2:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.017]\u001b[A\n",
      "Train step of epoch 2:  75%|███████▍  | 187/250 [02:47<00:56,  1.12it/s, loss=0.0528]\u001b[A\n",
      "Train step of epoch 2:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.0528]\u001b[A\n",
      "Train step of epoch 2:  75%|███████▌  | 188/250 [02:48<00:55,  1.12it/s, loss=0.0821]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 189/250 [02:48<00:54,  1.12it/s, loss=0.0821]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 189/250 [02:48<00:54,  1.12it/s, loss=0.27]  \u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.27]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▌  | 190/250 [02:49<00:53,  1.12it/s, loss=0.00303]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.00303]\u001b[A\n",
      "Train step of epoch 2:  76%|███████▋  | 191/250 [02:50<00:52,  1.12it/s, loss=0.0736] \u001b[A\n",
      "Train step of epoch 2:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.0736]\u001b[A\n",
      "Train step of epoch 2:  77%|███████▋  | 192/250 [02:51<00:51,  1.12it/s, loss=0.0631]\u001b[A\n",
      "Train step of epoch 2:  77%|███████▋  | 193/250 [02:52<00:51,  1.12it/s, loss=0.0631]\u001b[A\n",
      "Train step of epoch 2:  77%|███████▋  | 193/250 [02:52<00:51,  1.12it/s, loss=0.38]  \u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 194/250 [02:53<00:50,  1.12it/s, loss=0.38]\u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 194/250 [02:53<00:50,  1.12it/s, loss=0.00712]\u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.00712]\u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 195/250 [02:54<00:49,  1.12it/s, loss=0.267]  \u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=0.267]\u001b[A\n",
      "Train step of epoch 2:  78%|███████▊  | 196/250 [02:55<00:48,  1.12it/s, loss=0.022]\u001b[A\n",
      "Train step of epoch 2:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=0.022]\u001b[A\n",
      "Train step of epoch 2:  79%|███████▉  | 197/250 [02:56<00:47,  1.12it/s, loss=0.738]\u001b[A\n",
      "Train step of epoch 2:  79%|███████▉  | 198/250 [02:56<00:46,  1.12it/s, loss=0.738]\u001b[A\n",
      "Train step of epoch 2:  79%|███████▉  | 198/250 [02:57<00:46,  1.12it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 2:  80%|███████▉  | 199/250 [02:57<00:45,  1.12it/s, loss=0.0101]\u001b[A\n",
      "Train step of epoch 2:  80%|███████▉  | 199/250 [02:57<00:45,  1.12it/s, loss=0.00462]\u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.00462]\u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 200/250 [02:58<00:44,  1.12it/s, loss=0.0186] \u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.0186]\u001b[A\n",
      "Train step of epoch 2:  80%|████████  | 201/250 [02:59<00:43,  1.12it/s, loss=0.005] \u001b[A\n",
      "Train step of epoch 2:  81%|████████  | 202/250 [03:00<00:42,  1.12it/s, loss=0.005]\u001b[A\n",
      "Train step of epoch 2:  81%|████████  | 202/250 [03:00<00:42,  1.12it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  81%|████████  | 203/250 [03:01<00:42,  1.12it/s, loss=0.203]\u001b[A\n",
      "Train step of epoch 2:  81%|████████  | 203/250 [03:01<00:42,  1.12it/s, loss=0.091]\u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 204/250 [03:02<00:41,  1.12it/s, loss=0.091]\u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 204/250 [03:02<00:41,  1.12it/s, loss=0.0596]\u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.0596]\u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 205/250 [03:03<00:40,  1.12it/s, loss=0.181] \u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.181]\u001b[A\n",
      "Train step of epoch 2:  82%|████████▏ | 206/250 [03:04<00:39,  1.12it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 2:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.521]\u001b[A\n",
      "Train step of epoch 2:  83%|████████▎ | 207/250 [03:05<00:38,  1.12it/s, loss=0.0596]\u001b[A\n",
      "Train step of epoch 2:  83%|████████▎ | 208/250 [03:05<00:37,  1.12it/s, loss=0.0596]\u001b[A\n",
      "Train step of epoch 2:  83%|████████▎ | 208/250 [03:05<00:37,  1.12it/s, loss=1.06]  \u001b[A\n",
      "Train step of epoch 2:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=1.06]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▎ | 209/250 [03:06<00:36,  1.12it/s, loss=0.2] \u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.2]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 210/250 [03:07<00:35,  1.12it/s, loss=0.207]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.207]\u001b[A\n",
      "Train step of epoch 2:  84%|████████▍ | 211/250 [03:08<00:34,  1.12it/s, loss=0.0169]\u001b[A\n",
      "Train step of epoch 2:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.0169]\u001b[A\n",
      "Train step of epoch 2:  85%|████████▍ | 212/250 [03:09<00:33,  1.12it/s, loss=0.254] \u001b[A\n",
      "Train step of epoch 2:  85%|████████▌ | 213/250 [03:10<00:33,  1.12it/s, loss=0.254]\u001b[A\n",
      "Train step of epoch 2:  85%|████████▌ | 213/250 [03:10<00:33,  1.12it/s, loss=0.172]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=0.172]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▌ | 214/250 [03:11<00:32,  1.12it/s, loss=0.0583]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=0.0583]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▌ | 215/250 [03:12<00:31,  1.12it/s, loss=0.00283]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.00283]\u001b[A\n",
      "Train step of epoch 2:  86%|████████▋ | 216/250 [03:13<00:30,  1.12it/s, loss=0.758]  \u001b[A\n",
      "Train step of epoch 2:  87%|████████▋ | 217/250 [03:13<00:29,  1.12it/s, loss=0.758]\u001b[A\n",
      "Train step of epoch 2:  87%|████████▋ | 217/250 [03:14<00:29,  1.12it/s, loss=0.0165]\u001b[A\n",
      "Train step of epoch 2:  87%|████████▋ | 218/250 [03:14<00:28,  1.12it/s, loss=0.0165]\u001b[A\n",
      "Train step of epoch 2:  87%|████████▋ | 218/250 [03:14<00:28,  1.12it/s, loss=0.221] \u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.221]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 219/250 [03:15<00:27,  1.12it/s, loss=0.00814]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.00814]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 220/250 [03:16<00:26,  1.12it/s, loss=0.136]  \u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.136]\u001b[A\n",
      "Train step of epoch 2:  88%|████████▊ | 221/250 [03:17<00:25,  1.12it/s, loss=0.00715]\u001b[A\n",
      "Train step of epoch 2:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.00715]\u001b[A\n",
      "Train step of epoch 2:  89%|████████▉ | 222/250 [03:18<00:25,  1.12it/s, loss=0.0578] \u001b[A\n",
      "Train step of epoch 2:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=0.0578]\u001b[A\n",
      "Train step of epoch 2:  89%|████████▉ | 223/250 [03:19<00:24,  1.12it/s, loss=1.27]  \u001b[A\n",
      "Train step of epoch 2:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=1.27]\u001b[A\n",
      "Train step of epoch 2:  90%|████████▉ | 224/250 [03:20<00:23,  1.12it/s, loss=0.0421]\u001b[A\n",
      "Train step of epoch 2:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.0421]\u001b[A\n",
      "Train step of epoch 2:  90%|█████████ | 225/250 [03:21<00:22,  1.12it/s, loss=0.00302]\u001b[A\n",
      "Train step of epoch 2:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.00302]\u001b[A\n",
      "Train step of epoch 2:  90%|█████████ | 226/250 [03:22<00:21,  1.12it/s, loss=0.339]  \u001b[A\n",
      "Train step of epoch 2:  91%|█████████ | 227/250 [03:22<00:20,  1.12it/s, loss=0.339]\u001b[A\n",
      "Train step of epoch 2:  91%|█████████ | 227/250 [03:22<00:20,  1.12it/s, loss=0.0287]\u001b[A\n",
      "Train step of epoch 2:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.0287]\u001b[A\n",
      "Train step of epoch 2:  91%|█████████ | 228/250 [03:23<00:19,  1.12it/s, loss=0.0431]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.0431]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 229/250 [03:24<00:18,  1.12it/s, loss=0.166] \u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.166]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 230/250 [03:25<00:17,  1.12it/s, loss=0.00749]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.00749]\u001b[A\n",
      "Train step of epoch 2:  92%|█████████▏| 231/250 [03:26<00:16,  1.12it/s, loss=0.00904]\u001b[A\n",
      "Train step of epoch 2:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.00904]\u001b[A\n",
      "Train step of epoch 2:  93%|█████████▎| 232/250 [03:27<00:16,  1.12it/s, loss=0.0954] \u001b[A\n",
      "Train step of epoch 2:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.0954]\u001b[A\n",
      "Train step of epoch 2:  93%|█████████▎| 233/250 [03:28<00:15,  1.12it/s, loss=0.0753]\u001b[A\n",
      "Train step of epoch 2:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.0753]\u001b[A\n",
      "Train step of epoch 2:  94%|█████████▎| 234/250 [03:29<00:14,  1.12it/s, loss=0.00602]\u001b[A\n",
      "Train step of epoch 2:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=0.00602]\u001b[A\n",
      "Train step of epoch 2:  94%|█████████▍| 235/250 [03:30<00:13,  1.12it/s, loss=0.038]  \u001b[A\n",
      "Train step of epoch 2:  94%|█████████▍| 236/250 [03:30<00:12,  1.12it/s, loss=0.038]\u001b[A\n",
      "Train step of epoch 2:  94%|█████████▍| 236/250 [03:30<00:12,  1.12it/s, loss=0.0286]\u001b[A\n",
      "Train step of epoch 2:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.0286]\u001b[A\n",
      "Train step of epoch 2:  95%|█████████▍| 237/250 [03:31<00:11,  1.12it/s, loss=0.16]  \u001b[A\n",
      "Train step of epoch 2:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.16]\u001b[A\n",
      "Train step of epoch 2:  95%|█████████▌| 238/250 [03:32<00:10,  1.12it/s, loss=0.0585]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.0585]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 239/250 [03:33<00:09,  1.12it/s, loss=0.262] \u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.262]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▌| 240/250 [03:34<00:08,  1.12it/s, loss=0.0925]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.0925]\u001b[A\n",
      "Train step of epoch 2:  96%|█████████▋| 241/250 [03:35<00:08,  1.12it/s, loss=0.122] \u001b[A\n",
      "Train step of epoch 2:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.122]\u001b[A\n",
      "Train step of epoch 2:  97%|█████████▋| 242/250 [03:36<00:07,  1.12it/s, loss=0.0159]\u001b[A\n",
      "Train step of epoch 2:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.0159]\u001b[A\n",
      "Train step of epoch 2:  97%|█████████▋| 243/250 [03:37<00:06,  1.12it/s, loss=0.0289]\u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.0289]\u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 244/250 [03:38<00:05,  1.12it/s, loss=0.15]  \u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.15]\u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 245/250 [03:39<00:04,  1.12it/s, loss=0.00174]\u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 246/250 [03:39<00:03,  1.12it/s, loss=0.00174]\u001b[A\n",
      "Train step of epoch 2:  98%|█████████▊| 246/250 [03:39<00:03,  1.12it/s, loss=0.0745] \u001b[A\n",
      "Train step of epoch 2:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.0745]\u001b[A\n",
      "Train step of epoch 2:  99%|█████████▉| 247/250 [03:40<00:02,  1.12it/s, loss=0.0079]\u001b[A\n",
      "Train step of epoch 2:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=0.0079]\u001b[A\n",
      "Train step of epoch 2:  99%|█████████▉| 248/250 [03:41<00:01,  1.12it/s, loss=0.192] \u001b[A\n",
      "Train step of epoch 2: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=0.192]\u001b[A\n",
      "Train step of epoch 2: 100%|█████████▉| 249/250 [03:42<00:00,  1.12it/s, loss=0.39] \u001b[A\n",
      "Train step of epoch 2: 100%|██████████| 250/250 [03:43<00:00,  1.12it/s, loss=0.39]\u001b[A\n",
      "Train epoch: 100%|██████████| 3/3 [11:55<00:00, 238.32s/it]0,  1.12it/s, loss=0.0272]\u001b[A\n",
      "Train step of epoch 2: 100%|██████████| 250/250 [03:58<00:00,  1.05it/s, loss=1.46, dist_mean=1.51]\u001b[A\n",
      "Train epoch: 100%|██████████| 3/3 [11:55<00:00, 238.37s/it]\n"
     ]
    }
   ],
   "source": [
    "# fit() 실행 후 반환된 로그에서 손실 및 보상값 저장\n",
    "trainer.fit(use_lora=0)  # fit() 실행 후 반환되는 stats 활용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "60021ae1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 똥멍청이 입니다\n",
      "reward score: 0.4\n"
     ]
    }
   ],
   "source": [
    "def inference_RM(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    output = model(input_ids)\n",
    "    output_reward = output.cpu().detach().numpy()[0]\n",
    "\n",
    "    print('input: %s\\nreward score: %.1f'%(input_text, output_reward))\n",
    "\n",
    "    return output_reward\n",
    "\n",
    "input_text = '인공지능은 똥멍청이 입니다'\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "8189fcd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.\n",
      "reward score: 2.3\n"
     ]
    }
   ],
   "source": [
    "input_text = '인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다.'\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "680145fd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\n",
      "reward score: 1.5\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능(AI)은 컴퓨터에서 음성 및 작성된 언어를 보고 이해하고 번역하고 데이터를 분석하고 추천하는 기능을 포함하여 다양한 고급 기능을 수행할 수 있는 일련의 기술입니다. AI는 현대적인 컴퓨팅 혁신에서 중추적인 역할을 하며 개인과 비즈니스의 가치를 창출합니다. 예를 들어 광학 문자 인식(OCR)은 AI를 사용해 이미지 및 문서에서 텍스트 및 데이터를 추출하고, 구조화되지 않은 콘텐츠를 비즈니스에 바로 사용할 수 있게 만들고, 유용한 정보를 창출합니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "58aecfe3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "input: 인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\n",
      "reward score: 1.5\n"
     ]
    }
   ],
   "source": [
    "input_text = \"인공지능은 일반적으로 인간의 지능이 필요하거나 인간이 분석할 수 있는 것보다 규모가 큰 데이터를 포함하는 방식으로 추론, 학습 및 행동할 수 있는 컴퓨터 및 기계를 구축하는 것과 관련된 과학 분야입니다. AI는 컴퓨터 공학, 데이터 분석 및 통계, 하드웨어 및 소프트웨어 엔지니어링, 언어학, 신경 과학은 물론 철학과 심리학을 포함하여 여러 학문을 포괄하는 광범위한 분야입니다. 비즈니스의 운영 수준에서 AI는 주로 머신러닝과 딥 러닝을 기반으로 하는 기술 모음으로, 데이터 분석, 예상 및 예측, 객체 분류, 자연어 처리, 추천, 지능형 데이터 가져오기 등을 수행할 수 있습니다.\"\n",
    "\n",
    "output_reward = inference_RM(input_text=input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de62193",
   "metadata": {},
   "source": [
    "### 3. PPO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "69fc281a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from copy import deepcopy\n",
    "\n",
    "import torch\n",
    "from torch.optim import Adam\n",
    "from chatgpt.models.base import RewardModel\n",
    "from chatgpt.models.gpt import GPTActor, GPTCritic\n",
    "from chatgpt.trainer import PPOTrainer\n",
    "from chatgpt.trainer.strategies import NaiveStrategy\n",
    "from transformers import AutoTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "bddeee20",
   "metadata": {},
   "outputs": [],
   "source": [
    "with NaiveStrategy().model_init_context():\n",
    "    actor = GPTActor(pretrained='output_1_SFT', lora_rank=0).to(torch.cuda.current_device())\n",
    "    critic = GPTCritic(pretrained='output_2_RM', lora_rank=0).to(torch.cuda.current_device())\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        'skt/kogpt2-base-v2', bos_token='</s>', eos_token='</s>', unk_token='</s>', pad_token='</s>',\n",
    "        padding_side=\"right\", \n",
    "        model_max_length=512\n",
    "    )\n",
    "\n",
    "    initial_model = deepcopy(actor)\n",
    "    reward_model = RewardModel(deepcopy(critic.model), deepcopy(critic.value_head)).to(torch.cuda.current_device())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "e02097c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "actor_optim = Adam(actor.parameters(), lr=5e-6)\n",
    "critic_optim = Adam(critic.parameters(), lr=5e-6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "6f36c169",
   "metadata": {},
   "outputs": [],
   "source": [
    "(actor, actor_optim), (critic, critic_optim), reward_model, initial_model = NaiveStrategy().prepare(\n",
    "    (actor, actor_optim), (critic, critic_optim), reward_model, initial_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "8f0c1c0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('./data_kochatgpt/kochatgpt_3_PPO.jsonl', \"r\", encoding='utf-8-sig') as json_file:\n",
    "    list_data_dict = json.load(json_file)\n",
    "    list_prompt = [tmp['prompt'] for tmp in list_data_dict]\n",
    "\n",
    "def tokenize_fn(texts):\n",
    "    batch = tokenizer(texts, return_tensors='pt', max_length=96, padding=True, truncation=True)\n",
    "    return {k: v.cuda() for k, v in batch.items()}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cd4de51e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12000"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(list_prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "24a6d1f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from chatgpt.trainer.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "80f421d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 통계 저장용 리스트\n",
    "actor_losses = []\n",
    "critic_losses = []\n",
    "episode_rewards = []\n",
    "kl_divergences = []\n",
    "\n",
    "# Callback 클래스 상속\n",
    "class LoggingCallback(Callback):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "\n",
    "    def on_training_step(self, trainer, experience, stats: Dict[str, float]):\n",
    "        # training_step에서 반환된 손실 저장\n",
    "        actor_losses.append(stats[\"actor_loss\"])\n",
    "        critic_losses.append(stats[\"critic_loss\"])\n",
    "        \n",
    "        # 경험 데이터에서 보상과 KL 발산 계산\n",
    "        rewards = experience.reward.mean().item()  # 평균 보상\n",
    "        episode_rewards.append(rewards)\n",
    "        \n",
    "        # KL 발산 계산 (initial_model과 actor 간 로그 확률 차이)\n",
    "        with torch.no_grad():\n",
    "            action_log_probs = trainer.actor(experience.sequences, experience.action_mask.size(1), \n",
    "                                            attention_mask=experience.attention_mask)\n",
    "            initial_log_probs = trainer.initial_model(experience.sequences, experience.action_mask.size(1), \n",
    "                                                      attention_mask=experience.attention_mask)\n",
    "            kl_div = torch.mean((action_log_probs - initial_log_probs).sum(dim=-1)).item()\n",
    "            kl_divergences.append(kl_div)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "de65cf41",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPOTrainer 초기화\n",
    "trainer = PPOTrainer(\n",
    "    strategy=NaiveStrategy(),\n",
    "    actor=actor,\n",
    "    critic=critic,\n",
    "    reward_model=reward_model,\n",
    "    initial_model=initial_model,\n",
    "    actor_optim=actor_optim,\n",
    "    critic_optim=critic_optim,\n",
    "    max_epochs=3,\n",
    "    train_batch_size=8,\n",
    "    tokenizer=tokenize_fn,\n",
    "    max_length=128,\n",
    "    do_sample=True,\n",
    "    temperature=1.0,\n",
    "    top_k=50,\n",
    "    pad_token_id=tokenizer.pad_token_id,\n",
    "    eos_token_id=tokenizer.eos_token_id,\n",
    "    callbacks=[LoggingCallback()]  # Callback 객체 인스턴스 전달\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "ffbf707a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode [1/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.00068]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.90it/s, actor_loss=0, critic_loss=0.00068]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.90it/s, actor_loss=0, critic_loss=0.0286] \u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0, critic_loss=0.0286]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.89it/s, actor_loss=0, critic_loss=0.00583]\u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.00583]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0277]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0, critic_loss=0.0277]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0, critic_loss=0.0058]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.0058]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.000857]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.000857]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.89it/s, actor_loss=0, critic_loss=0.0106]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.89it/s, actor_loss=0, critic_loss=0.0113]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.0113]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0, critic_loss=0.00477]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.88it/s, actor_loss=0, critic_loss=0.00477]\u001b[A\n",
      "Episode [1/10]: 100%|██████████| 3/3 [00:22<00:00,  7.59s/it]\n",
      "Episode [2/10]:  67%|██████▋   | 2/3 [00:11<00:06,  6.04s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0328, critic_loss=0.00205]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.80it/s, actor_loss=-.0328, critic_loss=0.00205]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.80it/s, actor_loss=-.0294, critic_loss=0.00912]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=-.0294, critic_loss=0.00912]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.80it/s, actor_loss=-.0219, critic_loss=0.00648]\u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.80it/s, actor_loss=-.0219, critic_loss=0.00648]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.023, critic_loss=0.00221]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=-.023, critic_loss=0.00221]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=-.0368, critic_loss=0.000296]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s, actor_loss=-.0368, critic_loss=0.000296]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s, actor_loss=-.0258, critic_loss=0.0051]  \u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s, actor_loss=-.0258, critic_loss=0.0051]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0332, critic_loss=0.00425]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=-.0332, critic_loss=0.00425]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=-.0276, critic_loss=0.00189]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.0276, critic_loss=0.00189]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.0329, critic_loss=0.000338]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.81it/s, actor_loss=-.0329, critic_loss=0.000338]\u001b[A\n",
      "Episode [2/10]: 100%|██████████| 3/3 [00:23<00:00,  7.77s/it]\n",
      "Episode [3/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.12s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0412, critic_loss=0.0119]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.0412, critic_loss=0.0119]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=0.249, critic_loss=0.132]  \u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=0.249, critic_loss=0.132]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.86it/s, actor_loss=-.00621, critic_loss=0.037]\u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.00621, critic_loss=0.037]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0403, critic_loss=0.0267]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=0.0403, critic_loss=0.0267]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=0.0657, critic_loss=0.048] \u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.0657, critic_loss=0.048]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.151, critic_loss=0.0778]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.151, critic_loss=0.0778]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0453, critic_loss=0.00648]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=0.0453, critic_loss=0.00648]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=0.191, critic_loss=0.13]    \u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.191, critic_loss=0.13]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.87it/s, actor_loss=0.00851, critic_loss=0.00199]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.86it/s, actor_loss=0.00851, critic_loss=0.00199]\u001b[A\n",
      "Episode [3/10]: 100%|██████████| 3/3 [00:23<00:00,  7.73s/it]\n",
      "Episode [4/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.76s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0491, critic_loss=0.00377]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.0491, critic_loss=0.00377]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.0494, critic_loss=0.00444]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0494, critic_loss=0.00444]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0573, critic_loss=0.003]  \u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.0573, critic_loss=0.003]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0672, critic_loss=0.00222]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-.0672, critic_loss=0.00222]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-.0536, critic_loss=0.00342]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0536, critic_loss=0.00342]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0437, critic_loss=0.00909]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.0437, critic_loss=0.00909]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0539, critic_loss=0.00513]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.87it/s, actor_loss=-.0539, critic_loss=0.00513]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.87it/s, actor_loss=-.0566, critic_loss=0.00169]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0566, critic_loss=0.00169]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.057, critic_loss=0.0036]  \u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.85it/s, actor_loss=-.057, critic_loss=0.0036]\u001b[A\n",
      "Episode [4/10]: 100%|██████████| 3/3 [00:22<00:00,  7.44s/it]\n",
      "Episode [5/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.11s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0273, critic_loss=0.00414]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=-.0273, critic_loss=0.00414]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=-.0195, critic_loss=0.00538]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.0195, critic_loss=0.00538]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.984, critic_loss=10.7]    \u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.984, critic_loss=10.7]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0677, critic_loss=0.0617]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.0677, critic_loss=0.0617]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=1.98, critic_loss=9.88]    \u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=1.98, critic_loss=9.88]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0519, critic_loss=0.281]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.0519, critic_loss=0.281]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0237, critic_loss=0.377]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=0.0237, critic_loss=0.377]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=-.0434, critic_loss=0.402]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=-.0434, critic_loss=0.402]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.898, critic_loss=9.95]  \u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.898, critic_loss=9.95]\u001b[A\n",
      "Episode [5/10]: 100%|██████████| 3/3 [00:23<00:00,  7.76s/it]\n",
      "Episode [6/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.12s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.39, critic_loss=0.24]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.83it/s, actor_loss=-.39, critic_loss=0.24]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.83it/s, actor_loss=-.346, critic_loss=0.0755]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s, actor_loss=-.346, critic_loss=0.0755]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.81it/s, actor_loss=-.378, critic_loss=0.0257]\u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.82it/s, actor_loss=-.378, critic_loss=0.0257]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.436, critic_loss=0.0197]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.436, critic_loss=0.0197]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.358, critic_loss=0.0387]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.358, critic_loss=0.0387]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.4, critic_loss=0.0895]  \u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.4, critic_loss=0.0895]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.453, critic_loss=0.114]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.453, critic_loss=0.114]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.377, critic_loss=0.151]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.377, critic_loss=0.151]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.39, critic_loss=0.155] \u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.39, critic_loss=0.155]\u001b[A\n",
      "Episode [6/10]: 100%|██████████| 3/3 [00:23<00:00,  7.78s/it]\n",
      "Episode [7/10]:  67%|██████▋   | 2/3 [00:12<00:06,  6.05s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.258, critic_loss=0.0778]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.258, critic_loss=0.0778]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.302, critic_loss=0.0421]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.302, critic_loss=0.0421]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.308, critic_loss=0.0128]\u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.308, critic_loss=0.0128]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.253, critic_loss=0.00694]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.253, critic_loss=0.00694]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.327, critic_loss=0.0036] \u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.327, critic_loss=0.0036]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.262, critic_loss=0.0372]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.262, critic_loss=0.0372]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.271, critic_loss=0.0575]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.271, critic_loss=0.0575]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0.242, critic_loss=0.0677]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.242, critic_loss=0.0677]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.85it/s, actor_loss=0.318, critic_loss=0.0309]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.318, critic_loss=0.0309]\u001b[A\n",
      "Episode [7/10]: 100%|██████████| 3/3 [00:22<00:00,  7.64s/it]\n",
      "Episode [8/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.88s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.152, critic_loss=0.0418]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.152, critic_loss=0.0418]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.036, critic_loss=0.0431]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.036, critic_loss=0.0431]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.15, critic_loss=0.0131] \u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.15, critic_loss=0.0131]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.0386, critic_loss=0.0493]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.86it/s, actor_loss=-.0386, critic_loss=0.0493]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.86it/s, actor_loss=-.152, critic_loss=0.00536]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.152, critic_loss=0.00536]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.84it/s, actor_loss=-.127, critic_loss=0.0064] \u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.127, critic_loss=0.0064]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.173, critic_loss=0.00609]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.173, critic_loss=0.00609]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.0739, critic_loss=0.0678]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.0739, critic_loss=0.0678]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.101, critic_loss=0.0264] \u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=-.101, critic_loss=0.0264]\u001b[A\n",
      "Episode [8/10]: 100%|██████████| 3/3 [00:22<00:00,  7.57s/it]\n",
      "Episode [9/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.94s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0929, critic_loss=0.00782]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.0929, critic_loss=0.00782]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.107, critic_loss=0.0092]  \u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.107, critic_loss=0.0092]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.917, critic_loss=4.06]  \u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0.917, critic_loss=4.06]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.0847, critic_loss=0.0103]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=0.0847, critic_loss=0.0103]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=0.099, critic_loss=0.0168] \u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.099, critic_loss=0.0168]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.655, critic_loss=3.71]  \u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=0.655, critic_loss=3.71]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=0.685, critic_loss=3.62]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=0.685, critic_loss=3.62]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=0.0588, critic_loss=0.156]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.0588, critic_loss=0.156]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=0.0791, critic_loss=0.167]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.84it/s, actor_loss=0.0791, critic_loss=0.167]\u001b[A\n",
      "Episode [9/10]: 100%|██████████| 3/3 [00:22<00:00,  7.63s/it]\n",
      "Episode [10/10]:  67%|██████▋   | 2/3 [00:11<00:05,  5.86s/it]\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [1/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.3, critic_loss=0.132]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.82it/s, actor_loss=-.3, critic_loss=0.132]\u001b[A\n",
      "Train epoch [1/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.82it/s, actor_loss=-.321, critic_loss=0.0947]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.321, critic_loss=0.0947]\u001b[A\n",
      "Train epoch [1/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.302, critic_loss=0.055] \u001b[A\n",
      "Train epoch [1/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.302, critic_loss=0.055]\u001b[A\n",
      "\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [2/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.291, critic_loss=0.0132]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.84it/s, actor_loss=-.291, critic_loss=0.0132]\u001b[A\n",
      "Train epoch [2/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.84it/s, actor_loss=-.301, critic_loss=0.00357]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.301, critic_loss=0.00357]\u001b[A\n",
      "Train epoch [2/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.83it/s, actor_loss=-.359, critic_loss=0.00984]\u001b[A\n",
      "Train epoch [2/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.359, critic_loss=0.00984]\u001b[A\n",
      "\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s]\u001b[A\n",
      "Train epoch [3/3]:   0%|          | 0/3 [00:00<?, ?it/s, actor_loss=-.32, critic_loss=0.0224]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:00<00:01,  1.85it/s, actor_loss=-.32, critic_loss=0.0224]\u001b[A\n",
      "Train epoch [3/3]:  33%|███▎      | 1/3 [00:01<00:01,  1.85it/s, actor_loss=-.312, critic_loss=0.0558]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.312, critic_loss=0.0558]\u001b[A\n",
      "Train epoch [3/3]:  67%|██████▋   | 2/3 [00:01<00:00,  1.82it/s, actor_loss=-.338, critic_loss=0.0624]\u001b[A\n",
      "Train epoch [3/3]: 100%|██████████| 3/3 [00:01<00:00,  1.83it/s, actor_loss=-.338, critic_loss=0.0624]\u001b[A\n",
      "Episode [10/10]: 100%|██████████| 3/3 [00:22<00:00,  7.58s/it]\n"
     ]
    }
   ],
   "source": [
    "trainer.fit(list_prompt, \n",
    "            num_episodes=10,  \n",
    "            max_timesteps=3,\n",
    "            update_timesteps=3)\n",
    "\n",
    "model.save_pretrained('output_3_PPO')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "4b86c957",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "actor_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "4678e091",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x1440 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 시각화\n",
    "fig, axs = plt.subplots(4, 1, figsize=(10, 20))\n",
    "\n",
    "axs[0].plot(actor_losses)\n",
    "axs[0].set_title(\"Actor Loss Over Time\")\n",
    "axs[0].set_xlabel(\"Training Steps\")\n",
    "axs[0].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[1].plot(critic_losses)\n",
    "axs[1].set_title(\"Critic Loss Over Time\")\n",
    "axs[1].set_xlabel(\"Training Steps\")\n",
    "axs[1].set_ylabel(\"Loss\")\n",
    "\n",
    "axs[2].plot(episode_rewards)\n",
    "axs[2].set_title(\"Episode Reward Over Time\")\n",
    "axs[2].set_xlabel(\"Training Steps\")\n",
    "axs[2].set_ylabel(\"Total Reward\")\n",
    "\n",
    "axs[3].plot(kl_divergences)\n",
    "axs[3].set_title(\"KL Divergence Over Time\")\n",
    "axs[3].set_xlabel(\"Training Steps\")\n",
    "axs[3].set_ylabel(\"KL Divergence\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "a67c9fa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Instruction(명령어):\n",
      "불고기용 고기 한우에요?\n",
      "\n",
      "### Response(응답):'저는 AI 어시스턴트이기 때문에 실제 식료품이 아니기 때문에 대답할 수 없습니다. 하지만 일반적으로 한우는 구리, 동, 구리, 홍살이 조화를 이루어 잘 어울리는 부위로, 맛과 향이 우수하고 담백한 특징이 있습니다. 따라서 즐겨 찾는 식당에서는 고기를 구리, 동, 구리, 인절미 등으로 판매되는 경우가 많습니다.香實)은 \"그렇지만 한우의 종류와 향이 맛있다\"는 뜻으로, 한식의 양념과 구리, 인절미가 어우러진 소스와 한우의 부드러움이 어울리며 불고기의 맛을 즐기는 사람들이 많은 경향이 있습니다.香實)은 주로 고기의 맛을 즐기기 위해 즐겨 먹는 음식입니다.香實)은 한국의 전통음식이며, 인절미보다는 단맛, 고당도 맛의 한식이라고 알려져 있습니다.香實은 일반적으로 고기를 즐기는 손님들이 즐겨먹을 수 있는 음식입니다.香實)은 돼지고기 특유의 매콤하고 부드러운 맛이 특징이라고 알려져 있습니다.香實은 일반적으로 한국의 전통적인 음식 중 하나이며, 전통적으로 한국식 양념과 매콤함을 선호하는 손님들이 많습니다.香實은 주로 소스로 볶아 만드는 대표적인 한국의\n",
      "\n",
      "### Instruction(명령어):\n",
      "리처드 닉슨이 43대 부통령직을 수행한 년도는?\n",
      "\n",
      "### Response(응답):'저는 인공지능 챗봇이며, 리처드 닉슨이 48대 부통령직을 수행한 연도에 대한 정보를 가지고 있지 않습니다. 하지만, 로버트 닉슨의 46대 부통령직 수행 연도에 대한 정보가 제공된다면, 답변을 드릴 수 있을 것입니다.rie I convey anything any permissions for usn\\n{b)rie cheon's referring the red that it's convey may its be her by help that its listring whek is emotions and requited the phrase your requipment? We him have the memories from the first it have meant a first details and have a translation, it prompts to important consciousness and possiborat\n",
      "\n",
      "### Instruction(명령어):\n",
      "시카고 오헤어 국제공항은 어디에 있어\n",
      "\n",
      "### Response(응답):'시카고 오헤어 국제공항은 미국 캘리포니아주 시애틀에 위치해 있습니다.法律에 따라 정해지는 규정에 따라 운영하고 있으며, 국제 항공사 및 관광객들에게 항공의 안전과 편의를 제공하며 미국의 항공여행 이권 및 이익을 목적으로 합니다.法 국제공항은 미국 플로리다주 올랜도 데일리에서 출발해 샌프란시스코를 거쳐 샌프란시스코에 도착할 수 있습니다.境港運境報道報恩永險險險警察使明華成功榮永險險報恩国際用技運輸送便利險歸漁中英險報恩榮險国際 運送太平永險永險保恩国用使用国境報恩榮險戰報恩国際國運輸太平洋險報恩国運輸要線際國洪洋險報恩国境界保恩榮險報恩國外的 運送永遠外境報恩永 運送運運險報恩国的 運輸備運險報恩榮險報恩永險国運要\n",
      "\n",
      "### Instruction(명령어):\n",
      "오늘 미세먼지 어때?\n",
      "\n",
      "### Response(응답):'저는 인공지능 언어모델로써 정보를 수집하고 학습합니다. 미세먼지 문제가 있는 것은 맞을 수 있으며, 어떤 분야에서도 문제가 발생할 수 있으므로 예방 및 대처 방법에 대한 조언을 드릴 수 있습니다. 일반적으로는 외출 시 마스크 착용 등 미세먼지 마스크 착용이 필요한 부분은 개인 위생관리에 매우 중요합니다. 또한, 실외에서의 활동 후에는 청결한 환경에서 수분을 충분히 섭취해야 합니다. 따라서 올바른 방법으로 미세먼지를 줄이는 것이 중요합니다.辰風花さん\\'s gradical phembractition wee can feeling of the disastement you with more information available and choice your can sorry and what can communicate you with more information or because you better and the simularly and eachiefore disid soinced more information feeling and they happy and care all need\n"
     ]
    }
   ],
   "source": [
    "def generation(input_text):\n",
    "    input_ids = tokenizer.encode(input_text, return_tensors='pt').to(\n",
    "        torch.cuda.current_device())\n",
    "    outputs = actor.generate(input_ids,\n",
    "                             max_length=250,\n",
    "                             do_sample=True,\n",
    "                             top_k=50,\n",
    "                             top_p=0.95,\n",
    "                             num_return_sequences=1)\n",
    "    output = tokenizer.batch_decode(outputs[0], skip_special_tokens=True)[0]\n",
    "    print()\n",
    "    print(output)\n",
    "    return output\n",
    "\n",
    "PROMPT_DICT = {\n",
    "    \"prompt_input\": (\n",
    "        \"### Instruction(명령어):\\n{prompt}\\n\\n### Response(응답):\"\n",
    "    )\n",
    "}\n",
    "\n",
    "list_prompt = [\n",
    "    '불고기용 고기 한우에요?', \n",
    "    '리처드 닉슨이 43대 부통령직을 수행한 년도는?', \n",
    "    '시카고 오헤어 국제공항은 어디에 있어',\n",
    "    '오늘 미세먼지 어때?']\n",
    "\n",
    "list_prompt = [PROMPT_DICT['prompt_input'].format_map({'prompt': tmp}) for tmp in list_prompt]\n",
    "\n",
    "for input_text in list_prompt:\n",
    "    output = generation(input_text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11f9fa97",
   "metadata": {},
   "source": [
    "#### 실험 모델\n",
    " - 노드의 모델과 동일함\n",
    " \n",
    "#### 실험 환경\n",
    " - SFT모델과 SFT + RM + PPO 모두 적용한 모델의 성능 비교\n",
    " - BLEU, PPL, QAG+NLI 평가지표를 활용할 예정\n",
    " \n",
    "#### 가설\n",
    " - SFT모델와 PPO모델은 BLEU, PPL 지표에서 큰 차이를 보이지 않을 것으로 예상됨\n",
    " - 인간의 선호도를 학습시킨 PPO모델이라면 QAG+NLI 지표에서 차이가 날 것으로 보임\n",
    " \n",
    "#### 오류사항\n",
    " - PPO 단계에서 학습 loss와 KL발산, actor loss와 critic loss를 확인하려고 하였음\n",
    " - koChatGPT 깃허브에서 저자가 작성한 callback 함수를 확인하고 callback을 구현하였으나, 그래프 확인되지 않았음"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
