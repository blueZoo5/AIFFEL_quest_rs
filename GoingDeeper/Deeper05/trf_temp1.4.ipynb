{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d9ef3406",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import re\n",
    "import os\n",
    "import io\n",
    "import time\n",
    "import random\n",
    "\n",
    "import seaborn # Attention 시각화를 위해 필요!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "53443a70",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "슝=3\n"
     ]
    }
   ],
   "source": [
    "import matplotlib as mpl\n",
    "import matplotlib.font_manager as fm\n",
    "\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    " \n",
    "fontpath = '/usr/share/fonts/truetype/nanum/NanumBarunGothic.ttf'\n",
    "font = fm.FontProperties(fname=fontpath, size=9)\n",
    "plt.rc('font', family='NanumBarunGothic') \n",
    "mpl.font_manager.findfont(font)\n",
    "\n",
    "print(\"슝=3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51c826e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    def cal_angle(position, i):\n",
    "        return position / np.power(10000, int(i) / d_model)\n",
    "\n",
    "    def get_posi_angle_vec(position):\n",
    "        return [cal_angle(position, i) for i in range(d_model)]\n",
    "\n",
    "    sinusoid_table = np.array([get_posi_angle_vec(pos_i) for pos_i in range(pos)])\n",
    "    sinusoid_table[:, 0::2] = np.sin(sinusoid_table[:, 0::2])\n",
    "    sinusoid_table[:, 1::2] = np.cos(sinusoid_table[:, 1::2])\n",
    "    return sinusoid_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "720f7dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "            \n",
    "        self.depth = d_model // self.num_heads\n",
    "            \n",
    "        self.W_q = tf.keras.layers.Dense(d_model)\n",
    "        self.W_k = tf.keras.layers.Dense(d_model)\n",
    "        self.W_v = tf.keras.layers.Dense(d_model)\n",
    "            \n",
    "        self.linear = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def scaled_dot_product_attention(self, Q, K, V, mask):\n",
    "        d_k = tf.cast(K.shape[-1], tf.float32)\n",
    "        QK = tf.matmul(Q, K, transpose_b=True)\n",
    "\n",
    "        scaled_qk = QK / tf.math.sqrt(d_k)\n",
    "\n",
    "        if mask is not None: scaled_qk += (mask * -1e9)  \n",
    "\n",
    "        attentions = tf.nn.softmax(scaled_qk, axis=-1)\n",
    "        out = tf.matmul(attentions, V)\n",
    "\n",
    "        return out, attentions\n",
    "            \n",
    "\n",
    "    def split_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        split_x = tf.reshape(x, (batch_size, -1, self.num_heads, self.depth))\n",
    "        split_x = tf.transpose(split_x, perm=[0, 2, 1, 3])\n",
    "\n",
    "        return split_x\n",
    "\n",
    "    def combine_heads(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        combined_x = tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "        combined_x = tf.reshape(combined_x, (batch_size, -1, self.d_model))\n",
    "\n",
    "        return combined_x\n",
    "\n",
    "        \n",
    "    def call(self, Q, K, V, mask):\n",
    "        WQ = self.W_q(Q)\n",
    "        WK = self.W_k(K)\n",
    "        WV = self.W_v(V)\n",
    "        \n",
    "        WQ_splits = self.split_heads(WQ)\n",
    "        WK_splits = self.split_heads(WK)\n",
    "        WV_splits = self.split_heads(WV)\n",
    "            \n",
    "        out, attention_weights = self.scaled_dot_product_attention(\n",
    "            WQ_splits, WK_splits, WV_splits, mask)\n",
    "     \n",
    "        out = self.combine_heads(out)\n",
    "        out = self.linear(out)\n",
    "                \n",
    "        return out, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4bac22d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class PoswiseFeedForwardNet(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, d_ff):\n",
    "        super(PoswiseFeedForwardNet, self).__init__()\n",
    "        self.w_1 = tf.keras.layers.Dense(d_ff, activation='relu')\n",
    "        self.w_2 = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def call(self, x):\n",
    "        out = self.w_1(x)\n",
    "        out = self.w_2(out)\n",
    "            \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fce5439a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, n_heads, d_ff, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "\n",
    "        self.enc_self_attn = MultiHeadAttention(d_model, n_heads)\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, enc_attn = self.enc_self_attn(out, out, out, mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        return out, enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7d32aeb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, d_ff, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "\n",
    "        self.dec_self_attn = MultiHeadAttention(d_model, num_heads)\n",
    "        self.enc_dec_attn = MultiHeadAttention(d_model, num_heads)\n",
    "\n",
    "        self.ffn = PoswiseFeedForwardNet(d_model, d_ff)\n",
    "\n",
    "        self.norm_1 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_2 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "        self.norm_3 = tf.keras.layers.LayerNormalization(epsilon=1e-6)\n",
    "\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "    \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "\n",
    "        \"\"\"\n",
    "        Masked Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = x\n",
    "        out = self.norm_1(x)\n",
    "        out, dec_attn = self.dec_self_attn(out, out, out, causality_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        \"\"\"\n",
    "        Multi-Head Attention\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_2(out)\n",
    "        out, dec_enc_attn = self.enc_dec_attn(out, enc_out, enc_out, padding_mask)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "        \n",
    "        \"\"\"\n",
    "        Position-Wise Feed Forward Network\n",
    "        \"\"\"\n",
    "        residual = out\n",
    "        out = self.norm_3(out)\n",
    "        out = self.ffn(out)\n",
    "        out = self.dropout(out)\n",
    "        out += residual\n",
    "\n",
    "        return out, dec_attn, dec_enc_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e69870e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.enc_layers = [EncoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                        for _ in range(n_layers)]\n",
    "        \n",
    "    def call(self, x, mask):\n",
    "        out = x\n",
    "    \n",
    "        enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, enc_attn = self.enc_layers[i](out, mask)\n",
    "            enc_attns.append(enc_attn)\n",
    "        \n",
    "        return out, enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8bb76b55",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                 n_layers,\n",
    "                 d_model,\n",
    "                 n_heads,\n",
    "                 d_ff,\n",
    "                 dropout):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.n_layers = n_layers\n",
    "        self.dec_layers = [DecoderLayer(d_model, n_heads, d_ff, dropout) \n",
    "                            for _ in range(n_layers)]\n",
    "                            \n",
    "                            \n",
    "    def call(self, x, enc_out, causality_mask, padding_mask):\n",
    "        out = x\n",
    "    \n",
    "        dec_attns = list()\n",
    "        dec_enc_attns = list()\n",
    "        for i in range(self.n_layers):\n",
    "            out, dec_attn, dec_enc_attn = \\\n",
    "            self.dec_layers[i](out, enc_out, causality_mask, padding_mask)\n",
    "\n",
    "            dec_attns.append(dec_attn)\n",
    "            dec_enc_attns.append(dec_enc_attn)\n",
    "\n",
    "        return out, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "53785272",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self,\n",
    "                    n_layers,\n",
    "                    d_model,\n",
    "                    n_heads,\n",
    "                    d_ff,\n",
    "                    src_vocab_size,\n",
    "                    tgt_vocab_size,\n",
    "                    pos_len,\n",
    "                    dropout=0.2,\n",
    "                    shared=True):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "\n",
    "        self.enc_emb = tf.keras.layers.Embedding(src_vocab_size, d_model)\n",
    "        self.dec_emb = tf.keras.layers.Embedding(tgt_vocab_size, d_model)\n",
    "\n",
    "        self.pos_encoding = positional_encoding(pos_len, d_model)\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout)\n",
    "\n",
    "        self.encoder = Encoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "        self.decoder = Decoder(n_layers, d_model, n_heads, d_ff, dropout)\n",
    "\n",
    "        self.fc = tf.keras.layers.Dense(tgt_vocab_size)\n",
    "\n",
    "        self.shared = shared\n",
    "\n",
    "        if shared: self.fc.set_weights(tf.transpose(self.dec_emb.weights))\n",
    "\n",
    "    def embedding(self, emb, x):\n",
    "        seq_len = x.shape[1]\n",
    "        out = emb(x)\n",
    "\n",
    "        if self.shared: out *= tf.math.sqrt(self.d_model)\n",
    "\n",
    "        out += self.pos_encoding[np.newaxis, ...][:, :seq_len, :]\n",
    "        out = self.dropout(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "        \n",
    "    def call(self, enc_in, dec_in, enc_mask, causality_mask, dec_mask):\n",
    "        enc_in = self.embedding(self.enc_emb, enc_in)\n",
    "        dec_in = self.embedding(self.dec_emb, dec_in)\n",
    "\n",
    "        enc_out, enc_attns = self.encoder(enc_in, enc_mask)\n",
    "        \n",
    "        dec_out, dec_attns, dec_enc_attns = self.decoder(dec_in, enc_out, causality_mask, enc_mask)\n",
    "        \n",
    "        logits = self.fc(dec_out)\n",
    "        \n",
    "        return logits, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "832f884f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_padding_mask(seq):\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    return seq[:, tf.newaxis, tf.newaxis, :]\n",
    "\n",
    "def generate_causality_mask(size):\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # (seq_len, seq_len)\n",
    "\n",
    "def generate_masks(src, tgt):\n",
    "    enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_enc_mask = generate_padding_mask(src)\n",
    "\n",
    "    dec_mask = generate_causality_mask(tgt.shape[-1])\n",
    "    \n",
    "    # 디코더 causal 마스크를 batch_size에 맞게 확장\n",
    "    dec_mask = dec_mask[tf.newaxis, tf.newaxis, :, :]  # (1, 1, seq_len, seq_len)\n",
    "    dec_mask = tf.tile(dec_mask, [tf.shape(src)[0], 1, 1, 1])\n",
    "\n",
    "    return enc_mask, dec_enc_mask, dec_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17621c04",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "batch, length = 16, 20\n",
    "src_padding = 5\n",
    "tgt_padding = 15\n",
    "\n",
    "src_pad = tf.zeros(shape=(batch, src_padding))\n",
    "tgt_pad = tf.zeros(shape=(batch, tgt_padding))\n",
    "\n",
    "sample_data = tf.ones(shape=(batch, length))\n",
    "\n",
    "sample_src = tf.concat([sample_data, src_pad], axis=-1)\n",
    "sample_tgt = tf.concat([sample_data, tgt_pad], axis=-1)\n",
    "\n",
    "enc_mask, dec_enc_mask, dec_mask = \\\n",
    "generate_masks(sample_src, sample_tgt)\n",
    "\n",
    "fig = plt.figure(figsize=(15, 10))\n",
    "\n",
    "ax1 = fig.add_subplot(131)\n",
    "ax2 = fig.add_subplot(132)\n",
    "ax3 = fig.add_subplot(133)\n",
    "\n",
    "ax1.set_title('1) Encoder Mask')\n",
    "ax2.set_title('2) Encoder-Decoder Mask')\n",
    "ax3.set_title('3) Decoder Mask')\n",
    "\n",
    "ax1.imshow(enc_mask[:3, 0, 0].numpy(), cmap='Dark2')\n",
    "ax2.imshow(dec_enc_mask[0, 0].numpy(), cmap='Dark2')\n",
    "ax3.imshow(dec_mask[0, 0].numpy(), cmap='Dark2')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "6518f952",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LearningRateScheduler(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(LearningRateScheduler, self).__init__()\n",
    "        self.d_model = d_model\n",
    "        self.warmup_steps = warmup_steps\n",
    "    \n",
    "    def __call__(self, step):\n",
    "        arg1 = step ** -0.5\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        \n",
    "        return (self.d_model ** -0.5) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6595be35",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dir = os.getenv('HOME')+'/aiffel/transformer/data'\n",
    "kor_path = data_dir+\"/korean-english-park.train.ko\"\n",
    "eng_path = data_dir+\"/korean-english-park.train.en\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "da046d2a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터 정제 및 토큰화\n",
    "def clean_corpus(kor_path, eng_path):\n",
    "    seen = set()  # 중복 제거를 위한 set\n",
    "    cleaned_corpus = []\n",
    "\n",
    "    with open(kor_path, \"r\", encoding=\"utf-8\") as f_ko, \\\n",
    "    open(eng_path, \"r\", encoding=\"utf-8\") as f_en:\n",
    "        for k, e in zip(f_ko, f_en):  # 두 파일에서 동시에 한 줄씩 읽기\n",
    "            k, e = k.strip(), e.strip()  # 앞뒤 공백 제거\n",
    "            \n",
    "            pair = f\"{k}\\t{e}\"  # 탭으로 구분된 하나의 문자열로 저장\n",
    "            if pair not in seen:\n",
    "                seen.add(pair)\n",
    "                cleaned_corpus.append(pair)  # 리스트에 추가\n",
    "\n",
    "    return cleaned_corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "59a6c90f",
   "metadata": {},
   "outputs": [],
   "source": [
    "cleaned_corpus = clean_corpus(kor_path, eng_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "f6f03772",
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_sentence(sentence):\n",
    "    \n",
    "    # 소문자화\n",
    "    sentence = sentence.lower().strip()\n",
    "    \n",
    "    # 알파벳, 문장부호, 한글만 남기고 모두 제거\n",
    "    sentence = re.sub(r\"[^a-zA-Zㄱ-ㅎㅏ-ㅣ가-힣.,!?]\", \" \", sentence)\n",
    "    \n",
    "    # 문장부호 양옆에 공백 추가\n",
    "    sentence = re.sub(r\"([.,!?])\", r\" \\1 \", sentence)\n",
    "    \n",
    "    # 문장 앞뒤의 불필요한 공백 제거\n",
    "    sentence = re.sub(r\"\\s+\", \" \", sentence).strip()\n",
    "    \n",
    "    return sentence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0a039abd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sentencepiece as spm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "86749354",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sp_tokenizer_ko.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_tokenizer_ko\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sp_tokenizer_ko.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78940 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=5052697\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9501% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=1185\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999501\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78940 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 159141 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78940\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 195707\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 195707 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=83228 obj=12.5941 num_tokens=378612 num_tokens/piece=4.54909\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=70407 obj=11.4421 num_tokens=379926 num_tokens/piece=5.39614\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=52800 obj=11.4474 num_tokens=396867 num_tokens/piece=7.51642\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=52782 obj=11.414 num_tokens=397198 num_tokens/piece=7.52525\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=39586 obj=11.5541 num_tokens=420674 num_tokens/piece=10.6268\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=39586 obj=11.5175 num_tokens=420688 num_tokens/piece=10.6272\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=29689 obj=11.7111 num_tokens=447001 num_tokens/piece=15.0561\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=29689 obj=11.6697 num_tokens=447004 num_tokens/piece=15.0562\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22266 obj=11.9072 num_tokens=473417 num_tokens/piece=21.2619\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22266 obj=11.8614 num_tokens=473412 num_tokens/piece=21.2617\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=22000 obj=11.8796 num_tokens=474464 num_tokens/piece=21.5665\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=22000 obj=11.8771 num_tokens=474462 num_tokens/piece=21.5665\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: sp_tokenizer_ko.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: sp_tokenizer_ko.vocab\n",
      "sentencepiece_trainer.cc(77) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input: sp_tokenizer_en.txt\n",
      "  input_format: \n",
      "  model_prefix: sp_tokenizer_en\n",
      "  model_type: UNIGRAM\n",
      "  vocab_size: 20000\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 3\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: 0\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(329) LOG(INFO) SentenceIterator is not specified. Using MultiFileSentenceIterator.\n",
      "trainer_interface.cc(178) LOG(INFO) Loading corpus: sp_tokenizer_en.txt\n",
      "trainer_interface.cc(385) LOG(INFO) Loaded all 78929 sentences\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <pad>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(400) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(405) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(466) LOG(INFO) all chars count=10659815\n",
      "trainer_interface.cc(477) LOG(INFO) Done: 99.9909% characters are covered.\n",
      "trainer_interface.cc(487) LOG(INFO) Alphabet size=29\n",
      "trainer_interface.cc(488) LOG(INFO) Final character coverage=0.999909\n",
      "trainer_interface.cc(520) LOG(INFO) Done! preprocessed 78929 sentences.\n",
      "unigram_model_trainer.cc(139) LOG(INFO) Making suffix array...\n",
      "unigram_model_trainer.cc(143) LOG(INFO) Extracting frequent sub strings...\n",
      "unigram_model_trainer.cc(194) LOG(INFO) Initialized 82992 seed sentencepieces\n",
      "trainer_interface.cc(526) LOG(INFO) Tokenizing input sentences with whitespace: 78929\n",
      "trainer_interface.cc(537) LOG(INFO) Done! 44562\n",
      "unigram_model_trainer.cc(489) LOG(INFO) Using 44562 sentences for EM training\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=34531 obj=9.86219 num_tokens=83351 num_tokens/piece=2.4138\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=25849 obj=8.00619 num_tokens=83808 num_tokens/piece=3.24221\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=0 size=21977 obj=7.92346 num_tokens=84668 num_tokens/piece=3.85257\n",
      "unigram_model_trainer.cc(505) LOG(INFO) EM sub_iter=1 size=21849 obj=7.90466 num_tokens=84909 num_tokens/piece=3.88617\n",
      "trainer_interface.cc(615) LOG(INFO) Saving model: sp_tokenizer_en.model\n",
      "trainer_interface.cc(626) LOG(INFO) Saving vocabs: sp_tokenizer_en.vocab\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Sentencepiece를 활용하여 학습한 tokenizer를 생성합니다.\n",
    "def generate_tokenizer(corpus,\n",
    "                        vocab_size,\n",
    "                        lang=\"ko\",\n",
    "                        pad_id=0,\n",
    "                        bos_id=1,\n",
    "                        eos_id=2,\n",
    "                        unk_id=3):\n",
    "    \n",
    "    # 토크나이저 모델 저장할 경로 설정\n",
    "    model_prefix = f\"sp_tokenizer_{lang}\"\n",
    "    \n",
    "    # corpus 파일 저장 (SentencePiece는 파일 입력을 요구함)\n",
    "    corpus_file = f\"{model_prefix}.txt\"\n",
    "    with open(corpus_file, \"w\", encoding=\"utf-8\") as f:\n",
    "        f.write(\"\\n\".join(corpus))\n",
    "    \n",
    "    # SentencePiece 학습\n",
    "    spm.SentencePieceTrainer.train(\n",
    "        input=corpus_file,\n",
    "        model_prefix=model_prefix,\n",
    "        vocab_size=vocab_size,\n",
    "        pad_id=pad_id,\n",
    "        bos_id=bos_id,\n",
    "        eos_id=eos_id,\n",
    "        unk_id=unk_id,\n",
    "        model_type=\"unigram\"  # unigram 사용 (Unigram, char, word 가능)\n",
    "    )\n",
    "    \n",
    "    # 학습된 모델 로드\n",
    "    tokenizer = spm.SentencePieceProcessor()\n",
    "    tokenizer.load(f\"{model_prefix}.model\")\n",
    "\n",
    "    return tokenizer\n",
    "    \n",
    "\n",
    "SRC_VOCAB_SIZE = TGT_VOCAB_SIZE = 20000\n",
    "\n",
    "eng_corpus = []\n",
    "kor_corpus = []\n",
    "\n",
    "for pair in cleaned_corpus:\n",
    "    k, e = pair.split(\"\\t\")\n",
    "\n",
    "    kor_corpus.append(preprocess_sentence(k))\n",
    "    eng_corpus.append(preprocess_sentence(e))\n",
    "\n",
    "ko_tokenizer = generate_tokenizer(kor_corpus, SRC_VOCAB_SIZE, \"ko\")\n",
    "en_tokenizer = generate_tokenizer(eng_corpus, TGT_VOCAB_SIZE, \"en\")\n",
    "en_tokenizer.set_encode_extra_options(\"bos:eos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d118f9ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "33ee80fc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22b884664e07490c8c8b8472d5342b66",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/78941 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Process 과정을 보기 위해\n",
    "\n",
    "src_corpus = []\n",
    "tgt_corpus = []\n",
    "\n",
    "assert len(kor_corpus) == len(eng_corpus)\n",
    "\n",
    "# 토큰의 길이가 50 이하인 문장만 남깁니다. \n",
    "for idx in tqdm(range(len(kor_corpus))):\n",
    "\n",
    "    ko_tokens = ko_tokenizer.encode(kor_corpus[idx])  # 토큰 리스트로 변환\n",
    "    en_tokens = en_tokenizer.encode(eng_corpus[idx])\n",
    "\n",
    "    if len(ko_tokens) <= 50 and len(en_tokens) <= 50:\n",
    "        src_corpus.append(ko_tokens)\n",
    "        tgt_corpus.append(en_tokens)\n",
    "    \n",
    "# 패딩처리를 완료하여 학습용 데이터를 완성합니다. \n",
    "enc_train = tf.keras.preprocessing.sequence.pad_sequences(src_corpus, padding='post')\n",
    "dec_train = tf.keras.preprocessing.sequence.pad_sequences(tgt_corpus, padding='post')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "d0ebdcb9",
   "metadata": {},
   "outputs": [],
   "source": [
    "transformer = Transformer(\n",
    "    n_layers = 5,\n",
    "    d_ff = 512,\n",
    "    src_vocab_size = SRC_VOCAB_SIZE,\n",
    "    tgt_vocab_size = TGT_VOCAB_SIZE,\n",
    "    d_model = 256,\n",
    "    n_heads = 4,\n",
    "    dropout = 0.3,\n",
    "    pos_len=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "15a1c6c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = LearningRateScheduler(512)\n",
    "optimizer = tf.keras.optimizers.Adam(learning_rate,\n",
    "                                     beta_1=0.9,\n",
    "                                     beta_2=0.98, \n",
    "                                     epsilon=1e-9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c89b0a3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(\n",
    "    from_logits=True, reduction='none')\n",
    "\n",
    "def loss_function(real, pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(real, 0))\n",
    "    loss_ = loss_object(real, pred)\n",
    "\n",
    "    # Masking 되지 않은 입력의 개수로 Scaling하는 과정\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "\n",
    "    return tf.reduce_sum(loss_)/tf.reduce_sum(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "06b627a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 기울기 계산\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 기울기 적용하여 모델 파라미터 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e56d893c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Attention 시각화 함수\n",
    "\n",
    "def visualize_attention(src, tgt, enc_attns, dec_attns, dec_enc_attns):\n",
    "    def draw(data, ax, x=\"auto\", y=\"auto\"):\n",
    "        import seaborn\n",
    "        seaborn.heatmap(data, \n",
    "                        square=True,\n",
    "                        vmin=0.0, vmax=1.0, \n",
    "                        cbar=False, ax=ax,\n",
    "                        xticklabels=x,\n",
    "                        yticklabels=y)\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Encoder Layer\", layer + 1)\n",
    "        for h in range(4):\n",
    "            draw(enc_attns[layer][0, h, :len(src), :len(src)], axs[h], src, src)\n",
    "        plt.show()\n",
    "        \n",
    "    for layer in range(0, 2, 1):\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        print(\"Decoder Self Layer\", layer+1)\n",
    "        for h in range(4):\n",
    "            draw(dec_attns[layer][0, h, :len(tgt), :len(tgt)], axs[h], tgt, tgt)\n",
    "        plt.show()\n",
    "\n",
    "        print(\"Decoder Src Layer\", layer+1)\n",
    "        fig, axs = plt.subplots(1, 4, figsize=(20, 10))\n",
    "        for h in range(4):\n",
    "            draw(dec_enc_attns[layer][0, h, :len(tgt), :len(src)], axs[h], src, tgt)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "561a49db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Step 함수 정의\n",
    "\n",
    "@tf.function()\n",
    "def train_step(src, tgt, model, optimizer):\n",
    "    gold = tgt[:, 1:]\n",
    "        \n",
    "    enc_mask, dec_enc_mask, dec_mask = generate_masks(src, tgt)\n",
    "\n",
    "    # 계산된 loss에 tf.GradientTape()를 적용해 학습을 진행합니다.\n",
    "    with tf.GradientTape() as tape:\n",
    "        predictions, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        model(src, tgt, enc_mask, dec_enc_mask, dec_mask)\n",
    "        loss = loss_function(gold, predictions[:, :-1])\n",
    "\n",
    "    # 기울기 계산\n",
    "    gradients = tape.gradient(loss, model.trainable_variables)\n",
    "    \n",
    "    # 기울기 적용하여 모델 파라미터 업데이트\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    \n",
    "    return loss, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "cdf1bbf7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(sentence, model, src_tokenizer, tgt_tokenizer, max_len=50):\n",
    "    # 문장 전처리 (preprocess_sentence 함수가 정의되어 있다고 가정)\n",
    "    sentence = preprocess_sentence(sentence)\n",
    "\n",
    "    # 소스 문장 토크나이징\n",
    "    pieces = src_tokenizer.encode_as_pieces(sentence)  # 토큰 문자열 리스트\n",
    "    tokens = src_tokenizer.encode_as_ids(sentence)     # 토큰 ID 리스트\n",
    "\n",
    "    # 입력 패딩 (enc_train의 최대 길이에 맞춤)\n",
    "    _input = tf.keras.preprocessing.sequence.pad_sequences(\n",
    "        [tokens], maxlen=max_len, padding='post', value=src_tokenizer.pad_id()\n",
    "    )  # (1, max_len)\n",
    "\n",
    "    # 출력 초기화: BOS 토큰으로 시작\n",
    "    output = tf.expand_dims([tgt_tokenizer.bos_id()], 0)  # (1, 1)\n",
    "    ids = []\n",
    "\n",
    "    # Attention 가중치 저장\n",
    "    enc_attns, dec_attns, dec_enc_attns = [], [], []\n",
    "\n",
    "    # 디코더 예측 루프\n",
    "    for _ in range(max_len):\n",
    "        # 마스크 생성\n",
    "        enc_padding_mask = generate_padding_mask(_input)  # (1, 1, 1, src_len)\n",
    "        dec_padding_mask = generate_padding_mask(output)  # (1, 1, 1, tgt_len)\n",
    "        combined_mask = tf.maximum(\n",
    "            dec_padding_mask,\n",
    "            generate_causality_mask(tf.shape(output)[1])\n",
    "        )  # (1, 1, tgt_len, tgt_len)\n",
    "\n",
    "        # 모델 예측\n",
    "        predictions, enc_attns_t, dec_attns_t, dec_enc_attns_t = model(\n",
    "            _input,\n",
    "            output,\n",
    "            enc_padding_mask,\n",
    "            combined_mask,\n",
    "            enc_padding_mask\n",
    "        )\n",
    "        \n",
    "        # 마지막 토큰에 대한 로짓\n",
    "        #logits = predictions[:, -1, :]  # (1, tgt_vocab_size)\n",
    "\n",
    "        # 소프트맥스 온도 적용\n",
    "        #scaled_logits = logits / 1.1\n",
    "        #probs = tf.nn.softmax(scaled_logits, axis=-1)  # (1, tgt_vocab_size)\n",
    "\n",
    "        # 확률 분포에서 샘플링\n",
    "        #predicted_id = tf.random.categorical(probs, num_samples=1)[0, 0].numpy()  # 스칼라 값\n",
    "        #predicted_id = int(predicted_id)  # Python int로 변환\n",
    "        \n",
    "        scaled_logits = predictions[:, -1, :] / 1.4  # 온도 적용\n",
    "        predicted_id = int(tf.argmax(scaled_logits, axis=-1).numpy()[0]) \n",
    "\n",
    "\n",
    "        # EOS 토큰이면 종료\n",
    "        if predicted_id == tgt_tokenizer.eos_id():\n",
    "            print(f\"ids before decode: {ids}, types: {[type(x) for x in ids]}\")\n",
    "            result = tgt_tokenizer.decode_ids(ids)\n",
    "            \n",
    "            return pieces, result, enc_attns, dec_attns, dec_enc_attns\n",
    "\n",
    "        # 예측된 ID 추가\n",
    "        ids.append(predicted_id)\n",
    "        output = tf.concat([output, [[predicted_id]]], axis=-1)  # (1, tgt_len+1)\n",
    "\n",
    "        # Attention 가중치 저장\n",
    "        enc_attns.append(enc_attns_t)\n",
    "        dec_attns.append(dec_attns_t)\n",
    "        dec_enc_attns.append(dec_enc_attns_t)\n",
    "\n",
    "    # 최대 길이에 도달한 경우\n",
    "    print(f\"ids after loop: {ids}, types: {[type(x) for x in ids]}\")\n",
    "    result = tgt_tokenizer.decode_ids(ids)\n",
    "    return pieces, result, enc_attns, dec_attns, dec_enc_attns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "25c97af9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 번역 생성 및 Attention 시각화 결합\n",
    "\n",
    "def translate(sentence, model, src_tokenizer, tgt_tokenizer, plot_attention=False):\n",
    "    pieces, result, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "    evaluate(sentence, model, src_tokenizer, tgt_tokenizer)\n",
    "    \n",
    "    print('Input: %s' % (sentence))\n",
    "    print('Predicted translation: {}'.format(result))\n",
    "\n",
    "    if plot_attention:\n",
    "        visualize_attention(pieces, result.split(), enc_attns, dec_attns, dec_enc_attns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2dd4d7ed",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_1948/2719091706.py:18: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  t = tqdm_notebook(idx_list)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fbcabb90c8be463fae37b28ae461fa32",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [], types: []\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: \n",
      "ids before decode: [], types: []\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: \n",
      "ids before decode: [], types: []\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: \n",
      "ids before decode: [], types: []\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: \n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0289bcc979e4ef697d52d488c3b4304",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [131, 131, 131, 131, 131, 131, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama obama obama obama obama obama .\n",
      "ids before decode: [], types: []\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: \n",
      "ids before decode: [], types: []\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: \n",
      "ids before decode: [82, 82, 82, 82, 82, 82, 82, 82, 82, 82, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 6, 82, 6, 82, 6, 82, 6, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: police police police police police police police police police police , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , police , police , police , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "75cbcd3a61674462b80357c4ae5aca8e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [29, 29, 29, 25, 327, 8, 27, 161, 29, 25, 327, 8, 27, 161, 11, 36, 1964, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: he he he was going to because he was going to because of his resignation .\n",
      "ids before decode: [49, 49, 49, 170, 170, 23, 8, 4, 165, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they they they re rely to the city .\n",
      "ids before decode: [88, 88, 88, 88, 88, 88, 88, 88, 88, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: other other other other other other other other other .\n",
      "ids before decode: [114, 114, 17, 17, 17, 17, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: officials officials said said said said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a1c4b556df954510bc54f14a4c11d188",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [131, 131, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 657, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama obama america america america america america america america america america america america america america america america america america america america america america america america america america america .\n",
      "ids before decode: [362, 362, 362, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: area area area . . .\n",
      "ids before decode: [], types: []\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: \n",
      "ids before decode: [360, 360, 360, 360, 360, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: men men men men men .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5a7da7e99dea44a19b6dd137aefdfedc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 451, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain mccain .\n",
      "ids before decode: [5, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: . . . . . .\n",
      "ids before decode: [8, 8, 8, 4, 61, 61, 61, 23, 8, 4, 253, 8, 4, 61, 61, 61, 61, 3492, 344, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: to to to the new new newly to the way to the new new new new administrative capital .\n",
      "ids before decode: [5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "997ea2cde950448db6328718cfdd597e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [67, 67, 67, 720, 515, 131, 8, 739, 12, 36, 36, 36, 36, 36, 36, 131, 15, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president president president elect barack obama to stay in his his his his his his obama s .\n",
      "ids before decode: [316, 316, 316, 316, 6, 6, 13, 9, 404, 11, 11, 11, 11, 4, 165, 11, 4, 362, 11, 4, 165, 11, 9, 467, 362, 151, 49, 38, 324, 58, 257, 863, 863, 863, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: fire fire fire fire , , and a half of of of of the city of the area of the city of a big area where they are among those homes homes homes .\n",
      "ids before decode: [9, 9, 9, 9, 9, 61, 2878], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: a a a a a new zealand\n",
      "ids before decode: [129, 129, 129, 129, 10, 12, 9, 396, 127, 127, 127, 127, 127, 127, 6, 13, 13, 13, 13, 13, 13, 9, 396, 127, 60, 396, 127, 15, 1247, 9, 9, 9, 9, 396, 127, 396, 127, 66, 127, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: kill kill kill killed in a third day day day day day day , and and and and and and a third day two third day struck a a a a third day third day one day .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "731bbc39df2c4a1a861c6baa6e74fad8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [67, 67, 67, 67, 515, 131, 15, 15, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president president president president barack obama s s .\n",
      "ids before decode: [1504, 1504, 1504, 1504, 1504, 6, 9, 1504, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: mountain mountain mountain mountain mountain , a mountain .\n",
      "ids after loop: [1055, 1055, 1055, 1055, 21, 21, 21, 21, 21, 21, 21, 21, 18, 31, 31, 31, 234, 234, 234, 234, 234, 2825, 21, 6, 39, 93, 93, 93, 229, 365, 229, 365, 229, 365, 365, 365, 365, 18, 31, 234, 31, 234, 365, 229, 365, 229, 365, 229, 365, 229], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: bybybybydddddddd for an an anotherotherotherotherother gunsd , but no no no long run long run long run run run run for another another run long run long run long run long\n",
      "ids before decode: [5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e8b2c0a993e498e83c21a80f6ec5caf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [67, 67, 67, 67, 131, 22, 131, 22, 36, 232, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president president president president obama is obama is his office .\n",
      "ids before decode: [4, 4, 4, 4, 287, 154, 23, 1157, 10, 12, 4, 287, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the the the children nearly contained in the children .\n",
      "ids after loop: [86, 86, 86, 86, 22, 31, 234, 1046, 8, 159, 52, 18, 4, 1679, 1679, 1679, 7, 7, 18, 4, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 1679, 7, 6, 39, 423, 31, 1679, 1679, 1679, 18, 359, 1706, 7, 291, 27, 87, 27, 87, 27, 87, 27], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: there there there there is another threat to taken for the proposal proposal proposalss for the proposal proposal proposal proposal proposal proposal proposal proposal proposal proposals , but clear an proposal proposal proposal for major options should be us be us be us be\n",
      "ids before decode: [238, 81, 168, 168, 168, 168, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: authorities thursday thursday thursday thursday .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "02727cf123b24dd2a84ed7fabfe56d8b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [131, 36, 36, 36, 67, 131, 25, 1244, 36, 67, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama his his his president obama was holding his president .\n",
      "ids before decode: [1504, 1504, 1504, 1504, 1504, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: mountain mountain mountain mountain mountain .\n",
      "ids before decode: [80, 80, 80, 80, 80, 80, 18, 18, 18, 18, 9, 80, 9, 80, 18, 18, 9, 3776, 1747, 10, 12, 575, 6, 39, 22, 324, 4, 2458, 1336, 1336, 1336, 390, 58, 390, 58, 390, 58, 390, 58, 390, 58, 240, 58, 240, 423, 80, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: cnn cnn cnn cnn cnn cnn for for for for a cnn a cnn for for a newly sleeped in short , but is among the provincialialial don t don t don t don t don t want t want clear cnn .\n",
      "ids before decode: [60, 60, 60, 60, 530, 41, 540, 10, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: two two two two others were injured .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fe6d01ed5cdf4eb3bfefdd19721bb0e5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [36, 36, 36, 36, 244, 11, 36, 244], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: his his his his top of his top\n",
      "ids before decode: [86, 86, 86, 86, 37, 9, 1993, 674, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: there there there there have a dis airport .\n",
      "ids before decode: [128, 128, 128, 128, 128, 128, 86, 22, 31, 24, 3], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: what what what what what what there is an  ⁇ \n",
      "ids before decode: [205, 205, 205, 205, 205, 205, 205, 205, 117, 6, 29, 17, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: six six six six six six six six party , he said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "50d5a09db45f4e3daa7a2c6db7aae7e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [131, 131, 131, 131, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama obama obama obama .\n",
      "ids before decode: [49, 49, 49, 37, 37, 9, 165, 11, 4, 154, 23, 277, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they they they have have a city of the nearly well . .\n",
      "ids before decode: [368, 368, 368, 368, 504, 504, 504, 504, 30, 241, 64, 9, 593, 42, 27, 207, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: iran iran iran iran continue continue continue continue at least a key will be found .\n",
      "ids before decode: [9, 9, 9, 9, 9, 9, 219, 127, 6, 29, 17, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a a a a a a second day , he said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e88e7df24a974fa2a5c2d75968abbf7a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [256, 256, 256, 1280, 1280, 1280, 1280, 26, 36, 148, 1280, 6, 67, 256, 15, 1455, 14, 135, 1280, 36, 36, 131, 15, 1170, 135, 34, 36, 244, 798, 15, 304, 176, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: clinton clinton clinton shows shows shows shows with his bush shows , president clinton s commanding him shows his his obama spent him as his top player s white house . . . .\n",
      "ids before decode: [594, 594, 594, 594, 4, 1546, 27, 185, 185, 185, 185, 863, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: outside outside outside outside the society be many many many many homes . . .\n",
      "ids before decode: [956, 159, 159, 159, 159, 159, 159, 31, 157, 253, 229, 229, 159, 31, 31, 31, 1305, 247, 8, 159, 31, 68, 8, 31, 157, 253, 18, 327, 8, 159, 31, 1305, 247, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: thai take take take take take take an under way long long take an an an interior ministry to take an up to an under way for going to take an interior ministry .\n",
      "ids before decode: [238, 81, 888, 888, 888, 888, 10, 143, 4, 84, 359, 127, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: authorities express express express expressed against the first major day . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "760ac94142c54165a59b56c083db91e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [105, 105, 105, 67, 67, 720, 6, 67, 6, 105, 105, 67, 720, 10, 8, 36, 67, 11, 36, 36, 36, 67, 11, 36, 270, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: you you you president president elect , president , you you president elected to his president of his his his president of his presidential .\n",
      "ids before decode: [4, 4, 4, 90, 90, 90, 90, 90, 90, 90, 90, 9, 342, 768, 14, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: the the the into into into into into into into into a night seeking .\n",
      "ids before decode: [19, 19, 19, 19, 240, 240, 240, 240, 7, 8, 3042, 9, 1079, 8, 1079, 8, 1079, 8, 225, 8, 225, 8, 225, 97, 19, 225, 632, 10, 632, 10, 6, 1427, 1445, 4507, 4478, 5009, 189, 269, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: that that that that want want want wants to restrict a field to field to field to go to go to goes that go involved involved , especiallysom clayyodo news agency .\n",
      "ids before decode: [347, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: face . . . . . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "10df8153b8374185ac9fcf0d4268692d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [256, 256, 256, 15, 3067, 186, 4, 67, 720, 720, 720, 10, 36, 270, 1035], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: clinton clinton clinton sitting through the president elect elect elected his presidential nomination\n",
      "ids before decode: [140, 140, 140, 140, 140, 11, 10, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: most most most most most ofed .\n",
      "ids after loop: [1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64, 1593, 64], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot cannot\n",
      "ids before decode: [360, 360, 360, 360, 360, 360, 360, 360, 6, 2228, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: men men men men men men men men , collapsed .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b884a5266bef4800b7d8e010d2b751e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [131, 131, 131], types: [<class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: obama obama obama\n",
      "ids before decode: [140, 140, 140, 140, 140, 140, 1107, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: most most most most most most lives .\n",
      "ids before decode: [19, 19, 19, 19, 19, 19, 87, 10, 86, 22, 93, 93, 93, 93, 265], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: that that that that that that used there is no no no no place\n",
      "ids before decode: [360, 360, 360, 360, 38, 360, 38, 360, 6, 1899, 6, 6, 6, 6, 6, 75, 27, 207, 10, 16, 119, 6, 329, 6, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: men men men men are men are men , channel , , , , , also be founded on monday , family , .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0ac7913baebb4f95b33ce8740bd39885",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [34, 34, 34, 36, 84, 94, 9, 432, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: as as as his first time a hospital .\n",
      "ids before decode: [49, 49, 49, 37, 37, 37, 9, 92, 5, 5, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: they they they have have have ar . . . . . . .\n",
      "ids before decode: [19, 19, 19, 19, 19, 134, 370, 18, 370, 18, 370, 18, 370, 18, 370, 18, 370, 6, 39, 128, 1657, 59, 423, 23, 423, 23, 423, 23, 12, 59, 2951, 784, 7, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: that that that that that like business for business for business for business for business for business , but whatever its clearly clearly clearly in its islamist fronts .\n",
      "ids before decode: [49, 49, 49, 175, 360, 360, 6, 29, 17, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: they they they four men men , he said .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "22fbc60420d64b649ca22e15b221a30c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [752, 26, 26, 26, 26, 36, 1163], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: sex with with with with his parents\n",
      "ids before decode: [752, 752, 752, 752, 496, 496, 32, 4, 154, 23], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: sex sex sex sex away away from the nearly\n",
      "ids before decode: [128, 128, 128, 128, 128, 159, 24, 3], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: what what what what what take  ⁇ \n",
      "ids before decode: [238, 81, 287, 287, 287, 5, 5, 5, 5, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: authorities children children children . . . . . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16e3082cb67a41fdabd6afacaa9a3a7c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [84, 84, 84, 84, 84, 1457], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: first first first first first thing\n",
      "ids before decode: [140, 140, 140, 140, 140, 1014, 11, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: most most most most most kind of .\n",
      "ids before decode: [423, 423, 423, 423, 423, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: clear clear clear clear clear .\n",
      "ids before decode: [9, 9, 9, 9, 9, 175, 528, 75, 27, 27, 207, 10, 18, 9, 349, 437, 5, 5, 5, 5, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: a a a a a fourth also be be founded for a left workers . . . . .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1ee789b4f53343bb8ba35a7e1c74e683",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [67, 256, 256, 256, 15, 3067, 12, 4, 270, 686, 686], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: president clinton clinton clinton sitting in the presidential term term\n",
      "ids before decode: [140, 140, 140, 140, 140, 140, 289, 289, 289, 289, 289, 289, 289, 289, 289, 165, 11, 4, 26, 140, 12, 4, 304, 7, 12, 4, 26, 140, 11, 4, 304, 7, 12, 4, 5883, 443, 11, 4499, 4558, 274, 12, 4, 382, 165, 2082, 4499, 4558, 3014, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: most most most most most most both both both both both both both both both city of the with most in the whites in the with most of the whites in the rubble of sectarianion in the main city hall sectarian delhi .\n",
      "ids before decode: [453, 584, 584, 584, 584, 58, 2582, 2582, 2582, 2582, 2582, 58, 2582, 58, 2582, 30, 4, 1297, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: does never never never never t easy easy easy easy easy t easy t easy at the streets .\n",
      "ids before decode: [238, 81, 1498, 10, 1498, 10, 1498, 1498, 1498, 10, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: authorities occurred occurred occurr occurr occurred .\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cec5532e77ed4c7eb2a546616ba8bf7d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1127 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ids before decode: [752, 752, 752, 1373], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 오바마는 대통령이다.\n",
      "Predicted translation: sex sex sex friends\n",
      "ids before decode: [752, 752, 752, 752, 752, 594, 11, 4, 73, 11, 4, 761, 11, 4, 761, 11, 4, 761, 11, 4, 761, 46, 49, 37, 48, 858, 10, 12, 4, 1038, 2925, 289, 858, 10, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 시민들은 도시 속에 산다.\n",
      "Predicted translation: sex sex sex sex sex outside of the north of the heart of the heart of the heart of the heart after they have been lived in the rural both lived .\n",
      "ids before decode: [763, 763, 763, 763, 763, 10, 18, 59, 418, 716, 7, 134, 368, 368, 368, 374, 370, 632, 14, 18, 368, 18, 368, 38, 93, 3261, 345, 5], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 커피는 필요 없다.\n",
      "Predicted translation: threaten threaten threaten threaten threatened for its defense acts like iran iran iran case business involving for iran for iran are no indication life .\n",
      "ids after loop: [238, 81, 1085, 1085, 1085, 1085, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439, 7, 1439], types: [<class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>, <class 'int'>]\n",
      "Input: 일곱 명의 사망자가 발생했다.\n",
      "Predicted translation: authoritiesrarararas units units units units units units units units units units units units units units units units units units units units units unit\n"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm_notebook \n",
    "\n",
    "BATCH_SIZE = 64\n",
    "EPOCHS = 20\n",
    "\n",
    "examples = [\n",
    "            \"오바마는 대통령이다.\",\n",
    "            \"시민들은 도시 속에 산다.\",\n",
    "            \"커피는 필요 없다.\",\n",
    "            \"일곱 명의 사망자가 발생했다.\"\n",
    "]\n",
    "\n",
    "for epoch in range(EPOCHS):\n",
    "    total_loss = 0\n",
    "    \n",
    "    idx_list = list(range(0, enc_train.shape[0], BATCH_SIZE))\n",
    "    random.shuffle(idx_list)\n",
    "    t = tqdm_notebook(idx_list)\n",
    "\n",
    "    for (batch, idx) in enumerate(t):\n",
    "        batch_loss, enc_attns, dec_attns, dec_enc_attns = \\\n",
    "        train_step(enc_train[idx:idx+BATCH_SIZE],\n",
    "                    dec_train[idx:idx+BATCH_SIZE],\n",
    "                    transformer,\n",
    "                    optimizer)\n",
    "\n",
    "        total_loss += batch_loss\n",
    "        \n",
    "        t.set_description_str('Epoch %2d' % (epoch + 1))\n",
    "        t.set_postfix_str('Loss %.4f' % (total_loss.numpy() / (batch + 1)))\n",
    "\n",
    "    for example in examples:\n",
    "        translate(example, transformer, ko_tokenizer, en_tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "76a8d5ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "72117"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#len(enc_train)\n",
    "len(dec_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "b6c013ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 1055   352   522   483     7  1290  1845     8  1214   220  2967   937\n",
      "    29 10605   376     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0]\n",
      "[   1  341   11 1354 7450   14   22   56  107  105  244   50   24    3\n",
      "    2    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0    0    0    0]\n"
     ]
    }
   ],
   "source": [
    "print(enc_train[0])\n",
    "print(dec_train[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "41102c2e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<s>'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "en_tokenizer.id_to_piece(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ede8182",
   "metadata": {},
   "source": [
    "### 결론\n",
    "- 소프트맥스 온도 값이 1.1일 때보다 다양한 단어들이 나옴\n",
    "- 하지만 단어가 반복되는 문제는 여전히 발생함\n",
    "- 유의미한 단어들은 나타났지만 괜찮은 성능을 만들었다고 보기 어려움"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
